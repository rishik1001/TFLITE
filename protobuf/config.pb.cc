// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/protobuf/config.proto

#include "tensorflow/core/protobuf/config.pb.h"

#include <algorithm>

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/extension_set.h>
#include <google/protobuf/wire_format_lite.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>

PROTOBUF_PRAGMA_INIT_SEG

namespace _pb = ::PROTOBUF_NAMESPACE_ID;
namespace _pbi = _pb::internal;

namespace tensorflow {
PROTOBUF_CONSTEXPR GPUOptions_Experimental_VirtualDevices::GPUOptions_Experimental_VirtualDevices(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.memory_limit_mb_)*/{}
  , /*decltype(_impl_.priority_)*/{}
  , /*decltype(_impl_._priority_cached_byte_size_)*/{0}
  , /*decltype(_impl_.device_ordinal_)*/{}
  , /*decltype(_impl_._device_ordinal_cached_byte_size_)*/{0}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct GPUOptions_Experimental_VirtualDevicesDefaultTypeInternal {
  PROTOBUF_CONSTEXPR GPUOptions_Experimental_VirtualDevicesDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~GPUOptions_Experimental_VirtualDevicesDefaultTypeInternal() {}
  union {
    GPUOptions_Experimental_VirtualDevices _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 GPUOptions_Experimental_VirtualDevicesDefaultTypeInternal _GPUOptions_Experimental_VirtualDevices_default_instance_;
PROTOBUF_CONSTEXPR GPUOptions_Experimental_StreamMergeOptions::GPUOptions_Experimental_StreamMergeOptions(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.merge_host_to_device_stream_)*/false
  , /*decltype(_impl_.merge_device_to_host_stream_)*/false
  , /*decltype(_impl_.merge_device_to_device_stream_)*/false
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct GPUOptions_Experimental_StreamMergeOptionsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR GPUOptions_Experimental_StreamMergeOptionsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~GPUOptions_Experimental_StreamMergeOptionsDefaultTypeInternal() {}
  union {
    GPUOptions_Experimental_StreamMergeOptions _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 GPUOptions_Experimental_StreamMergeOptionsDefaultTypeInternal _GPUOptions_Experimental_StreamMergeOptions_default_instance_;
PROTOBUF_CONSTEXPR GPUOptions_Experimental::GPUOptions_Experimental(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.virtual_devices_)*/{}
  , /*decltype(_impl_.collective_ring_order_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.stream_merge_options_)*/nullptr
  , /*decltype(_impl_.num_dev_to_dev_copy_streams_)*/0
  , /*decltype(_impl_.kernel_tracker_max_interval_)*/0
  , /*decltype(_impl_.use_unified_memory_)*/false
  , /*decltype(_impl_.timestamped_allocator_)*/false
  , /*decltype(_impl_.use_cuda_malloc_async_)*/false
  , /*decltype(_impl_.disallow_retry_on_allocation_failure_)*/false
  , /*decltype(_impl_.kernel_tracker_max_bytes_)*/0
  , /*decltype(_impl_.internal_fragmentation_fraction_)*/0
  , /*decltype(_impl_.kernel_tracker_max_pending_)*/0
  , /*decltype(_impl_.gpu_host_mem_limit_in_mb_)*/0
  , /*decltype(_impl_.num_virtual_devices_per_gpu_)*/0
  , /*decltype(_impl_.gpu_host_mem_disallow_growth_)*/false
  , /*decltype(_impl_.populate_pjrt_gpu_client_creation_info_)*/false
  , /*decltype(_impl_.gpu_system_memory_size_in_mb_)*/0
  , /*decltype(_impl_.node_id_)*/0
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct GPUOptions_ExperimentalDefaultTypeInternal {
  PROTOBUF_CONSTEXPR GPUOptions_ExperimentalDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~GPUOptions_ExperimentalDefaultTypeInternal() {}
  union {
    GPUOptions_Experimental _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 GPUOptions_ExperimentalDefaultTypeInternal _GPUOptions_Experimental_default_instance_;
PROTOBUF_CONSTEXPR GPUOptions::GPUOptions(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.allocator_type_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.visible_device_list_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.experimental_)*/nullptr
  , /*decltype(_impl_.per_process_gpu_memory_fraction_)*/0
  , /*decltype(_impl_.deferred_deletion_bytes_)*/int64_t{0}
  , /*decltype(_impl_.polling_active_delay_usecs_)*/0
  , /*decltype(_impl_.allow_growth_)*/false
  , /*decltype(_impl_.force_gpu_compatible_)*/false
  , /*decltype(_impl_.polling_inactive_delay_msecs_)*/0
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct GPUOptionsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR GPUOptionsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~GPUOptionsDefaultTypeInternal() {}
  union {
    GPUOptions _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 GPUOptionsDefaultTypeInternal _GPUOptions_default_instance_;
PROTOBUF_CONSTEXPR OptimizerOptions::OptimizerOptions(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.opt_level_)*/0
  , /*decltype(_impl_.do_common_subexpression_elimination_)*/false
  , /*decltype(_impl_.do_constant_folding_)*/false
  , /*decltype(_impl_.do_function_inlining_)*/false
  , /*decltype(_impl_.cpu_global_jit_)*/false
  , /*decltype(_impl_.max_folded_constant_in_bytes_)*/int64_t{0}
  , /*decltype(_impl_.global_jit_level_)*/0
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct OptimizerOptionsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR OptimizerOptionsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~OptimizerOptionsDefaultTypeInternal() {}
  union {
    OptimizerOptions _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 OptimizerOptionsDefaultTypeInternal _OptimizerOptions_default_instance_;
PROTOBUF_CONSTEXPR GraphOptions::GraphOptions(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.optimizer_options_)*/nullptr
  , /*decltype(_impl_.rewrite_options_)*/nullptr
  , /*decltype(_impl_.build_cost_model_)*/int64_t{0}
  , /*decltype(_impl_.enable_recv_scheduling_)*/false
  , /*decltype(_impl_.infer_shapes_)*/false
  , /*decltype(_impl_.place_pruned_graph_)*/false
  , /*decltype(_impl_.enable_bfloat16_sendrecv_)*/false
  , /*decltype(_impl_.timeline_step_)*/0
  , /*decltype(_impl_.build_cost_model_after_)*/int64_t{0}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct GraphOptionsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR GraphOptionsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~GraphOptionsDefaultTypeInternal() {}
  union {
    GraphOptions _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 GraphOptionsDefaultTypeInternal _GraphOptions_default_instance_;
PROTOBUF_CONSTEXPR ThreadPoolOptionProto::ThreadPoolOptionProto(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.global_name_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.num_threads_)*/0
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct ThreadPoolOptionProtoDefaultTypeInternal {
  PROTOBUF_CONSTEXPR ThreadPoolOptionProtoDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~ThreadPoolOptionProtoDefaultTypeInternal() {}
  union {
    ThreadPoolOptionProto _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 ThreadPoolOptionProtoDefaultTypeInternal _ThreadPoolOptionProto_default_instance_;
PROTOBUF_CONSTEXPR SessionMetadata::SessionMetadata(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.name_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.version_)*/int64_t{0}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct SessionMetadataDefaultTypeInternal {
  PROTOBUF_CONSTEXPR SessionMetadataDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~SessionMetadataDefaultTypeInternal() {}
  union {
    SessionMetadata _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 SessionMetadataDefaultTypeInternal _SessionMetadata_default_instance_;
PROTOBUF_CONSTEXPR ConfigProto_DeviceCountEntry_DoNotUse::ConfigProto_DeviceCountEntry_DoNotUse(
    ::_pbi::ConstantInitialized) {}
struct ConfigProto_DeviceCountEntry_DoNotUseDefaultTypeInternal {
  PROTOBUF_CONSTEXPR ConfigProto_DeviceCountEntry_DoNotUseDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~ConfigProto_DeviceCountEntry_DoNotUseDefaultTypeInternal() {}
  union {
    ConfigProto_DeviceCountEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 ConfigProto_DeviceCountEntry_DoNotUseDefaultTypeInternal _ConfigProto_DeviceCountEntry_DoNotUse_default_instance_;
PROTOBUF_CONSTEXPR ConfigProto_Experimental::ConfigProto_Experimental(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.collective_group_leader_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.executor_type_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.session_metadata_)*/nullptr
  , /*decltype(_impl_.coordination_config_)*/nullptr
  , /*decltype(_impl_.recv_buf_max_chunk_)*/0
  , /*decltype(_impl_.use_numa_affinity_)*/false
  , /*decltype(_impl_.collective_deterministic_sequential_execution_)*/false
  , /*decltype(_impl_.collective_nccl_)*/false
  , /*decltype(_impl_.share_session_state_in_clusterspec_propagation_)*/false
  , /*decltype(_impl_.disable_thread_spinning_)*/false
  , /*decltype(_impl_.share_cluster_devices_in_session_)*/false
  , /*decltype(_impl_.optimize_for_static_graph_)*/false
  , /*decltype(_impl_.enable_mlir_bridge_)*/false
  , /*decltype(_impl_.mlir_bridge_rollout_)*/0
  , /*decltype(_impl_.xla_fusion_autotuner_thresh_)*/int64_t{0}
  , /*decltype(_impl_.enable_mlir_graph_optimization_)*/false
  , /*decltype(_impl_.disable_output_partition_graphs_)*/false
  , /*decltype(_impl_.use_tfrt_)*/false
  , /*decltype(_impl_.enable_multi_host_)*/false
  , /*decltype(_impl_.disable_optimize_for_static_graph_)*/false
  , /*decltype(_impl_.disable_eager_executor_streaming_enqueue_)*/false
  , /*decltype(_impl_.target_tpu_)*/false
  , /*decltype(_impl_.target_gpu_)*/false
  , /*decltype(_impl_.disable_functional_ops_lowering_)*/false
  , /*decltype(_impl_.xla_prefer_single_graph_cluster_)*/false
  , /*decltype(_impl_.backend_server_port_)*/0
  , /*decltype(_impl_.stream_merge_threshold_)*/0
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct ConfigProto_ExperimentalDefaultTypeInternal {
  PROTOBUF_CONSTEXPR ConfigProto_ExperimentalDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~ConfigProto_ExperimentalDefaultTypeInternal() {}
  union {
    ConfigProto_Experimental _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 ConfigProto_ExperimentalDefaultTypeInternal _ConfigProto_Experimental_default_instance_;
PROTOBUF_CONSTEXPR ConfigProto::ConfigProto(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.device_count_)*/{::_pbi::ConstantInitialized()}
  , /*decltype(_impl_.device_filters_)*/{}
  , /*decltype(_impl_.session_inter_op_thread_pool_)*/{}
  , /*decltype(_impl_.gpu_options_)*/nullptr
  , /*decltype(_impl_.graph_options_)*/nullptr
  , /*decltype(_impl_.rpc_options_)*/nullptr
  , /*decltype(_impl_.cluster_def_)*/nullptr
  , /*decltype(_impl_.experimental_)*/nullptr
  , /*decltype(_impl_.intra_op_parallelism_threads_)*/0
  , /*decltype(_impl_.placement_period_)*/0
  , /*decltype(_impl_.inter_op_parallelism_threads_)*/0
  , /*decltype(_impl_.use_per_session_threads_)*/false
  , /*decltype(_impl_.allow_soft_placement_)*/false
  , /*decltype(_impl_.log_device_placement_)*/false
  , /*decltype(_impl_.isolate_session_state_)*/false
  , /*decltype(_impl_.operation_timeout_in_ms_)*/int64_t{0}
  , /*decltype(_impl_.share_cluster_devices_in_session_)*/false
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct ConfigProtoDefaultTypeInternal {
  PROTOBUF_CONSTEXPR ConfigProtoDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~ConfigProtoDefaultTypeInternal() {}
  union {
    ConfigProto _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 ConfigProtoDefaultTypeInternal _ConfigProto_default_instance_;
PROTOBUF_CONSTEXPR RunOptions_Experimental_RunHandlerPoolOptions::RunOptions_Experimental_RunHandlerPoolOptions(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.priority_)*/int64_t{0}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct RunOptions_Experimental_RunHandlerPoolOptionsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR RunOptions_Experimental_RunHandlerPoolOptionsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~RunOptions_Experimental_RunHandlerPoolOptionsDefaultTypeInternal() {}
  union {
    RunOptions_Experimental_RunHandlerPoolOptions _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 RunOptions_Experimental_RunHandlerPoolOptionsDefaultTypeInternal _RunOptions_Experimental_RunHandlerPoolOptions_default_instance_;
PROTOBUF_CONSTEXPR RunOptions_Experimental::RunOptions_Experimental(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.run_handler_pool_options_)*/nullptr
  , /*decltype(_impl_.collective_graph_key_)*/int64_t{0}
  , /*decltype(_impl_.use_run_handler_pool_)*/false
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct RunOptions_ExperimentalDefaultTypeInternal {
  PROTOBUF_CONSTEXPR RunOptions_ExperimentalDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~RunOptions_ExperimentalDefaultTypeInternal() {}
  union {
    RunOptions_Experimental _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 RunOptions_ExperimentalDefaultTypeInternal _RunOptions_Experimental_default_instance_;
PROTOBUF_CONSTEXPR RunOptions::RunOptions(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.debug_options_)*/nullptr
  , /*decltype(_impl_.experimental_)*/nullptr
  , /*decltype(_impl_.timeout_in_ms_)*/int64_t{0}
  , /*decltype(_impl_.trace_level_)*/0
  , /*decltype(_impl_.inter_op_thread_pool_)*/0
  , /*decltype(_impl_.output_partition_graphs_)*/false
  , /*decltype(_impl_.report_tensor_allocations_upon_oom_)*/false
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct RunOptionsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR RunOptionsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~RunOptionsDefaultTypeInternal() {}
  union {
    RunOptions _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 RunOptionsDefaultTypeInternal _RunOptions_default_instance_;
PROTOBUF_CONSTEXPR RunMetadata_FunctionGraphs::RunMetadata_FunctionGraphs(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.partition_graphs_)*/{}
  , /*decltype(_impl_.pre_optimization_graph_)*/nullptr
  , /*decltype(_impl_.post_optimization_graph_)*/nullptr
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct RunMetadata_FunctionGraphsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR RunMetadata_FunctionGraphsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~RunMetadata_FunctionGraphsDefaultTypeInternal() {}
  union {
    RunMetadata_FunctionGraphs _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 RunMetadata_FunctionGraphsDefaultTypeInternal _RunMetadata_FunctionGraphs_default_instance_;
PROTOBUF_CONSTEXPR RunMetadata::RunMetadata(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.partition_graphs_)*/{}
  , /*decltype(_impl_.function_graphs_)*/{}
  , /*decltype(_impl_.step_stats_)*/nullptr
  , /*decltype(_impl_.cost_graph_)*/nullptr
  , /*decltype(_impl_.session_metadata_)*/nullptr
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct RunMetadataDefaultTypeInternal {
  PROTOBUF_CONSTEXPR RunMetadataDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~RunMetadataDefaultTypeInternal() {}
  union {
    RunMetadata _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 RunMetadataDefaultTypeInternal _RunMetadata_default_instance_;
PROTOBUF_CONSTEXPR TensorConnection::TensorConnection(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.from_tensor_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.to_tensor_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct TensorConnectionDefaultTypeInternal {
  PROTOBUF_CONSTEXPR TensorConnectionDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~TensorConnectionDefaultTypeInternal() {}
  union {
    TensorConnection _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 TensorConnectionDefaultTypeInternal _TensorConnection_default_instance_;
PROTOBUF_CONSTEXPR CallableOptions_FeedDevicesEntry_DoNotUse::CallableOptions_FeedDevicesEntry_DoNotUse(
    ::_pbi::ConstantInitialized) {}
struct CallableOptions_FeedDevicesEntry_DoNotUseDefaultTypeInternal {
  PROTOBUF_CONSTEXPR CallableOptions_FeedDevicesEntry_DoNotUseDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~CallableOptions_FeedDevicesEntry_DoNotUseDefaultTypeInternal() {}
  union {
    CallableOptions_FeedDevicesEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 CallableOptions_FeedDevicesEntry_DoNotUseDefaultTypeInternal _CallableOptions_FeedDevicesEntry_DoNotUse_default_instance_;
PROTOBUF_CONSTEXPR CallableOptions_FetchDevicesEntry_DoNotUse::CallableOptions_FetchDevicesEntry_DoNotUse(
    ::_pbi::ConstantInitialized) {}
struct CallableOptions_FetchDevicesEntry_DoNotUseDefaultTypeInternal {
  PROTOBUF_CONSTEXPR CallableOptions_FetchDevicesEntry_DoNotUseDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~CallableOptions_FetchDevicesEntry_DoNotUseDefaultTypeInternal() {}
  union {
    CallableOptions_FetchDevicesEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 CallableOptions_FetchDevicesEntry_DoNotUseDefaultTypeInternal _CallableOptions_FetchDevicesEntry_DoNotUse_default_instance_;
PROTOBUF_CONSTEXPR CallableOptions::CallableOptions(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.feed_)*/{}
  , /*decltype(_impl_.fetch_)*/{}
  , /*decltype(_impl_.target_)*/{}
  , /*decltype(_impl_.tensor_connection_)*/{}
  , /*decltype(_impl_.feed_devices_)*/{::_pbi::ConstantInitialized()}
  , /*decltype(_impl_.fetch_devices_)*/{::_pbi::ConstantInitialized()}
  , /*decltype(_impl_.run_options_)*/nullptr
  , /*decltype(_impl_.fetch_skip_sync_)*/false
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct CallableOptionsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR CallableOptionsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~CallableOptionsDefaultTypeInternal() {}
  union {
    CallableOptions _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 CallableOptionsDefaultTypeInternal _CallableOptions_default_instance_;
}  // namespace tensorflow
static ::_pb::Metadata file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[20];
static const ::_pb::EnumDescriptor* file_level_enum_descriptors_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[4];
static constexpr ::_pb::ServiceDescriptor const** file_level_service_descriptors_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto = nullptr;

const uint32_t TableStruct_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto::offsets[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental_VirtualDevices, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental_VirtualDevices, _impl_.memory_limit_mb_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental_VirtualDevices, _impl_.priority_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental_VirtualDevices, _impl_.device_ordinal_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental_StreamMergeOptions, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental_StreamMergeOptions, _impl_.merge_host_to_device_stream_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental_StreamMergeOptions, _impl_.merge_device_to_host_stream_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental_StreamMergeOptions, _impl_.merge_device_to_device_stream_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.virtual_devices_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.num_virtual_devices_per_gpu_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.use_unified_memory_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.num_dev_to_dev_copy_streams_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.collective_ring_order_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.timestamped_allocator_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.kernel_tracker_max_interval_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.kernel_tracker_max_bytes_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.kernel_tracker_max_pending_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.internal_fragmentation_fraction_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.use_cuda_malloc_async_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.disallow_retry_on_allocation_failure_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.gpu_host_mem_limit_in_mb_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.gpu_host_mem_disallow_growth_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.gpu_system_memory_size_in_mb_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.populate_pjrt_gpu_client_creation_info_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.node_id_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions_Experimental, _impl_.stream_merge_options_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions, _impl_.per_process_gpu_memory_fraction_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions, _impl_.allow_growth_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions, _impl_.allocator_type_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions, _impl_.deferred_deletion_bytes_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions, _impl_.visible_device_list_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions, _impl_.polling_active_delay_usecs_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions, _impl_.polling_inactive_delay_msecs_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions, _impl_.force_gpu_compatible_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GPUOptions, _impl_.experimental_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::OptimizerOptions, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::OptimizerOptions, _impl_.do_common_subexpression_elimination_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::OptimizerOptions, _impl_.do_constant_folding_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::OptimizerOptions, _impl_.max_folded_constant_in_bytes_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::OptimizerOptions, _impl_.do_function_inlining_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::OptimizerOptions, _impl_.opt_level_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::OptimizerOptions, _impl_.global_jit_level_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::OptimizerOptions, _impl_.cpu_global_jit_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::GraphOptions, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::GraphOptions, _impl_.enable_recv_scheduling_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GraphOptions, _impl_.optimizer_options_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GraphOptions, _impl_.build_cost_model_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GraphOptions, _impl_.build_cost_model_after_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GraphOptions, _impl_.infer_shapes_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GraphOptions, _impl_.place_pruned_graph_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GraphOptions, _impl_.enable_bfloat16_sendrecv_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GraphOptions, _impl_.timeline_step_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::GraphOptions, _impl_.rewrite_options_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::ThreadPoolOptionProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::ThreadPoolOptionProto, _impl_.num_threads_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ThreadPoolOptionProto, _impl_.global_name_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::SessionMetadata, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::SessionMetadata, _impl_.name_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::SessionMetadata, _impl_.version_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_DeviceCountEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_DeviceCountEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_DeviceCountEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_DeviceCountEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.collective_group_leader_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.executor_type_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.recv_buf_max_chunk_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.use_numa_affinity_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.collective_deterministic_sequential_execution_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.collective_nccl_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.share_session_state_in_clusterspec_propagation_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.disable_thread_spinning_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.share_cluster_devices_in_session_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.session_metadata_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.optimize_for_static_graph_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.enable_mlir_bridge_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.mlir_bridge_rollout_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.enable_mlir_graph_optimization_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.disable_output_partition_graphs_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.xla_fusion_autotuner_thresh_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.use_tfrt_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.enable_multi_host_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.backend_server_port_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.target_tpu_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.target_gpu_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.stream_merge_threshold_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.disable_functional_ops_lowering_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.xla_prefer_single_graph_cluster_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.coordination_config_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.disable_optimize_for_static_graph_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto_Experimental, _impl_.disable_eager_executor_streaming_enqueue_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.device_count_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.intra_op_parallelism_threads_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.inter_op_parallelism_threads_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.use_per_session_threads_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.session_inter_op_thread_pool_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.placement_period_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.device_filters_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.gpu_options_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.allow_soft_placement_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.log_device_placement_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.graph_options_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.operation_timeout_in_ms_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.rpc_options_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.cluster_def_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.isolate_session_state_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.share_cluster_devices_in_session_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::ConfigProto, _impl_.experimental_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions_Experimental_RunHandlerPoolOptions, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions_Experimental_RunHandlerPoolOptions, _impl_.priority_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions_Experimental, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions_Experimental, _impl_.collective_graph_key_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions_Experimental, _impl_.use_run_handler_pool_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions_Experimental, _impl_.run_handler_pool_options_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions, _impl_.trace_level_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions, _impl_.timeout_in_ms_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions, _impl_.inter_op_thread_pool_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions, _impl_.output_partition_graphs_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions, _impl_.debug_options_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions, _impl_.report_tensor_allocations_upon_oom_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunOptions, _impl_.experimental_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunMetadata_FunctionGraphs, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunMetadata_FunctionGraphs, _impl_.partition_graphs_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunMetadata_FunctionGraphs, _impl_.pre_optimization_graph_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunMetadata_FunctionGraphs, _impl_.post_optimization_graph_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunMetadata, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunMetadata, _impl_.step_stats_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunMetadata, _impl_.cost_graph_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunMetadata, _impl_.partition_graphs_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunMetadata, _impl_.function_graphs_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::RunMetadata, _impl_.session_metadata_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::TensorConnection, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::TensorConnection, _impl_.from_tensor_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::TensorConnection, _impl_.to_tensor_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions_FeedDevicesEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions_FeedDevicesEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions_FeedDevicesEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions_FeedDevicesEntry_DoNotUse, value_),
  0,
  1,
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions_FetchDevicesEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions_FetchDevicesEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions_FetchDevicesEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions_FetchDevicesEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions, _impl_.feed_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions, _impl_.fetch_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions, _impl_.target_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions, _impl_.run_options_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions, _impl_.tensor_connection_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions, _impl_.feed_devices_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions, _impl_.fetch_devices_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::CallableOptions, _impl_.fetch_skip_sync_),
};
static const ::_pbi::MigrationSchema schemas[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  { 0, -1, -1, sizeof(::tensorflow::GPUOptions_Experimental_VirtualDevices)},
  { 9, -1, -1, sizeof(::tensorflow::GPUOptions_Experimental_StreamMergeOptions)},
  { 18, -1, -1, sizeof(::tensorflow::GPUOptions_Experimental)},
  { 42, -1, -1, sizeof(::tensorflow::GPUOptions)},
  { 57, -1, -1, sizeof(::tensorflow::OptimizerOptions)},
  { 70, -1, -1, sizeof(::tensorflow::GraphOptions)},
  { 85, -1, -1, sizeof(::tensorflow::ThreadPoolOptionProto)},
  { 93, -1, -1, sizeof(::tensorflow::SessionMetadata)},
  { 101, 109, -1, sizeof(::tensorflow::ConfigProto_DeviceCountEntry_DoNotUse)},
  { 111, -1, -1, sizeof(::tensorflow::ConfigProto_Experimental)},
  { 144, -1, -1, sizeof(::tensorflow::ConfigProto)},
  { 167, -1, -1, sizeof(::tensorflow::RunOptions_Experimental_RunHandlerPoolOptions)},
  { 174, -1, -1, sizeof(::tensorflow::RunOptions_Experimental)},
  { 183, -1, -1, sizeof(::tensorflow::RunOptions)},
  { 196, -1, -1, sizeof(::tensorflow::RunMetadata_FunctionGraphs)},
  { 205, -1, -1, sizeof(::tensorflow::RunMetadata)},
  { 216, -1, -1, sizeof(::tensorflow::TensorConnection)},
  { 224, 232, -1, sizeof(::tensorflow::CallableOptions_FeedDevicesEntry_DoNotUse)},
  { 234, 242, -1, sizeof(::tensorflow::CallableOptions_FetchDevicesEntry_DoNotUse)},
  { 244, -1, -1, sizeof(::tensorflow::CallableOptions)},
};

static const ::_pb::Message* const file_default_instances[] = {
  &::tensorflow::_GPUOptions_Experimental_VirtualDevices_default_instance_._instance,
  &::tensorflow::_GPUOptions_Experimental_StreamMergeOptions_default_instance_._instance,
  &::tensorflow::_GPUOptions_Experimental_default_instance_._instance,
  &::tensorflow::_GPUOptions_default_instance_._instance,
  &::tensorflow::_OptimizerOptions_default_instance_._instance,
  &::tensorflow::_GraphOptions_default_instance_._instance,
  &::tensorflow::_ThreadPoolOptionProto_default_instance_._instance,
  &::tensorflow::_SessionMetadata_default_instance_._instance,
  &::tensorflow::_ConfigProto_DeviceCountEntry_DoNotUse_default_instance_._instance,
  &::tensorflow::_ConfigProto_Experimental_default_instance_._instance,
  &::tensorflow::_ConfigProto_default_instance_._instance,
  &::tensorflow::_RunOptions_Experimental_RunHandlerPoolOptions_default_instance_._instance,
  &::tensorflow::_RunOptions_Experimental_default_instance_._instance,
  &::tensorflow::_RunOptions_default_instance_._instance,
  &::tensorflow::_RunMetadata_FunctionGraphs_default_instance_._instance,
  &::tensorflow::_RunMetadata_default_instance_._instance,
  &::tensorflow::_TensorConnection_default_instance_._instance,
  &::tensorflow::_CallableOptions_FeedDevicesEntry_DoNotUse_default_instance_._instance,
  &::tensorflow::_CallableOptions_FetchDevicesEntry_DoNotUse_default_instance_._instance,
  &::tensorflow::_CallableOptions_default_instance_._instance,
};

const char descriptor_table_protodef_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) =
  "\n%tensorflow/core/protobuf/config.proto\022"
  "\ntensorflow\032*tensorflow/core/framework/c"
  "ost_graph.proto\032%tensorflow/core/framewo"
  "rk/graph.proto\032*tensorflow/core/framewor"
  "k/step_stats.proto\032&tensorflow/core/prot"
  "obuf/cluster.proto\032$tensorflow/core/prot"
  "obuf/debug.proto\032.tensorflow/core/protob"
  "uf/rewriter_config.proto\032*tensorflow/cor"
  "e/protobuf/rpc_options.proto\032&tsl/protob"
  "uf/coordination_config.proto\"\211\n\n\nGPUOpti"
  "ons\022\'\n\037per_process_gpu_memory_fraction\030\001"
  " \001(\001\022\024\n\014allow_growth\030\004 \001(\010\022\026\n\016allocator_"
  "type\030\002 \001(\t\022\037\n\027deferred_deletion_bytes\030\003 "
  "\001(\003\022\033\n\023visible_device_list\030\005 \001(\t\022\"\n\032poll"
  "ing_active_delay_usecs\030\006 \001(\005\022$\n\034polling_"
  "inactive_delay_msecs\030\007 \001(\005\022\034\n\024force_gpu_"
  "compatible\030\010 \001(\010\0229\n\014experimental\030\t \001(\0132#"
  ".tensorflow.GPUOptions.Experimental\032\302\007\n\014"
  "Experimental\022K\n\017virtual_devices\030\001 \003(\01322."
  "tensorflow.GPUOptions.Experimental.Virtu"
  "alDevices\022#\n\033num_virtual_devices_per_gpu"
  "\030\017 \001(\005\022\032\n\022use_unified_memory\030\002 \001(\010\022#\n\033nu"
  "m_dev_to_dev_copy_streams\030\003 \001(\005\022\035\n\025colle"
  "ctive_ring_order\030\004 \001(\t\022\035\n\025timestamped_al"
  "locator\030\005 \001(\010\022#\n\033kernel_tracker_max_inte"
  "rval\030\007 \001(\005\022 \n\030kernel_tracker_max_bytes\030\010"
  " \001(\005\022\"\n\032kernel_tracker_max_pending\030\t \001(\005"
  "\022\'\n\037internal_fragmentation_fraction\030\n \001("
  "\001\022\035\n\025use_cuda_malloc_async\030\013 \001(\010\022,\n$disa"
  "llow_retry_on_allocation_failure\030\014 \001(\010\022 "
  "\n\030gpu_host_mem_limit_in_mb\030\r \001(\002\022$\n\034gpu_"
  "host_mem_disallow_growth\030\016 \001(\010\022$\n\034gpu_sy"
  "stem_memory_size_in_mb\030\020 \001(\005\022.\n&populate"
  "_pjrt_gpu_client_creation_info\030\021 \001(\010\022\017\n\007"
  "node_id\030\022 \001(\005\022T\n\024stream_merge_options\030\023 "
  "\001(\01326.tensorflow.GPUOptions.Experimental"
  ".StreamMergeOptions\032S\n\016VirtualDevices\022\027\n"
  "\017memory_limit_mb\030\001 \003(\002\022\020\n\010priority\030\002 \003(\005"
  "\022\026\n\016device_ordinal\030\003 \003(\005\032\205\001\n\022StreamMerge"
  "Options\022#\n\033merge_host_to_device_stream\030\001"
  " \001(\010\022#\n\033merge_device_to_host_stream\030\002 \001("
  "\010\022%\n\035merge_device_to_device_stream\030\003 \001(\010"
  "\"\235\003\n\020OptimizerOptions\022+\n#do_common_subex"
  "pression_elimination\030\001 \001(\010\022\033\n\023do_constan"
  "t_folding\030\002 \001(\010\022$\n\034max_folded_constant_i"
  "n_bytes\030\006 \001(\003\022\034\n\024do_function_inlining\030\004 "
  "\001(\010\0225\n\topt_level\030\003 \001(\0162\".tensorflow.Opti"
  "mizerOptions.Level\022E\n\020global_jit_level\030\005"
  " \001(\0162+.tensorflow.OptimizerOptions.Globa"
  "lJitLevel\022\026\n\016cpu_global_jit\030\007 \001(\010\" \n\005Lev"
  "el\022\006\n\002L1\020\000\022\017\n\002L0\020\377\377\377\377\377\377\377\377\377\001\"C\n\016GlobalJit"
  "Level\022\013\n\007DEFAULT\020\000\022\020\n\003OFF\020\377\377\377\377\377\377\377\377\377\001\022\010\n\004"
  "ON_1\020\001\022\010\n\004ON_2\020\002\"\356\002\n\014GraphOptions\022\036\n\026ena"
  "ble_recv_scheduling\030\002 \001(\010\0227\n\021optimizer_o"
  "ptions\030\003 \001(\0132\034.tensorflow.OptimizerOptio"
  "ns\022\030\n\020build_cost_model\030\004 \001(\003\022\036\n\026build_co"
  "st_model_after\030\t \001(\003\022\024\n\014infer_shapes\030\005 \001"
  "(\010\022\032\n\022place_pruned_graph\030\006 \001(\010\022 \n\030enable"
  "_bfloat16_sendrecv\030\007 \001(\010\022\025\n\rtimeline_ste"
  "p\030\010 \001(\005\0223\n\017rewrite_options\030\n \001(\0132\032.tenso"
  "rflow.RewriterConfigJ\004\010\001\020\002R%skip_common_"
  "subexpression_elimination\"A\n\025ThreadPoolO"
  "ptionProto\022\023\n\013num_threads\030\001 \001(\005\022\023\n\013globa"
  "l_name\030\002 \001(\t\"0\n\017SessionMetadata\022\014\n\004name\030"
  "\001 \001(\t\022\017\n\007version\030\002 \001(\003\"\225\020\n\013ConfigProto\022>"
  "\n\014device_count\030\001 \003(\0132(.tensorflow.Config"
  "Proto.DeviceCountEntry\022$\n\034intra_op_paral"
  "lelism_threads\030\002 \001(\005\022$\n\034inter_op_paralle"
  "lism_threads\030\005 \001(\005\022\037\n\027use_per_session_th"
  "reads\030\t \001(\010\022G\n\034session_inter_op_thread_p"
  "ool\030\014 \003(\0132!.tensorflow.ThreadPoolOptionP"
  "roto\022\030\n\020placement_period\030\003 \001(\005\022\026\n\016device"
  "_filters\030\004 \003(\t\022+\n\013gpu_options\030\006 \001(\0132\026.te"
  "nsorflow.GPUOptions\022\034\n\024allow_soft_placem"
  "ent\030\007 \001(\010\022\034\n\024log_device_placement\030\010 \001(\010\022"
  "/\n\rgraph_options\030\n \001(\0132\030.tensorflow.Grap"
  "hOptions\022\037\n\027operation_timeout_in_ms\030\013 \001("
  "\003\022+\n\013rpc_options\030\r \001(\0132\026.tensorflow.RPCO"
  "ptions\022+\n\013cluster_def\030\016 \001(\0132\026.tensorflow"
  ".ClusterDef\022\035\n\025isolate_session_state\030\017 \001"
  "(\010\022(\n share_cluster_devices_in_session\030\021"
  " \001(\010\022:\n\014experimental\030\020 \001(\0132$.tensorflow."
  "ConfigProto.Experimental\0322\n\020DeviceCountE"
  "ntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\005:\0028\001\032\217\n\n"
  "\014Experimental\022\037\n\027collective_group_leader"
  "\030\001 \001(\t\022\025\n\rexecutor_type\030\003 \001(\t\022\032\n\022recv_bu"
  "f_max_chunk\030\004 \001(\005\022\031\n\021use_numa_affinity\030\005"
  " \001(\010\0225\n-collective_deterministic_sequent"
  "ial_execution\030\006 \001(\010\022\027\n\017collective_nccl\030\007"
  " \001(\010\0226\n.share_session_state_in_clustersp"
  "ec_propagation\030\010 \001(\010\022\037\n\027disable_thread_s"
  "pinning\030\t \001(\010\022(\n share_cluster_devices_i"
  "n_session\030\n \001(\010\0225\n\020session_metadata\030\013 \001("
  "\0132\033.tensorflow.SessionMetadata\022!\n\031optimi"
  "ze_for_static_graph\030\014 \001(\010\022\032\n\022enable_mlir"
  "_bridge\030\r \001(\010\022S\n\023mlir_bridge_rollout\030\021 \001"
  "(\01626.tensorflow.ConfigProto.Experimental"
  ".MlirBridgeRollout\022&\n\036enable_mlir_graph_"
  "optimization\030\020 \001(\010\022\'\n\037disable_output_par"
  "tition_graphs\030\016 \001(\010\022#\n\033xla_fusion_autotu"
  "ner_thresh\030\017 \001(\003\022\020\n\010use_tfrt\030\022 \001(\010\022\031\n\021en"
  "able_multi_host\030\033 \001(\010\022\033\n\023backend_server_"
  "port\030\034 \001(\005\022\022\n\ntarget_tpu\030\035 \001(\010\022\022\n\ntarget"
  "_gpu\030\036 \001(\010\022\036\n\026stream_merge_threshold\030\037 \001"
  "(\005\022\'\n\037disable_functional_ops_lowering\030\025 "
  "\001(\010\022\'\n\037xla_prefer_single_graph_cluster\030\026"
  " \001(\010\022B\n\023coordination_config\030\027 \001(\0132%.tens"
  "orflow.CoordinationServiceConfig\022)\n!disa"
  "ble_optimize_for_static_graph\030\030 \001(\010\0220\n(d"
  "isable_eager_executor_streaming_enqueue\030"
  "\032 \001(\010\"\336\001\n\021MlirBridgeRollout\022#\n\037MLIR_BRID"
  "GE_ROLLOUT_UNSPECIFIED\020\000\022\037\n\033MLIR_BRIDGE_"
  "ROLLOUT_ENABLED\020\001\022 \n\034MLIR_BRIDGE_ROLLOUT"
  "_DISABLED\020\002\"\004\010\003\020\003\"\004\010\004\020\004*%MLIR_BRIDGE_ROL"
  "LOUT_SAFE_MODE_ENABLED*.MLIR_BRIDGE_ROLL"
  "OUT_SAFE_MODE_FALLBACK_ENABLEDJ\004\010\002\020\003J\004\010\023"
  "\020\024J\004\010\024\020\025J\004\010\031\020\032\"\341\004\n\nRunOptions\0226\n\013trace_l"
  "evel\030\001 \001(\0162!.tensorflow.RunOptions.Trace"
  "Level\022\025\n\rtimeout_in_ms\030\002 \001(\003\022\034\n\024inter_op"
  "_thread_pool\030\003 \001(\005\022\037\n\027output_partition_g"
  "raphs\030\005 \001(\010\022/\n\rdebug_options\030\006 \001(\0132\030.ten"
  "sorflow.DebugOptions\022*\n\"report_tensor_al"
  "locations_upon_oom\030\007 \001(\010\0229\n\014experimental"
  "\030\010 \001(\0132#.tensorflow.RunOptions.Experimen"
  "tal\032\322\001\n\014Experimental\022\034\n\024collective_graph"
  "_key\030\001 \001(\003\022\034\n\024use_run_handler_pool\030\002 \001(\010"
  "\022[\n\030run_handler_pool_options\030\003 \001(\01329.ten"
  "sorflow.RunOptions.Experimental.RunHandl"
  "erPoolOptions\032)\n\025RunHandlerPoolOptions\022\020"
  "\n\010priority\030\001 \001(\003\"R\n\nTraceLevel\022\014\n\010NO_TRA"
  "CE\020\000\022\022\n\016SOFTWARE_TRACE\020\001\022\022\n\016HARDWARE_TRA"
  "CE\020\002\022\016\n\nFULL_TRACE\020\003J\004\010\004\020\005\"\276\003\n\013RunMetada"
  "ta\022)\n\nstep_stats\030\001 \001(\0132\025.tensorflow.Step"
  "Stats\022,\n\ncost_graph\030\002 \001(\0132\030.tensorflow.C"
  "ostGraphDef\022.\n\020partition_graphs\030\003 \003(\0132\024."
  "tensorflow.GraphDef\022\?\n\017function_graphs\030\004"
  " \003(\0132&.tensorflow.RunMetadata.FunctionGr"
  "aphs\0225\n\020session_metadata\030\005 \001(\0132\033.tensorf"
  "low.SessionMetadata\032\255\001\n\016FunctionGraphs\022."
  "\n\020partition_graphs\030\001 \003(\0132\024.tensorflow.Gr"
  "aphDef\0224\n\026pre_optimization_graph\030\002 \001(\0132\024"
  ".tensorflow.GraphDef\0225\n\027post_optimizatio"
  "n_graph\030\003 \001(\0132\024.tensorflow.GraphDef\":\n\020T"
  "ensorConnection\022\023\n\013from_tensor\030\001 \001(\t\022\021\n\t"
  "to_tensor\030\002 \001(\t\"\260\003\n\017CallableOptions\022\014\n\004f"
  "eed\030\001 \003(\t\022\r\n\005fetch\030\002 \003(\t\022\016\n\006target\030\003 \003(\t"
  "\022+\n\013run_options\030\004 \001(\0132\026.tensorflow.RunOp"
  "tions\0227\n\021tensor_connection\030\005 \003(\0132\034.tenso"
  "rflow.TensorConnection\022B\n\014feed_devices\030\006"
  " \003(\0132,.tensorflow.CallableOptions.FeedDe"
  "vicesEntry\022D\n\rfetch_devices\030\007 \003(\0132-.tens"
  "orflow.CallableOptions.FetchDevicesEntry"
  "\022\027\n\017fetch_skip_sync\030\010 \001(\010\0322\n\020FeedDevices"
  "Entry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001\0323\n"
  "\021FetchDevicesEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value"
  "\030\002 \001(\t:\0028\001B\204\001\n\030org.tensorflow.frameworkB"
  "\014ConfigProtosP\001ZUgithub.com/tensorflow/t"
  "ensorflow/tensorflow/go/core/protobuf/fo"
  "r_core_protos_go_proto\370\001\001b\006proto3"
  ;
static const ::_pbi::DescriptorTable* const descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_deps[8] = {
  &::descriptor_table_tensorflow_2fcore_2fframework_2fcost_5fgraph_2eproto,
  &::descriptor_table_tensorflow_2fcore_2fframework_2fgraph_2eproto,
  &::descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto,
  &::descriptor_table_tensorflow_2fcore_2fprotobuf_2fcluster_2eproto,
  &::descriptor_table_tensorflow_2fcore_2fprotobuf_2fdebug_2eproto,
  &::descriptor_table_tensorflow_2fcore_2fprotobuf_2frewriter_5fconfig_2eproto,
  &::descriptor_table_tensorflow_2fcore_2fprotobuf_2frpc_5foptions_2eproto,
  &::descriptor_table_tsl_2fprotobuf_2fcoordination_5fconfig_2eproto,
};
static ::_pbi::once_flag descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once;
const ::_pbi::DescriptorTable descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto = {
    false, false, 6353, descriptor_table_protodef_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto,
    "tensorflow/core/protobuf/config.proto",
    &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once, descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_deps, 8, 20,
    schemas, file_default_instances, TableStruct_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto::offsets,
    file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto, file_level_enum_descriptors_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto,
    file_level_service_descriptors_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto,
};
PROTOBUF_ATTRIBUTE_WEAK const ::_pbi::DescriptorTable* descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter() {
  return &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto;
}

// Force running AddDescriptors() at dynamic initialization time.
PROTOBUF_ATTRIBUTE_INIT_PRIORITY2 static ::_pbi::AddDescriptorsRunner dynamic_init_dummy_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto(&descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto);
namespace tensorflow {
const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* OptimizerOptions_Level_descriptor() {
  ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto);
  return file_level_enum_descriptors_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[0];
}
bool OptimizerOptions_Level_IsValid(int value) {
  switch (value) {
    case -1:
    case 0:
      return true;
    default:
      return false;
  }
}

#if (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
constexpr OptimizerOptions_Level OptimizerOptions::L1;
constexpr OptimizerOptions_Level OptimizerOptions::L0;
constexpr OptimizerOptions_Level OptimizerOptions::Level_MIN;
constexpr OptimizerOptions_Level OptimizerOptions::Level_MAX;
constexpr int OptimizerOptions::Level_ARRAYSIZE;
#endif  // (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* OptimizerOptions_GlobalJitLevel_descriptor() {
  ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto);
  return file_level_enum_descriptors_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[1];
}
bool OptimizerOptions_GlobalJitLevel_IsValid(int value) {
  switch (value) {
    case -1:
    case 0:
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
constexpr OptimizerOptions_GlobalJitLevel OptimizerOptions::DEFAULT;
constexpr OptimizerOptions_GlobalJitLevel OptimizerOptions::OFF;
constexpr OptimizerOptions_GlobalJitLevel OptimizerOptions::ON_1;
constexpr OptimizerOptions_GlobalJitLevel OptimizerOptions::ON_2;
constexpr OptimizerOptions_GlobalJitLevel OptimizerOptions::GlobalJitLevel_MIN;
constexpr OptimizerOptions_GlobalJitLevel OptimizerOptions::GlobalJitLevel_MAX;
constexpr int OptimizerOptions::GlobalJitLevel_ARRAYSIZE;
#endif  // (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* ConfigProto_Experimental_MlirBridgeRollout_descriptor() {
  ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto);
  return file_level_enum_descriptors_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[2];
}
bool ConfigProto_Experimental_MlirBridgeRollout_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
constexpr ConfigProto_Experimental_MlirBridgeRollout ConfigProto_Experimental::MLIR_BRIDGE_ROLLOUT_UNSPECIFIED;
constexpr ConfigProto_Experimental_MlirBridgeRollout ConfigProto_Experimental::MLIR_BRIDGE_ROLLOUT_ENABLED;
constexpr ConfigProto_Experimental_MlirBridgeRollout ConfigProto_Experimental::MLIR_BRIDGE_ROLLOUT_DISABLED;
constexpr ConfigProto_Experimental_MlirBridgeRollout ConfigProto_Experimental::MlirBridgeRollout_MIN;
constexpr ConfigProto_Experimental_MlirBridgeRollout ConfigProto_Experimental::MlirBridgeRollout_MAX;
constexpr int ConfigProto_Experimental::MlirBridgeRollout_ARRAYSIZE;
#endif  // (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* RunOptions_TraceLevel_descriptor() {
  ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto);
  return file_level_enum_descriptors_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[3];
}
bool RunOptions_TraceLevel_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
      return true;
    default:
      return false;
  }
}

#if (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
constexpr RunOptions_TraceLevel RunOptions::NO_TRACE;
constexpr RunOptions_TraceLevel RunOptions::SOFTWARE_TRACE;
constexpr RunOptions_TraceLevel RunOptions::HARDWARE_TRACE;
constexpr RunOptions_TraceLevel RunOptions::FULL_TRACE;
constexpr RunOptions_TraceLevel RunOptions::TraceLevel_MIN;
constexpr RunOptions_TraceLevel RunOptions::TraceLevel_MAX;
constexpr int RunOptions::TraceLevel_ARRAYSIZE;
#endif  // (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))

// ===================================================================

class GPUOptions_Experimental_VirtualDevices::_Internal {
 public:
};

GPUOptions_Experimental_VirtualDevices::GPUOptions_Experimental_VirtualDevices(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.GPUOptions.Experimental.VirtualDevices)
}
GPUOptions_Experimental_VirtualDevices::GPUOptions_Experimental_VirtualDevices(const GPUOptions_Experimental_VirtualDevices& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  GPUOptions_Experimental_VirtualDevices* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.memory_limit_mb_){from._impl_.memory_limit_mb_}
    , decltype(_impl_.priority_){from._impl_.priority_}
    , /*decltype(_impl_._priority_cached_byte_size_)*/{0}
    , decltype(_impl_.device_ordinal_){from._impl_.device_ordinal_}
    , /*decltype(_impl_._device_ordinal_cached_byte_size_)*/{0}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:tensorflow.GPUOptions.Experimental.VirtualDevices)
}

inline void GPUOptions_Experimental_VirtualDevices::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.memory_limit_mb_){arena}
    , decltype(_impl_.priority_){arena}
    , /*decltype(_impl_._priority_cached_byte_size_)*/{0}
    , decltype(_impl_.device_ordinal_){arena}
    , /*decltype(_impl_._device_ordinal_cached_byte_size_)*/{0}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

GPUOptions_Experimental_VirtualDevices::~GPUOptions_Experimental_VirtualDevices() {
  // @@protoc_insertion_point(destructor:tensorflow.GPUOptions.Experimental.VirtualDevices)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void GPUOptions_Experimental_VirtualDevices::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.memory_limit_mb_.~RepeatedField();
  _impl_.priority_.~RepeatedField();
  _impl_.device_ordinal_.~RepeatedField();
}

void GPUOptions_Experimental_VirtualDevices::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void GPUOptions_Experimental_VirtualDevices::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.GPUOptions.Experimental.VirtualDevices)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.memory_limit_mb_.Clear();
  _impl_.priority_.Clear();
  _impl_.device_ordinal_.Clear();
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* GPUOptions_Experimental_VirtualDevices::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // repeated float memory_limit_mb = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::PackedFloatParser(_internal_mutable_memory_limit_mb(), ptr, ctx);
          CHK_(ptr);
        } else if (static_cast<uint8_t>(tag) == 13) {
          _internal_add_memory_limit_mb(::PROTOBUF_NAMESPACE_ID::internal::UnalignedLoad<float>(ptr));
          ptr += sizeof(float);
        } else
          goto handle_unusual;
        continue;
      // repeated int32 priority = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::PackedInt32Parser(_internal_mutable_priority(), ptr, ctx);
          CHK_(ptr);
        } else if (static_cast<uint8_t>(tag) == 16) {
          _internal_add_priority(::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr));
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // repeated int32 device_ordinal = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::PackedInt32Parser(_internal_mutable_device_ordinal(), ptr, ctx);
          CHK_(ptr);
        } else if (static_cast<uint8_t>(tag) == 24) {
          _internal_add_device_ordinal(::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr));
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* GPUOptions_Experimental_VirtualDevices::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.GPUOptions.Experimental.VirtualDevices)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated float memory_limit_mb = 1;
  if (this->_internal_memory_limit_mb_size() > 0) {
    target = stream->WriteFixedPacked(1, _internal_memory_limit_mb(), target);
  }

  // repeated int32 priority = 2;
  {
    int byte_size = _impl_._priority_cached_byte_size_.load(std::memory_order_relaxed);
    if (byte_size > 0) {
      target = stream->WriteInt32Packed(
          2, _internal_priority(), byte_size, target);
    }
  }

  // repeated int32 device_ordinal = 3;
  {
    int byte_size = _impl_._device_ordinal_cached_byte_size_.load(std::memory_order_relaxed);
    if (byte_size > 0) {
      target = stream->WriteInt32Packed(
          3, _internal_device_ordinal(), byte_size, target);
    }
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.GPUOptions.Experimental.VirtualDevices)
  return target;
}

size_t GPUOptions_Experimental_VirtualDevices::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.GPUOptions.Experimental.VirtualDevices)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated float memory_limit_mb = 1;
  {
    unsigned int count = static_cast<unsigned int>(this->_internal_memory_limit_mb_size());
    size_t data_size = 4UL * count;
    if (data_size > 0) {
      total_size += 1 +
        ::_pbi::WireFormatLite::Int32Size(static_cast<int32_t>(data_size));
    }
    total_size += data_size;
  }

  // repeated int32 priority = 2;
  {
    size_t data_size = ::_pbi::WireFormatLite::
      Int32Size(this->_impl_.priority_);
    if (data_size > 0) {
      total_size += 1 +
        ::_pbi::WireFormatLite::Int32Size(static_cast<int32_t>(data_size));
    }
    int cached_size = ::_pbi::ToCachedSize(data_size);
    _impl_._priority_cached_byte_size_.store(cached_size,
                                    std::memory_order_relaxed);
    total_size += data_size;
  }

  // repeated int32 device_ordinal = 3;
  {
    size_t data_size = ::_pbi::WireFormatLite::
      Int32Size(this->_impl_.device_ordinal_);
    if (data_size > 0) {
      total_size += 1 +
        ::_pbi::WireFormatLite::Int32Size(static_cast<int32_t>(data_size));
    }
    int cached_size = ::_pbi::ToCachedSize(data_size);
    _impl_._device_ordinal_cached_byte_size_.store(cached_size,
                                    std::memory_order_relaxed);
    total_size += data_size;
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData GPUOptions_Experimental_VirtualDevices::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    GPUOptions_Experimental_VirtualDevices::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GPUOptions_Experimental_VirtualDevices::GetClassData() const { return &_class_data_; }


void GPUOptions_Experimental_VirtualDevices::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<GPUOptions_Experimental_VirtualDevices*>(&to_msg);
  auto& from = static_cast<const GPUOptions_Experimental_VirtualDevices&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.GPUOptions.Experimental.VirtualDevices)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.memory_limit_mb_.MergeFrom(from._impl_.memory_limit_mb_);
  _this->_impl_.priority_.MergeFrom(from._impl_.priority_);
  _this->_impl_.device_ordinal_.MergeFrom(from._impl_.device_ordinal_);
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void GPUOptions_Experimental_VirtualDevices::CopyFrom(const GPUOptions_Experimental_VirtualDevices& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.GPUOptions.Experimental.VirtualDevices)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GPUOptions_Experimental_VirtualDevices::IsInitialized() const {
  return true;
}

void GPUOptions_Experimental_VirtualDevices::InternalSwap(GPUOptions_Experimental_VirtualDevices* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.memory_limit_mb_.InternalSwap(&other->_impl_.memory_limit_mb_);
  _impl_.priority_.InternalSwap(&other->_impl_.priority_);
  _impl_.device_ordinal_.InternalSwap(&other->_impl_.device_ordinal_);
}

::PROTOBUF_NAMESPACE_ID::Metadata GPUOptions_Experimental_VirtualDevices::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[0]);
}

// ===================================================================

class GPUOptions_Experimental_StreamMergeOptions::_Internal {
 public:
};

GPUOptions_Experimental_StreamMergeOptions::GPUOptions_Experimental_StreamMergeOptions(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
}
GPUOptions_Experimental_StreamMergeOptions::GPUOptions_Experimental_StreamMergeOptions(const GPUOptions_Experimental_StreamMergeOptions& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  GPUOptions_Experimental_StreamMergeOptions* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.merge_host_to_device_stream_){}
    , decltype(_impl_.merge_device_to_host_stream_){}
    , decltype(_impl_.merge_device_to_device_stream_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  ::memcpy(&_impl_.merge_host_to_device_stream_, &from._impl_.merge_host_to_device_stream_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.merge_device_to_device_stream_) -
    reinterpret_cast<char*>(&_impl_.merge_host_to_device_stream_)) + sizeof(_impl_.merge_device_to_device_stream_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
}

inline void GPUOptions_Experimental_StreamMergeOptions::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.merge_host_to_device_stream_){false}
    , decltype(_impl_.merge_device_to_host_stream_){false}
    , decltype(_impl_.merge_device_to_device_stream_){false}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

GPUOptions_Experimental_StreamMergeOptions::~GPUOptions_Experimental_StreamMergeOptions() {
  // @@protoc_insertion_point(destructor:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void GPUOptions_Experimental_StreamMergeOptions::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
}

void GPUOptions_Experimental_StreamMergeOptions::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void GPUOptions_Experimental_StreamMergeOptions::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  ::memset(&_impl_.merge_host_to_device_stream_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.merge_device_to_device_stream_) -
      reinterpret_cast<char*>(&_impl_.merge_host_to_device_stream_)) + sizeof(_impl_.merge_device_to_device_stream_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* GPUOptions_Experimental_StreamMergeOptions::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // bool merge_host_to_device_stream = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          _impl_.merge_host_to_device_stream_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool merge_device_to_host_stream = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.merge_device_to_host_stream_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool merge_device_to_device_stream = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          _impl_.merge_device_to_device_stream_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* GPUOptions_Experimental_StreamMergeOptions::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // bool merge_host_to_device_stream = 1;
  if (this->_internal_merge_host_to_device_stream() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(1, this->_internal_merge_host_to_device_stream(), target);
  }

  // bool merge_device_to_host_stream = 2;
  if (this->_internal_merge_device_to_host_stream() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(2, this->_internal_merge_device_to_host_stream(), target);
  }

  // bool merge_device_to_device_stream = 3;
  if (this->_internal_merge_device_to_device_stream() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(3, this->_internal_merge_device_to_device_stream(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
  return target;
}

size_t GPUOptions_Experimental_StreamMergeOptions::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // bool merge_host_to_device_stream = 1;
  if (this->_internal_merge_host_to_device_stream() != 0) {
    total_size += 1 + 1;
  }

  // bool merge_device_to_host_stream = 2;
  if (this->_internal_merge_device_to_host_stream() != 0) {
    total_size += 1 + 1;
  }

  // bool merge_device_to_device_stream = 3;
  if (this->_internal_merge_device_to_device_stream() != 0) {
    total_size += 1 + 1;
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData GPUOptions_Experimental_StreamMergeOptions::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    GPUOptions_Experimental_StreamMergeOptions::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GPUOptions_Experimental_StreamMergeOptions::GetClassData() const { return &_class_data_; }


void GPUOptions_Experimental_StreamMergeOptions::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<GPUOptions_Experimental_StreamMergeOptions*>(&to_msg);
  auto& from = static_cast<const GPUOptions_Experimental_StreamMergeOptions&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_merge_host_to_device_stream() != 0) {
    _this->_internal_set_merge_host_to_device_stream(from._internal_merge_host_to_device_stream());
  }
  if (from._internal_merge_device_to_host_stream() != 0) {
    _this->_internal_set_merge_device_to_host_stream(from._internal_merge_device_to_host_stream());
  }
  if (from._internal_merge_device_to_device_stream() != 0) {
    _this->_internal_set_merge_device_to_device_stream(from._internal_merge_device_to_device_stream());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void GPUOptions_Experimental_StreamMergeOptions::CopyFrom(const GPUOptions_Experimental_StreamMergeOptions& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.GPUOptions.Experimental.StreamMergeOptions)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GPUOptions_Experimental_StreamMergeOptions::IsInitialized() const {
  return true;
}

void GPUOptions_Experimental_StreamMergeOptions::InternalSwap(GPUOptions_Experimental_StreamMergeOptions* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(GPUOptions_Experimental_StreamMergeOptions, _impl_.merge_device_to_device_stream_)
      + sizeof(GPUOptions_Experimental_StreamMergeOptions::_impl_.merge_device_to_device_stream_)
      - PROTOBUF_FIELD_OFFSET(GPUOptions_Experimental_StreamMergeOptions, _impl_.merge_host_to_device_stream_)>(
          reinterpret_cast<char*>(&_impl_.merge_host_to_device_stream_),
          reinterpret_cast<char*>(&other->_impl_.merge_host_to_device_stream_));
}

::PROTOBUF_NAMESPACE_ID::Metadata GPUOptions_Experimental_StreamMergeOptions::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[1]);
}

// ===================================================================

class GPUOptions_Experimental::_Internal {
 public:
  static const ::tensorflow::GPUOptions_Experimental_StreamMergeOptions& stream_merge_options(const GPUOptions_Experimental* msg);
};

const ::tensorflow::GPUOptions_Experimental_StreamMergeOptions&
GPUOptions_Experimental::_Internal::stream_merge_options(const GPUOptions_Experimental* msg) {
  return *msg->_impl_.stream_merge_options_;
}
GPUOptions_Experimental::GPUOptions_Experimental(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.GPUOptions.Experimental)
}
GPUOptions_Experimental::GPUOptions_Experimental(const GPUOptions_Experimental& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  GPUOptions_Experimental* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.virtual_devices_){from._impl_.virtual_devices_}
    , decltype(_impl_.collective_ring_order_){}
    , decltype(_impl_.stream_merge_options_){nullptr}
    , decltype(_impl_.num_dev_to_dev_copy_streams_){}
    , decltype(_impl_.kernel_tracker_max_interval_){}
    , decltype(_impl_.use_unified_memory_){}
    , decltype(_impl_.timestamped_allocator_){}
    , decltype(_impl_.use_cuda_malloc_async_){}
    , decltype(_impl_.disallow_retry_on_allocation_failure_){}
    , decltype(_impl_.kernel_tracker_max_bytes_){}
    , decltype(_impl_.internal_fragmentation_fraction_){}
    , decltype(_impl_.kernel_tracker_max_pending_){}
    , decltype(_impl_.gpu_host_mem_limit_in_mb_){}
    , decltype(_impl_.num_virtual_devices_per_gpu_){}
    , decltype(_impl_.gpu_host_mem_disallow_growth_){}
    , decltype(_impl_.populate_pjrt_gpu_client_creation_info_){}
    , decltype(_impl_.gpu_system_memory_size_in_mb_){}
    , decltype(_impl_.node_id_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _impl_.collective_ring_order_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.collective_ring_order_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_collective_ring_order().empty()) {
    _this->_impl_.collective_ring_order_.Set(from._internal_collective_ring_order(), 
      _this->GetArenaForAllocation());
  }
  if (from._internal_has_stream_merge_options()) {
    _this->_impl_.stream_merge_options_ = new ::tensorflow::GPUOptions_Experimental_StreamMergeOptions(*from._impl_.stream_merge_options_);
  }
  ::memcpy(&_impl_.num_dev_to_dev_copy_streams_, &from._impl_.num_dev_to_dev_copy_streams_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.node_id_) -
    reinterpret_cast<char*>(&_impl_.num_dev_to_dev_copy_streams_)) + sizeof(_impl_.node_id_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.GPUOptions.Experimental)
}

inline void GPUOptions_Experimental::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.virtual_devices_){arena}
    , decltype(_impl_.collective_ring_order_){}
    , decltype(_impl_.stream_merge_options_){nullptr}
    , decltype(_impl_.num_dev_to_dev_copy_streams_){0}
    , decltype(_impl_.kernel_tracker_max_interval_){0}
    , decltype(_impl_.use_unified_memory_){false}
    , decltype(_impl_.timestamped_allocator_){false}
    , decltype(_impl_.use_cuda_malloc_async_){false}
    , decltype(_impl_.disallow_retry_on_allocation_failure_){false}
    , decltype(_impl_.kernel_tracker_max_bytes_){0}
    , decltype(_impl_.internal_fragmentation_fraction_){0}
    , decltype(_impl_.kernel_tracker_max_pending_){0}
    , decltype(_impl_.gpu_host_mem_limit_in_mb_){0}
    , decltype(_impl_.num_virtual_devices_per_gpu_){0}
    , decltype(_impl_.gpu_host_mem_disallow_growth_){false}
    , decltype(_impl_.populate_pjrt_gpu_client_creation_info_){false}
    , decltype(_impl_.gpu_system_memory_size_in_mb_){0}
    , decltype(_impl_.node_id_){0}
    , /*decltype(_impl_._cached_size_)*/{}
  };
  _impl_.collective_ring_order_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.collective_ring_order_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
}

GPUOptions_Experimental::~GPUOptions_Experimental() {
  // @@protoc_insertion_point(destructor:tensorflow.GPUOptions.Experimental)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void GPUOptions_Experimental::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.virtual_devices_.~RepeatedPtrField();
  _impl_.collective_ring_order_.Destroy();
  if (this != internal_default_instance()) delete _impl_.stream_merge_options_;
}

void GPUOptions_Experimental::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void GPUOptions_Experimental::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.GPUOptions.Experimental)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.virtual_devices_.Clear();
  _impl_.collective_ring_order_.ClearToEmpty();
  if (GetArenaForAllocation() == nullptr && _impl_.stream_merge_options_ != nullptr) {
    delete _impl_.stream_merge_options_;
  }
  _impl_.stream_merge_options_ = nullptr;
  ::memset(&_impl_.num_dev_to_dev_copy_streams_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.node_id_) -
      reinterpret_cast<char*>(&_impl_.num_dev_to_dev_copy_streams_)) + sizeof(_impl_.node_id_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* GPUOptions_Experimental::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_virtual_devices(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<10>(ptr));
        } else
          goto handle_unusual;
        continue;
      // bool use_unified_memory = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.use_unified_memory_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 num_dev_to_dev_copy_streams = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          _impl_.num_dev_to_dev_copy_streams_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // string collective_ring_order = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 34)) {
          auto str = _internal_mutable_collective_ring_order();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.GPUOptions.Experimental.collective_ring_order"));
        } else
          goto handle_unusual;
        continue;
      // bool timestamped_allocator = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 40)) {
          _impl_.timestamped_allocator_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 kernel_tracker_max_interval = 7;
      case 7:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 56)) {
          _impl_.kernel_tracker_max_interval_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 kernel_tracker_max_bytes = 8;
      case 8:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 64)) {
          _impl_.kernel_tracker_max_bytes_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 kernel_tracker_max_pending = 9;
      case 9:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 72)) {
          _impl_.kernel_tracker_max_pending_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // double internal_fragmentation_fraction = 10;
      case 10:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 81)) {
          _impl_.internal_fragmentation_fraction_ = ::PROTOBUF_NAMESPACE_ID::internal::UnalignedLoad<double>(ptr);
          ptr += sizeof(double);
        } else
          goto handle_unusual;
        continue;
      // bool use_cuda_malloc_async = 11;
      case 11:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 88)) {
          _impl_.use_cuda_malloc_async_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool disallow_retry_on_allocation_failure = 12;
      case 12:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 96)) {
          _impl_.disallow_retry_on_allocation_failure_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // float gpu_host_mem_limit_in_mb = 13;
      case 13:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 109)) {
          _impl_.gpu_host_mem_limit_in_mb_ = ::PROTOBUF_NAMESPACE_ID::internal::UnalignedLoad<float>(ptr);
          ptr += sizeof(float);
        } else
          goto handle_unusual;
        continue;
      // bool gpu_host_mem_disallow_growth = 14;
      case 14:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 112)) {
          _impl_.gpu_host_mem_disallow_growth_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 num_virtual_devices_per_gpu = 15;
      case 15:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 120)) {
          _impl_.num_virtual_devices_per_gpu_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 gpu_system_memory_size_in_mb = 16;
      case 16:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 128)) {
          _impl_.gpu_system_memory_size_in_mb_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool populate_pjrt_gpu_client_creation_info = 17;
      case 17:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 136)) {
          _impl_.populate_pjrt_gpu_client_creation_info_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 node_id = 18;
      case 18:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 144)) {
          _impl_.node_id_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;
      case 19:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 154)) {
          ptr = ctx->ParseMessage(_internal_mutable_stream_merge_options(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* GPUOptions_Experimental::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.GPUOptions.Experimental)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_virtual_devices_size()); i < n; i++) {
    const auto& repfield = this->_internal_virtual_devices(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(1, repfield, repfield.GetCachedSize(), target, stream);
  }

  // bool use_unified_memory = 2;
  if (this->_internal_use_unified_memory() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(2, this->_internal_use_unified_memory(), target);
  }

  // int32 num_dev_to_dev_copy_streams = 3;
  if (this->_internal_num_dev_to_dev_copy_streams() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(3, this->_internal_num_dev_to_dev_copy_streams(), target);
  }

  // string collective_ring_order = 4;
  if (!this->_internal_collective_ring_order().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_collective_ring_order().data(), static_cast<int>(this->_internal_collective_ring_order().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.GPUOptions.Experimental.collective_ring_order");
    target = stream->WriteStringMaybeAliased(
        4, this->_internal_collective_ring_order(), target);
  }

  // bool timestamped_allocator = 5;
  if (this->_internal_timestamped_allocator() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(5, this->_internal_timestamped_allocator(), target);
  }

  // int32 kernel_tracker_max_interval = 7;
  if (this->_internal_kernel_tracker_max_interval() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(7, this->_internal_kernel_tracker_max_interval(), target);
  }

  // int32 kernel_tracker_max_bytes = 8;
  if (this->_internal_kernel_tracker_max_bytes() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(8, this->_internal_kernel_tracker_max_bytes(), target);
  }

  // int32 kernel_tracker_max_pending = 9;
  if (this->_internal_kernel_tracker_max_pending() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(9, this->_internal_kernel_tracker_max_pending(), target);
  }

  // double internal_fragmentation_fraction = 10;
  static_assert(sizeof(uint64_t) == sizeof(double), "Code assumes uint64_t and double are the same size.");
  double tmp_internal_fragmentation_fraction = this->_internal_internal_fragmentation_fraction();
  uint64_t raw_internal_fragmentation_fraction;
  memcpy(&raw_internal_fragmentation_fraction, &tmp_internal_fragmentation_fraction, sizeof(tmp_internal_fragmentation_fraction));
  if (raw_internal_fragmentation_fraction != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteDoubleToArray(10, this->_internal_internal_fragmentation_fraction(), target);
  }

  // bool use_cuda_malloc_async = 11;
  if (this->_internal_use_cuda_malloc_async() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(11, this->_internal_use_cuda_malloc_async(), target);
  }

  // bool disallow_retry_on_allocation_failure = 12;
  if (this->_internal_disallow_retry_on_allocation_failure() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(12, this->_internal_disallow_retry_on_allocation_failure(), target);
  }

  // float gpu_host_mem_limit_in_mb = 13;
  static_assert(sizeof(uint32_t) == sizeof(float), "Code assumes uint32_t and float are the same size.");
  float tmp_gpu_host_mem_limit_in_mb = this->_internal_gpu_host_mem_limit_in_mb();
  uint32_t raw_gpu_host_mem_limit_in_mb;
  memcpy(&raw_gpu_host_mem_limit_in_mb, &tmp_gpu_host_mem_limit_in_mb, sizeof(tmp_gpu_host_mem_limit_in_mb));
  if (raw_gpu_host_mem_limit_in_mb != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteFloatToArray(13, this->_internal_gpu_host_mem_limit_in_mb(), target);
  }

  // bool gpu_host_mem_disallow_growth = 14;
  if (this->_internal_gpu_host_mem_disallow_growth() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(14, this->_internal_gpu_host_mem_disallow_growth(), target);
  }

  // int32 num_virtual_devices_per_gpu = 15;
  if (this->_internal_num_virtual_devices_per_gpu() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(15, this->_internal_num_virtual_devices_per_gpu(), target);
  }

  // int32 gpu_system_memory_size_in_mb = 16;
  if (this->_internal_gpu_system_memory_size_in_mb() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(16, this->_internal_gpu_system_memory_size_in_mb(), target);
  }

  // bool populate_pjrt_gpu_client_creation_info = 17;
  if (this->_internal_populate_pjrt_gpu_client_creation_info() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(17, this->_internal_populate_pjrt_gpu_client_creation_info(), target);
  }

  // int32 node_id = 18;
  if (this->_internal_node_id() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(18, this->_internal_node_id(), target);
  }

  // .tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;
  if (this->_internal_has_stream_merge_options()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(19, _Internal::stream_merge_options(this),
        _Internal::stream_merge_options(this).GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.GPUOptions.Experimental)
  return target;
}

size_t GPUOptions_Experimental::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.GPUOptions.Experimental)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;
  total_size += 1UL * this->_internal_virtual_devices_size();
  for (const auto& msg : this->_impl_.virtual_devices_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // string collective_ring_order = 4;
  if (!this->_internal_collective_ring_order().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_collective_ring_order());
  }

  // .tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;
  if (this->_internal_has_stream_merge_options()) {
    total_size += 2 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.stream_merge_options_);
  }

  // int32 num_dev_to_dev_copy_streams = 3;
  if (this->_internal_num_dev_to_dev_copy_streams() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_num_dev_to_dev_copy_streams());
  }

  // int32 kernel_tracker_max_interval = 7;
  if (this->_internal_kernel_tracker_max_interval() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_kernel_tracker_max_interval());
  }

  // bool use_unified_memory = 2;
  if (this->_internal_use_unified_memory() != 0) {
    total_size += 1 + 1;
  }

  // bool timestamped_allocator = 5;
  if (this->_internal_timestamped_allocator() != 0) {
    total_size += 1 + 1;
  }

  // bool use_cuda_malloc_async = 11;
  if (this->_internal_use_cuda_malloc_async() != 0) {
    total_size += 1 + 1;
  }

  // bool disallow_retry_on_allocation_failure = 12;
  if (this->_internal_disallow_retry_on_allocation_failure() != 0) {
    total_size += 1 + 1;
  }

  // int32 kernel_tracker_max_bytes = 8;
  if (this->_internal_kernel_tracker_max_bytes() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_kernel_tracker_max_bytes());
  }

  // double internal_fragmentation_fraction = 10;
  static_assert(sizeof(uint64_t) == sizeof(double), "Code assumes uint64_t and double are the same size.");
  double tmp_internal_fragmentation_fraction = this->_internal_internal_fragmentation_fraction();
  uint64_t raw_internal_fragmentation_fraction;
  memcpy(&raw_internal_fragmentation_fraction, &tmp_internal_fragmentation_fraction, sizeof(tmp_internal_fragmentation_fraction));
  if (raw_internal_fragmentation_fraction != 0) {
    total_size += 1 + 8;
  }

  // int32 kernel_tracker_max_pending = 9;
  if (this->_internal_kernel_tracker_max_pending() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_kernel_tracker_max_pending());
  }

  // float gpu_host_mem_limit_in_mb = 13;
  static_assert(sizeof(uint32_t) == sizeof(float), "Code assumes uint32_t and float are the same size.");
  float tmp_gpu_host_mem_limit_in_mb = this->_internal_gpu_host_mem_limit_in_mb();
  uint32_t raw_gpu_host_mem_limit_in_mb;
  memcpy(&raw_gpu_host_mem_limit_in_mb, &tmp_gpu_host_mem_limit_in_mb, sizeof(tmp_gpu_host_mem_limit_in_mb));
  if (raw_gpu_host_mem_limit_in_mb != 0) {
    total_size += 1 + 4;
  }

  // int32 num_virtual_devices_per_gpu = 15;
  if (this->_internal_num_virtual_devices_per_gpu() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_num_virtual_devices_per_gpu());
  }

  // bool gpu_host_mem_disallow_growth = 14;
  if (this->_internal_gpu_host_mem_disallow_growth() != 0) {
    total_size += 1 + 1;
  }

  // bool populate_pjrt_gpu_client_creation_info = 17;
  if (this->_internal_populate_pjrt_gpu_client_creation_info() != 0) {
    total_size += 2 + 1;
  }

  // int32 gpu_system_memory_size_in_mb = 16;
  if (this->_internal_gpu_system_memory_size_in_mb() != 0) {
    total_size += 2 +
      ::_pbi::WireFormatLite::Int32Size(
        this->_internal_gpu_system_memory_size_in_mb());
  }

  // int32 node_id = 18;
  if (this->_internal_node_id() != 0) {
    total_size += 2 +
      ::_pbi::WireFormatLite::Int32Size(
        this->_internal_node_id());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData GPUOptions_Experimental::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    GPUOptions_Experimental::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GPUOptions_Experimental::GetClassData() const { return &_class_data_; }


void GPUOptions_Experimental::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<GPUOptions_Experimental*>(&to_msg);
  auto& from = static_cast<const GPUOptions_Experimental&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.GPUOptions.Experimental)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.virtual_devices_.MergeFrom(from._impl_.virtual_devices_);
  if (!from._internal_collective_ring_order().empty()) {
    _this->_internal_set_collective_ring_order(from._internal_collective_ring_order());
  }
  if (from._internal_has_stream_merge_options()) {
    _this->_internal_mutable_stream_merge_options()->::tensorflow::GPUOptions_Experimental_StreamMergeOptions::MergeFrom(
        from._internal_stream_merge_options());
  }
  if (from._internal_num_dev_to_dev_copy_streams() != 0) {
    _this->_internal_set_num_dev_to_dev_copy_streams(from._internal_num_dev_to_dev_copy_streams());
  }
  if (from._internal_kernel_tracker_max_interval() != 0) {
    _this->_internal_set_kernel_tracker_max_interval(from._internal_kernel_tracker_max_interval());
  }
  if (from._internal_use_unified_memory() != 0) {
    _this->_internal_set_use_unified_memory(from._internal_use_unified_memory());
  }
  if (from._internal_timestamped_allocator() != 0) {
    _this->_internal_set_timestamped_allocator(from._internal_timestamped_allocator());
  }
  if (from._internal_use_cuda_malloc_async() != 0) {
    _this->_internal_set_use_cuda_malloc_async(from._internal_use_cuda_malloc_async());
  }
  if (from._internal_disallow_retry_on_allocation_failure() != 0) {
    _this->_internal_set_disallow_retry_on_allocation_failure(from._internal_disallow_retry_on_allocation_failure());
  }
  if (from._internal_kernel_tracker_max_bytes() != 0) {
    _this->_internal_set_kernel_tracker_max_bytes(from._internal_kernel_tracker_max_bytes());
  }
  static_assert(sizeof(uint64_t) == sizeof(double), "Code assumes uint64_t and double are the same size.");
  double tmp_internal_fragmentation_fraction = from._internal_internal_fragmentation_fraction();
  uint64_t raw_internal_fragmentation_fraction;
  memcpy(&raw_internal_fragmentation_fraction, &tmp_internal_fragmentation_fraction, sizeof(tmp_internal_fragmentation_fraction));
  if (raw_internal_fragmentation_fraction != 0) {
    _this->_internal_set_internal_fragmentation_fraction(from._internal_internal_fragmentation_fraction());
  }
  if (from._internal_kernel_tracker_max_pending() != 0) {
    _this->_internal_set_kernel_tracker_max_pending(from._internal_kernel_tracker_max_pending());
  }
  static_assert(sizeof(uint32_t) == sizeof(float), "Code assumes uint32_t and float are the same size.");
  float tmp_gpu_host_mem_limit_in_mb = from._internal_gpu_host_mem_limit_in_mb();
  uint32_t raw_gpu_host_mem_limit_in_mb;
  memcpy(&raw_gpu_host_mem_limit_in_mb, &tmp_gpu_host_mem_limit_in_mb, sizeof(tmp_gpu_host_mem_limit_in_mb));
  if (raw_gpu_host_mem_limit_in_mb != 0) {
    _this->_internal_set_gpu_host_mem_limit_in_mb(from._internal_gpu_host_mem_limit_in_mb());
  }
  if (from._internal_num_virtual_devices_per_gpu() != 0) {
    _this->_internal_set_num_virtual_devices_per_gpu(from._internal_num_virtual_devices_per_gpu());
  }
  if (from._internal_gpu_host_mem_disallow_growth() != 0) {
    _this->_internal_set_gpu_host_mem_disallow_growth(from._internal_gpu_host_mem_disallow_growth());
  }
  if (from._internal_populate_pjrt_gpu_client_creation_info() != 0) {
    _this->_internal_set_populate_pjrt_gpu_client_creation_info(from._internal_populate_pjrt_gpu_client_creation_info());
  }
  if (from._internal_gpu_system_memory_size_in_mb() != 0) {
    _this->_internal_set_gpu_system_memory_size_in_mb(from._internal_gpu_system_memory_size_in_mb());
  }
  if (from._internal_node_id() != 0) {
    _this->_internal_set_node_id(from._internal_node_id());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void GPUOptions_Experimental::CopyFrom(const GPUOptions_Experimental& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.GPUOptions.Experimental)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GPUOptions_Experimental::IsInitialized() const {
  return true;
}

void GPUOptions_Experimental::InternalSwap(GPUOptions_Experimental* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.virtual_devices_.InternalSwap(&other->_impl_.virtual_devices_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.collective_ring_order_, lhs_arena,
      &other->_impl_.collective_ring_order_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(GPUOptions_Experimental, _impl_.node_id_)
      + sizeof(GPUOptions_Experimental::_impl_.node_id_)
      - PROTOBUF_FIELD_OFFSET(GPUOptions_Experimental, _impl_.stream_merge_options_)>(
          reinterpret_cast<char*>(&_impl_.stream_merge_options_),
          reinterpret_cast<char*>(&other->_impl_.stream_merge_options_));
}

::PROTOBUF_NAMESPACE_ID::Metadata GPUOptions_Experimental::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[2]);
}

// ===================================================================

class GPUOptions::_Internal {
 public:
  static const ::tensorflow::GPUOptions_Experimental& experimental(const GPUOptions* msg);
};

const ::tensorflow::GPUOptions_Experimental&
GPUOptions::_Internal::experimental(const GPUOptions* msg) {
  return *msg->_impl_.experimental_;
}
GPUOptions::GPUOptions(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.GPUOptions)
}
GPUOptions::GPUOptions(const GPUOptions& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  GPUOptions* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.allocator_type_){}
    , decltype(_impl_.visible_device_list_){}
    , decltype(_impl_.experimental_){nullptr}
    , decltype(_impl_.per_process_gpu_memory_fraction_){}
    , decltype(_impl_.deferred_deletion_bytes_){}
    , decltype(_impl_.polling_active_delay_usecs_){}
    , decltype(_impl_.allow_growth_){}
    , decltype(_impl_.force_gpu_compatible_){}
    , decltype(_impl_.polling_inactive_delay_msecs_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _impl_.allocator_type_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.allocator_type_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_allocator_type().empty()) {
    _this->_impl_.allocator_type_.Set(from._internal_allocator_type(), 
      _this->GetArenaForAllocation());
  }
  _impl_.visible_device_list_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.visible_device_list_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_visible_device_list().empty()) {
    _this->_impl_.visible_device_list_.Set(from._internal_visible_device_list(), 
      _this->GetArenaForAllocation());
  }
  if (from._internal_has_experimental()) {
    _this->_impl_.experimental_ = new ::tensorflow::GPUOptions_Experimental(*from._impl_.experimental_);
  }
  ::memcpy(&_impl_.per_process_gpu_memory_fraction_, &from._impl_.per_process_gpu_memory_fraction_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.polling_inactive_delay_msecs_) -
    reinterpret_cast<char*>(&_impl_.per_process_gpu_memory_fraction_)) + sizeof(_impl_.polling_inactive_delay_msecs_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.GPUOptions)
}

inline void GPUOptions::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.allocator_type_){}
    , decltype(_impl_.visible_device_list_){}
    , decltype(_impl_.experimental_){nullptr}
    , decltype(_impl_.per_process_gpu_memory_fraction_){0}
    , decltype(_impl_.deferred_deletion_bytes_){int64_t{0}}
    , decltype(_impl_.polling_active_delay_usecs_){0}
    , decltype(_impl_.allow_growth_){false}
    , decltype(_impl_.force_gpu_compatible_){false}
    , decltype(_impl_.polling_inactive_delay_msecs_){0}
    , /*decltype(_impl_._cached_size_)*/{}
  };
  _impl_.allocator_type_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.allocator_type_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  _impl_.visible_device_list_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.visible_device_list_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
}

GPUOptions::~GPUOptions() {
  // @@protoc_insertion_point(destructor:tensorflow.GPUOptions)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void GPUOptions::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.allocator_type_.Destroy();
  _impl_.visible_device_list_.Destroy();
  if (this != internal_default_instance()) delete _impl_.experimental_;
}

void GPUOptions::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void GPUOptions::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.GPUOptions)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.allocator_type_.ClearToEmpty();
  _impl_.visible_device_list_.ClearToEmpty();
  if (GetArenaForAllocation() == nullptr && _impl_.experimental_ != nullptr) {
    delete _impl_.experimental_;
  }
  _impl_.experimental_ = nullptr;
  ::memset(&_impl_.per_process_gpu_memory_fraction_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.polling_inactive_delay_msecs_) -
      reinterpret_cast<char*>(&_impl_.per_process_gpu_memory_fraction_)) + sizeof(_impl_.polling_inactive_delay_msecs_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* GPUOptions::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // double per_process_gpu_memory_fraction = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 9)) {
          _impl_.per_process_gpu_memory_fraction_ = ::PROTOBUF_NAMESPACE_ID::internal::UnalignedLoad<double>(ptr);
          ptr += sizeof(double);
        } else
          goto handle_unusual;
        continue;
      // string allocator_type = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          auto str = _internal_mutable_allocator_type();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.GPUOptions.allocator_type"));
        } else
          goto handle_unusual;
        continue;
      // int64 deferred_deletion_bytes = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          _impl_.deferred_deletion_bytes_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool allow_growth = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 32)) {
          _impl_.allow_growth_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // string visible_device_list = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 42)) {
          auto str = _internal_mutable_visible_device_list();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.GPUOptions.visible_device_list"));
        } else
          goto handle_unusual;
        continue;
      // int32 polling_active_delay_usecs = 6;
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 48)) {
          _impl_.polling_active_delay_usecs_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 polling_inactive_delay_msecs = 7;
      case 7:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 56)) {
          _impl_.polling_inactive_delay_msecs_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool force_gpu_compatible = 8;
      case 8:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 64)) {
          _impl_.force_gpu_compatible_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.GPUOptions.Experimental experimental = 9;
      case 9:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 74)) {
          ptr = ctx->ParseMessage(_internal_mutable_experimental(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* GPUOptions::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.GPUOptions)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // double per_process_gpu_memory_fraction = 1;
  static_assert(sizeof(uint64_t) == sizeof(double), "Code assumes uint64_t and double are the same size.");
  double tmp_per_process_gpu_memory_fraction = this->_internal_per_process_gpu_memory_fraction();
  uint64_t raw_per_process_gpu_memory_fraction;
  memcpy(&raw_per_process_gpu_memory_fraction, &tmp_per_process_gpu_memory_fraction, sizeof(tmp_per_process_gpu_memory_fraction));
  if (raw_per_process_gpu_memory_fraction != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteDoubleToArray(1, this->_internal_per_process_gpu_memory_fraction(), target);
  }

  // string allocator_type = 2;
  if (!this->_internal_allocator_type().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_allocator_type().data(), static_cast<int>(this->_internal_allocator_type().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.GPUOptions.allocator_type");
    target = stream->WriteStringMaybeAliased(
        2, this->_internal_allocator_type(), target);
  }

  // int64 deferred_deletion_bytes = 3;
  if (this->_internal_deferred_deletion_bytes() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(3, this->_internal_deferred_deletion_bytes(), target);
  }

  // bool allow_growth = 4;
  if (this->_internal_allow_growth() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(4, this->_internal_allow_growth(), target);
  }

  // string visible_device_list = 5;
  if (!this->_internal_visible_device_list().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_visible_device_list().data(), static_cast<int>(this->_internal_visible_device_list().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.GPUOptions.visible_device_list");
    target = stream->WriteStringMaybeAliased(
        5, this->_internal_visible_device_list(), target);
  }

  // int32 polling_active_delay_usecs = 6;
  if (this->_internal_polling_active_delay_usecs() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(6, this->_internal_polling_active_delay_usecs(), target);
  }

  // int32 polling_inactive_delay_msecs = 7;
  if (this->_internal_polling_inactive_delay_msecs() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(7, this->_internal_polling_inactive_delay_msecs(), target);
  }

  // bool force_gpu_compatible = 8;
  if (this->_internal_force_gpu_compatible() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(8, this->_internal_force_gpu_compatible(), target);
  }

  // .tensorflow.GPUOptions.Experimental experimental = 9;
  if (this->_internal_has_experimental()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(9, _Internal::experimental(this),
        _Internal::experimental(this).GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.GPUOptions)
  return target;
}

size_t GPUOptions::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.GPUOptions)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // string allocator_type = 2;
  if (!this->_internal_allocator_type().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_allocator_type());
  }

  // string visible_device_list = 5;
  if (!this->_internal_visible_device_list().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_visible_device_list());
  }

  // .tensorflow.GPUOptions.Experimental experimental = 9;
  if (this->_internal_has_experimental()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.experimental_);
  }

  // double per_process_gpu_memory_fraction = 1;
  static_assert(sizeof(uint64_t) == sizeof(double), "Code assumes uint64_t and double are the same size.");
  double tmp_per_process_gpu_memory_fraction = this->_internal_per_process_gpu_memory_fraction();
  uint64_t raw_per_process_gpu_memory_fraction;
  memcpy(&raw_per_process_gpu_memory_fraction, &tmp_per_process_gpu_memory_fraction, sizeof(tmp_per_process_gpu_memory_fraction));
  if (raw_per_process_gpu_memory_fraction != 0) {
    total_size += 1 + 8;
  }

  // int64 deferred_deletion_bytes = 3;
  if (this->_internal_deferred_deletion_bytes() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_deferred_deletion_bytes());
  }

  // int32 polling_active_delay_usecs = 6;
  if (this->_internal_polling_active_delay_usecs() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_polling_active_delay_usecs());
  }

  // bool allow_growth = 4;
  if (this->_internal_allow_growth() != 0) {
    total_size += 1 + 1;
  }

  // bool force_gpu_compatible = 8;
  if (this->_internal_force_gpu_compatible() != 0) {
    total_size += 1 + 1;
  }

  // int32 polling_inactive_delay_msecs = 7;
  if (this->_internal_polling_inactive_delay_msecs() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_polling_inactive_delay_msecs());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData GPUOptions::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    GPUOptions::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GPUOptions::GetClassData() const { return &_class_data_; }


void GPUOptions::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<GPUOptions*>(&to_msg);
  auto& from = static_cast<const GPUOptions&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.GPUOptions)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (!from._internal_allocator_type().empty()) {
    _this->_internal_set_allocator_type(from._internal_allocator_type());
  }
  if (!from._internal_visible_device_list().empty()) {
    _this->_internal_set_visible_device_list(from._internal_visible_device_list());
  }
  if (from._internal_has_experimental()) {
    _this->_internal_mutable_experimental()->::tensorflow::GPUOptions_Experimental::MergeFrom(
        from._internal_experimental());
  }
  static_assert(sizeof(uint64_t) == sizeof(double), "Code assumes uint64_t and double are the same size.");
  double tmp_per_process_gpu_memory_fraction = from._internal_per_process_gpu_memory_fraction();
  uint64_t raw_per_process_gpu_memory_fraction;
  memcpy(&raw_per_process_gpu_memory_fraction, &tmp_per_process_gpu_memory_fraction, sizeof(tmp_per_process_gpu_memory_fraction));
  if (raw_per_process_gpu_memory_fraction != 0) {
    _this->_internal_set_per_process_gpu_memory_fraction(from._internal_per_process_gpu_memory_fraction());
  }
  if (from._internal_deferred_deletion_bytes() != 0) {
    _this->_internal_set_deferred_deletion_bytes(from._internal_deferred_deletion_bytes());
  }
  if (from._internal_polling_active_delay_usecs() != 0) {
    _this->_internal_set_polling_active_delay_usecs(from._internal_polling_active_delay_usecs());
  }
  if (from._internal_allow_growth() != 0) {
    _this->_internal_set_allow_growth(from._internal_allow_growth());
  }
  if (from._internal_force_gpu_compatible() != 0) {
    _this->_internal_set_force_gpu_compatible(from._internal_force_gpu_compatible());
  }
  if (from._internal_polling_inactive_delay_msecs() != 0) {
    _this->_internal_set_polling_inactive_delay_msecs(from._internal_polling_inactive_delay_msecs());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void GPUOptions::CopyFrom(const GPUOptions& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.GPUOptions)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GPUOptions::IsInitialized() const {
  return true;
}

void GPUOptions::InternalSwap(GPUOptions* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.allocator_type_, lhs_arena,
      &other->_impl_.allocator_type_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.visible_device_list_, lhs_arena,
      &other->_impl_.visible_device_list_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(GPUOptions, _impl_.polling_inactive_delay_msecs_)
      + sizeof(GPUOptions::_impl_.polling_inactive_delay_msecs_)
      - PROTOBUF_FIELD_OFFSET(GPUOptions, _impl_.experimental_)>(
          reinterpret_cast<char*>(&_impl_.experimental_),
          reinterpret_cast<char*>(&other->_impl_.experimental_));
}

::PROTOBUF_NAMESPACE_ID::Metadata GPUOptions::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[3]);
}

// ===================================================================

class OptimizerOptions::_Internal {
 public:
};

OptimizerOptions::OptimizerOptions(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.OptimizerOptions)
}
OptimizerOptions::OptimizerOptions(const OptimizerOptions& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  OptimizerOptions* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.opt_level_){}
    , decltype(_impl_.do_common_subexpression_elimination_){}
    , decltype(_impl_.do_constant_folding_){}
    , decltype(_impl_.do_function_inlining_){}
    , decltype(_impl_.cpu_global_jit_){}
    , decltype(_impl_.max_folded_constant_in_bytes_){}
    , decltype(_impl_.global_jit_level_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  ::memcpy(&_impl_.opt_level_, &from._impl_.opt_level_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.global_jit_level_) -
    reinterpret_cast<char*>(&_impl_.opt_level_)) + sizeof(_impl_.global_jit_level_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.OptimizerOptions)
}

inline void OptimizerOptions::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.opt_level_){0}
    , decltype(_impl_.do_common_subexpression_elimination_){false}
    , decltype(_impl_.do_constant_folding_){false}
    , decltype(_impl_.do_function_inlining_){false}
    , decltype(_impl_.cpu_global_jit_){false}
    , decltype(_impl_.max_folded_constant_in_bytes_){int64_t{0}}
    , decltype(_impl_.global_jit_level_){0}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

OptimizerOptions::~OptimizerOptions() {
  // @@protoc_insertion_point(destructor:tensorflow.OptimizerOptions)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void OptimizerOptions::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
}

void OptimizerOptions::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void OptimizerOptions::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.OptimizerOptions)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  ::memset(&_impl_.opt_level_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.global_jit_level_) -
      reinterpret_cast<char*>(&_impl_.opt_level_)) + sizeof(_impl_.global_jit_level_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* OptimizerOptions::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // bool do_common_subexpression_elimination = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          _impl_.do_common_subexpression_elimination_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool do_constant_folding = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.do_constant_folding_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.OptimizerOptions.Level opt_level = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          uint64_t val = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
          _internal_set_opt_level(static_cast<::tensorflow::OptimizerOptions_Level>(val));
        } else
          goto handle_unusual;
        continue;
      // bool do_function_inlining = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 32)) {
          _impl_.do_function_inlining_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.OptimizerOptions.GlobalJitLevel global_jit_level = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 40)) {
          uint64_t val = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
          _internal_set_global_jit_level(static_cast<::tensorflow::OptimizerOptions_GlobalJitLevel>(val));
        } else
          goto handle_unusual;
        continue;
      // int64 max_folded_constant_in_bytes = 6;
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 48)) {
          _impl_.max_folded_constant_in_bytes_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool cpu_global_jit = 7;
      case 7:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 56)) {
          _impl_.cpu_global_jit_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* OptimizerOptions::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.OptimizerOptions)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // bool do_common_subexpression_elimination = 1;
  if (this->_internal_do_common_subexpression_elimination() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(1, this->_internal_do_common_subexpression_elimination(), target);
  }

  // bool do_constant_folding = 2;
  if (this->_internal_do_constant_folding() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(2, this->_internal_do_constant_folding(), target);
  }

  // .tensorflow.OptimizerOptions.Level opt_level = 3;
  if (this->_internal_opt_level() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteEnumToArray(
      3, this->_internal_opt_level(), target);
  }

  // bool do_function_inlining = 4;
  if (this->_internal_do_function_inlining() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(4, this->_internal_do_function_inlining(), target);
  }

  // .tensorflow.OptimizerOptions.GlobalJitLevel global_jit_level = 5;
  if (this->_internal_global_jit_level() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteEnumToArray(
      5, this->_internal_global_jit_level(), target);
  }

  // int64 max_folded_constant_in_bytes = 6;
  if (this->_internal_max_folded_constant_in_bytes() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(6, this->_internal_max_folded_constant_in_bytes(), target);
  }

  // bool cpu_global_jit = 7;
  if (this->_internal_cpu_global_jit() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(7, this->_internal_cpu_global_jit(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.OptimizerOptions)
  return target;
}

size_t OptimizerOptions::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.OptimizerOptions)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .tensorflow.OptimizerOptions.Level opt_level = 3;
  if (this->_internal_opt_level() != 0) {
    total_size += 1 +
      ::_pbi::WireFormatLite::EnumSize(this->_internal_opt_level());
  }

  // bool do_common_subexpression_elimination = 1;
  if (this->_internal_do_common_subexpression_elimination() != 0) {
    total_size += 1 + 1;
  }

  // bool do_constant_folding = 2;
  if (this->_internal_do_constant_folding() != 0) {
    total_size += 1 + 1;
  }

  // bool do_function_inlining = 4;
  if (this->_internal_do_function_inlining() != 0) {
    total_size += 1 + 1;
  }

  // bool cpu_global_jit = 7;
  if (this->_internal_cpu_global_jit() != 0) {
    total_size += 1 + 1;
  }

  // int64 max_folded_constant_in_bytes = 6;
  if (this->_internal_max_folded_constant_in_bytes() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_max_folded_constant_in_bytes());
  }

  // .tensorflow.OptimizerOptions.GlobalJitLevel global_jit_level = 5;
  if (this->_internal_global_jit_level() != 0) {
    total_size += 1 +
      ::_pbi::WireFormatLite::EnumSize(this->_internal_global_jit_level());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData OptimizerOptions::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    OptimizerOptions::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*OptimizerOptions::GetClassData() const { return &_class_data_; }


void OptimizerOptions::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<OptimizerOptions*>(&to_msg);
  auto& from = static_cast<const OptimizerOptions&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.OptimizerOptions)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_opt_level() != 0) {
    _this->_internal_set_opt_level(from._internal_opt_level());
  }
  if (from._internal_do_common_subexpression_elimination() != 0) {
    _this->_internal_set_do_common_subexpression_elimination(from._internal_do_common_subexpression_elimination());
  }
  if (from._internal_do_constant_folding() != 0) {
    _this->_internal_set_do_constant_folding(from._internal_do_constant_folding());
  }
  if (from._internal_do_function_inlining() != 0) {
    _this->_internal_set_do_function_inlining(from._internal_do_function_inlining());
  }
  if (from._internal_cpu_global_jit() != 0) {
    _this->_internal_set_cpu_global_jit(from._internal_cpu_global_jit());
  }
  if (from._internal_max_folded_constant_in_bytes() != 0) {
    _this->_internal_set_max_folded_constant_in_bytes(from._internal_max_folded_constant_in_bytes());
  }
  if (from._internal_global_jit_level() != 0) {
    _this->_internal_set_global_jit_level(from._internal_global_jit_level());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void OptimizerOptions::CopyFrom(const OptimizerOptions& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.OptimizerOptions)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool OptimizerOptions::IsInitialized() const {
  return true;
}

void OptimizerOptions::InternalSwap(OptimizerOptions* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(OptimizerOptions, _impl_.global_jit_level_)
      + sizeof(OptimizerOptions::_impl_.global_jit_level_)
      - PROTOBUF_FIELD_OFFSET(OptimizerOptions, _impl_.opt_level_)>(
          reinterpret_cast<char*>(&_impl_.opt_level_),
          reinterpret_cast<char*>(&other->_impl_.opt_level_));
}

::PROTOBUF_NAMESPACE_ID::Metadata OptimizerOptions::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[4]);
}

// ===================================================================

class GraphOptions::_Internal {
 public:
  static const ::tensorflow::OptimizerOptions& optimizer_options(const GraphOptions* msg);
  static const ::tensorflow::RewriterConfig& rewrite_options(const GraphOptions* msg);
};

const ::tensorflow::OptimizerOptions&
GraphOptions::_Internal::optimizer_options(const GraphOptions* msg) {
  return *msg->_impl_.optimizer_options_;
}
const ::tensorflow::RewriterConfig&
GraphOptions::_Internal::rewrite_options(const GraphOptions* msg) {
  return *msg->_impl_.rewrite_options_;
}
void GraphOptions::clear_rewrite_options() {
  if (GetArenaForAllocation() == nullptr && _impl_.rewrite_options_ != nullptr) {
    delete _impl_.rewrite_options_;
  }
  _impl_.rewrite_options_ = nullptr;
}
GraphOptions::GraphOptions(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.GraphOptions)
}
GraphOptions::GraphOptions(const GraphOptions& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  GraphOptions* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.optimizer_options_){nullptr}
    , decltype(_impl_.rewrite_options_){nullptr}
    , decltype(_impl_.build_cost_model_){}
    , decltype(_impl_.enable_recv_scheduling_){}
    , decltype(_impl_.infer_shapes_){}
    , decltype(_impl_.place_pruned_graph_){}
    , decltype(_impl_.enable_bfloat16_sendrecv_){}
    , decltype(_impl_.timeline_step_){}
    , decltype(_impl_.build_cost_model_after_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_optimizer_options()) {
    _this->_impl_.optimizer_options_ = new ::tensorflow::OptimizerOptions(*from._impl_.optimizer_options_);
  }
  if (from._internal_has_rewrite_options()) {
    _this->_impl_.rewrite_options_ = new ::tensorflow::RewriterConfig(*from._impl_.rewrite_options_);
  }
  ::memcpy(&_impl_.build_cost_model_, &from._impl_.build_cost_model_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.build_cost_model_after_) -
    reinterpret_cast<char*>(&_impl_.build_cost_model_)) + sizeof(_impl_.build_cost_model_after_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.GraphOptions)
}

inline void GraphOptions::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.optimizer_options_){nullptr}
    , decltype(_impl_.rewrite_options_){nullptr}
    , decltype(_impl_.build_cost_model_){int64_t{0}}
    , decltype(_impl_.enable_recv_scheduling_){false}
    , decltype(_impl_.infer_shapes_){false}
    , decltype(_impl_.place_pruned_graph_){false}
    , decltype(_impl_.enable_bfloat16_sendrecv_){false}
    , decltype(_impl_.timeline_step_){0}
    , decltype(_impl_.build_cost_model_after_){int64_t{0}}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

GraphOptions::~GraphOptions() {
  // @@protoc_insertion_point(destructor:tensorflow.GraphOptions)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void GraphOptions::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  if (this != internal_default_instance()) delete _impl_.optimizer_options_;
  if (this != internal_default_instance()) delete _impl_.rewrite_options_;
}

void GraphOptions::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void GraphOptions::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.GraphOptions)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaForAllocation() == nullptr && _impl_.optimizer_options_ != nullptr) {
    delete _impl_.optimizer_options_;
  }
  _impl_.optimizer_options_ = nullptr;
  if (GetArenaForAllocation() == nullptr && _impl_.rewrite_options_ != nullptr) {
    delete _impl_.rewrite_options_;
  }
  _impl_.rewrite_options_ = nullptr;
  ::memset(&_impl_.build_cost_model_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.build_cost_model_after_) -
      reinterpret_cast<char*>(&_impl_.build_cost_model_)) + sizeof(_impl_.build_cost_model_after_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* GraphOptions::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // bool enable_recv_scheduling = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.enable_recv_scheduling_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.OptimizerOptions optimizer_options = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          ptr = ctx->ParseMessage(_internal_mutable_optimizer_options(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 build_cost_model = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 32)) {
          _impl_.build_cost_model_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool infer_shapes = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 40)) {
          _impl_.infer_shapes_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool place_pruned_graph = 6;
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 48)) {
          _impl_.place_pruned_graph_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool enable_bfloat16_sendrecv = 7;
      case 7:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 56)) {
          _impl_.enable_bfloat16_sendrecv_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 timeline_step = 8;
      case 8:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 64)) {
          _impl_.timeline_step_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 build_cost_model_after = 9;
      case 9:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 72)) {
          _impl_.build_cost_model_after_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.RewriterConfig rewrite_options = 10;
      case 10:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 82)) {
          ptr = ctx->ParseMessage(_internal_mutable_rewrite_options(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* GraphOptions::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.GraphOptions)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // bool enable_recv_scheduling = 2;
  if (this->_internal_enable_recv_scheduling() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(2, this->_internal_enable_recv_scheduling(), target);
  }

  // .tensorflow.OptimizerOptions optimizer_options = 3;
  if (this->_internal_has_optimizer_options()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(3, _Internal::optimizer_options(this),
        _Internal::optimizer_options(this).GetCachedSize(), target, stream);
  }

  // int64 build_cost_model = 4;
  if (this->_internal_build_cost_model() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(4, this->_internal_build_cost_model(), target);
  }

  // bool infer_shapes = 5;
  if (this->_internal_infer_shapes() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(5, this->_internal_infer_shapes(), target);
  }

  // bool place_pruned_graph = 6;
  if (this->_internal_place_pruned_graph() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(6, this->_internal_place_pruned_graph(), target);
  }

  // bool enable_bfloat16_sendrecv = 7;
  if (this->_internal_enable_bfloat16_sendrecv() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(7, this->_internal_enable_bfloat16_sendrecv(), target);
  }

  // int32 timeline_step = 8;
  if (this->_internal_timeline_step() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(8, this->_internal_timeline_step(), target);
  }

  // int64 build_cost_model_after = 9;
  if (this->_internal_build_cost_model_after() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(9, this->_internal_build_cost_model_after(), target);
  }

  // .tensorflow.RewriterConfig rewrite_options = 10;
  if (this->_internal_has_rewrite_options()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(10, _Internal::rewrite_options(this),
        _Internal::rewrite_options(this).GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.GraphOptions)
  return target;
}

size_t GraphOptions::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.GraphOptions)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .tensorflow.OptimizerOptions optimizer_options = 3;
  if (this->_internal_has_optimizer_options()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.optimizer_options_);
  }

  // .tensorflow.RewriterConfig rewrite_options = 10;
  if (this->_internal_has_rewrite_options()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.rewrite_options_);
  }

  // int64 build_cost_model = 4;
  if (this->_internal_build_cost_model() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_build_cost_model());
  }

  // bool enable_recv_scheduling = 2;
  if (this->_internal_enable_recv_scheduling() != 0) {
    total_size += 1 + 1;
  }

  // bool infer_shapes = 5;
  if (this->_internal_infer_shapes() != 0) {
    total_size += 1 + 1;
  }

  // bool place_pruned_graph = 6;
  if (this->_internal_place_pruned_graph() != 0) {
    total_size += 1 + 1;
  }

  // bool enable_bfloat16_sendrecv = 7;
  if (this->_internal_enable_bfloat16_sendrecv() != 0) {
    total_size += 1 + 1;
  }

  // int32 timeline_step = 8;
  if (this->_internal_timeline_step() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_timeline_step());
  }

  // int64 build_cost_model_after = 9;
  if (this->_internal_build_cost_model_after() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_build_cost_model_after());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData GraphOptions::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    GraphOptions::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GraphOptions::GetClassData() const { return &_class_data_; }


void GraphOptions::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<GraphOptions*>(&to_msg);
  auto& from = static_cast<const GraphOptions&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.GraphOptions)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_has_optimizer_options()) {
    _this->_internal_mutable_optimizer_options()->::tensorflow::OptimizerOptions::MergeFrom(
        from._internal_optimizer_options());
  }
  if (from._internal_has_rewrite_options()) {
    _this->_internal_mutable_rewrite_options()->::tensorflow::RewriterConfig::MergeFrom(
        from._internal_rewrite_options());
  }
  if (from._internal_build_cost_model() != 0) {
    _this->_internal_set_build_cost_model(from._internal_build_cost_model());
  }
  if (from._internal_enable_recv_scheduling() != 0) {
    _this->_internal_set_enable_recv_scheduling(from._internal_enable_recv_scheduling());
  }
  if (from._internal_infer_shapes() != 0) {
    _this->_internal_set_infer_shapes(from._internal_infer_shapes());
  }
  if (from._internal_place_pruned_graph() != 0) {
    _this->_internal_set_place_pruned_graph(from._internal_place_pruned_graph());
  }
  if (from._internal_enable_bfloat16_sendrecv() != 0) {
    _this->_internal_set_enable_bfloat16_sendrecv(from._internal_enable_bfloat16_sendrecv());
  }
  if (from._internal_timeline_step() != 0) {
    _this->_internal_set_timeline_step(from._internal_timeline_step());
  }
  if (from._internal_build_cost_model_after() != 0) {
    _this->_internal_set_build_cost_model_after(from._internal_build_cost_model_after());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void GraphOptions::CopyFrom(const GraphOptions& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.GraphOptions)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool GraphOptions::IsInitialized() const {
  return true;
}

void GraphOptions::InternalSwap(GraphOptions* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(GraphOptions, _impl_.build_cost_model_after_)
      + sizeof(GraphOptions::_impl_.build_cost_model_after_)
      - PROTOBUF_FIELD_OFFSET(GraphOptions, _impl_.optimizer_options_)>(
          reinterpret_cast<char*>(&_impl_.optimizer_options_),
          reinterpret_cast<char*>(&other->_impl_.optimizer_options_));
}

::PROTOBUF_NAMESPACE_ID::Metadata GraphOptions::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[5]);
}

// ===================================================================

class ThreadPoolOptionProto::_Internal {
 public:
};

ThreadPoolOptionProto::ThreadPoolOptionProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.ThreadPoolOptionProto)
}
ThreadPoolOptionProto::ThreadPoolOptionProto(const ThreadPoolOptionProto& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  ThreadPoolOptionProto* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.global_name_){}
    , decltype(_impl_.num_threads_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _impl_.global_name_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.global_name_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_global_name().empty()) {
    _this->_impl_.global_name_.Set(from._internal_global_name(), 
      _this->GetArenaForAllocation());
  }
  _this->_impl_.num_threads_ = from._impl_.num_threads_;
  // @@protoc_insertion_point(copy_constructor:tensorflow.ThreadPoolOptionProto)
}

inline void ThreadPoolOptionProto::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.global_name_){}
    , decltype(_impl_.num_threads_){0}
    , /*decltype(_impl_._cached_size_)*/{}
  };
  _impl_.global_name_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.global_name_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
}

ThreadPoolOptionProto::~ThreadPoolOptionProto() {
  // @@protoc_insertion_point(destructor:tensorflow.ThreadPoolOptionProto)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void ThreadPoolOptionProto::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.global_name_.Destroy();
}

void ThreadPoolOptionProto::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void ThreadPoolOptionProto::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.ThreadPoolOptionProto)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.global_name_.ClearToEmpty();
  _impl_.num_threads_ = 0;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* ThreadPoolOptionProto::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // int32 num_threads = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          _impl_.num_threads_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // string global_name = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          auto str = _internal_mutable_global_name();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.ThreadPoolOptionProto.global_name"));
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* ThreadPoolOptionProto::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.ThreadPoolOptionProto)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // int32 num_threads = 1;
  if (this->_internal_num_threads() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(1, this->_internal_num_threads(), target);
  }

  // string global_name = 2;
  if (!this->_internal_global_name().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_global_name().data(), static_cast<int>(this->_internal_global_name().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.ThreadPoolOptionProto.global_name");
    target = stream->WriteStringMaybeAliased(
        2, this->_internal_global_name(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.ThreadPoolOptionProto)
  return target;
}

size_t ThreadPoolOptionProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.ThreadPoolOptionProto)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // string global_name = 2;
  if (!this->_internal_global_name().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_global_name());
  }

  // int32 num_threads = 1;
  if (this->_internal_num_threads() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_num_threads());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData ThreadPoolOptionProto::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    ThreadPoolOptionProto::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*ThreadPoolOptionProto::GetClassData() const { return &_class_data_; }


void ThreadPoolOptionProto::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<ThreadPoolOptionProto*>(&to_msg);
  auto& from = static_cast<const ThreadPoolOptionProto&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.ThreadPoolOptionProto)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (!from._internal_global_name().empty()) {
    _this->_internal_set_global_name(from._internal_global_name());
  }
  if (from._internal_num_threads() != 0) {
    _this->_internal_set_num_threads(from._internal_num_threads());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void ThreadPoolOptionProto::CopyFrom(const ThreadPoolOptionProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.ThreadPoolOptionProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ThreadPoolOptionProto::IsInitialized() const {
  return true;
}

void ThreadPoolOptionProto::InternalSwap(ThreadPoolOptionProto* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.global_name_, lhs_arena,
      &other->_impl_.global_name_, rhs_arena
  );
  swap(_impl_.num_threads_, other->_impl_.num_threads_);
}

::PROTOBUF_NAMESPACE_ID::Metadata ThreadPoolOptionProto::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[6]);
}

// ===================================================================

class SessionMetadata::_Internal {
 public:
};

SessionMetadata::SessionMetadata(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.SessionMetadata)
}
SessionMetadata::SessionMetadata(const SessionMetadata& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  SessionMetadata* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.name_){}
    , decltype(_impl_.version_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _impl_.name_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.name_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_name().empty()) {
    _this->_impl_.name_.Set(from._internal_name(), 
      _this->GetArenaForAllocation());
  }
  _this->_impl_.version_ = from._impl_.version_;
  // @@protoc_insertion_point(copy_constructor:tensorflow.SessionMetadata)
}

inline void SessionMetadata::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.name_){}
    , decltype(_impl_.version_){int64_t{0}}
    , /*decltype(_impl_._cached_size_)*/{}
  };
  _impl_.name_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.name_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
}

SessionMetadata::~SessionMetadata() {
  // @@protoc_insertion_point(destructor:tensorflow.SessionMetadata)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void SessionMetadata::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.name_.Destroy();
}

void SessionMetadata::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void SessionMetadata::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.SessionMetadata)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.name_.ClearToEmpty();
  _impl_.version_ = int64_t{0};
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* SessionMetadata::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // string name = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          auto str = _internal_mutable_name();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.SessionMetadata.name"));
        } else
          goto handle_unusual;
        continue;
      // int64 version = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.version_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* SessionMetadata::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.SessionMetadata)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // string name = 1;
  if (!this->_internal_name().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_name().data(), static_cast<int>(this->_internal_name().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.SessionMetadata.name");
    target = stream->WriteStringMaybeAliased(
        1, this->_internal_name(), target);
  }

  // int64 version = 2;
  if (this->_internal_version() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(2, this->_internal_version(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.SessionMetadata)
  return target;
}

size_t SessionMetadata::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.SessionMetadata)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // string name = 1;
  if (!this->_internal_name().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_name());
  }

  // int64 version = 2;
  if (this->_internal_version() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_version());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData SessionMetadata::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    SessionMetadata::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*SessionMetadata::GetClassData() const { return &_class_data_; }


void SessionMetadata::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<SessionMetadata*>(&to_msg);
  auto& from = static_cast<const SessionMetadata&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.SessionMetadata)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (!from._internal_name().empty()) {
    _this->_internal_set_name(from._internal_name());
  }
  if (from._internal_version() != 0) {
    _this->_internal_set_version(from._internal_version());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void SessionMetadata::CopyFrom(const SessionMetadata& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.SessionMetadata)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SessionMetadata::IsInitialized() const {
  return true;
}

void SessionMetadata::InternalSwap(SessionMetadata* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.name_, lhs_arena,
      &other->_impl_.name_, rhs_arena
  );
  swap(_impl_.version_, other->_impl_.version_);
}

::PROTOBUF_NAMESPACE_ID::Metadata SessionMetadata::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[7]);
}

// ===================================================================

ConfigProto_DeviceCountEntry_DoNotUse::ConfigProto_DeviceCountEntry_DoNotUse() {}
ConfigProto_DeviceCountEntry_DoNotUse::ConfigProto_DeviceCountEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena)
    : SuperType(arena) {}
void ConfigProto_DeviceCountEntry_DoNotUse::MergeFrom(const ConfigProto_DeviceCountEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::PROTOBUF_NAMESPACE_ID::Metadata ConfigProto_DeviceCountEntry_DoNotUse::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[8]);
}

// ===================================================================

class ConfigProto_Experimental::_Internal {
 public:
  static const ::tensorflow::SessionMetadata& session_metadata(const ConfigProto_Experimental* msg);
  static const ::tensorflow::CoordinationServiceConfig& coordination_config(const ConfigProto_Experimental* msg);
};

const ::tensorflow::SessionMetadata&
ConfigProto_Experimental::_Internal::session_metadata(const ConfigProto_Experimental* msg) {
  return *msg->_impl_.session_metadata_;
}
const ::tensorflow::CoordinationServiceConfig&
ConfigProto_Experimental::_Internal::coordination_config(const ConfigProto_Experimental* msg) {
  return *msg->_impl_.coordination_config_;
}
void ConfigProto_Experimental::clear_coordination_config() {
  if (GetArenaForAllocation() == nullptr && _impl_.coordination_config_ != nullptr) {
    delete _impl_.coordination_config_;
  }
  _impl_.coordination_config_ = nullptr;
}
ConfigProto_Experimental::ConfigProto_Experimental(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.ConfigProto.Experimental)
}
ConfigProto_Experimental::ConfigProto_Experimental(const ConfigProto_Experimental& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  ConfigProto_Experimental* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.collective_group_leader_){}
    , decltype(_impl_.executor_type_){}
    , decltype(_impl_.session_metadata_){nullptr}
    , decltype(_impl_.coordination_config_){nullptr}
    , decltype(_impl_.recv_buf_max_chunk_){}
    , decltype(_impl_.use_numa_affinity_){}
    , decltype(_impl_.collective_deterministic_sequential_execution_){}
    , decltype(_impl_.collective_nccl_){}
    , decltype(_impl_.share_session_state_in_clusterspec_propagation_){}
    , decltype(_impl_.disable_thread_spinning_){}
    , decltype(_impl_.share_cluster_devices_in_session_){}
    , decltype(_impl_.optimize_for_static_graph_){}
    , decltype(_impl_.enable_mlir_bridge_){}
    , decltype(_impl_.mlir_bridge_rollout_){}
    , decltype(_impl_.xla_fusion_autotuner_thresh_){}
    , decltype(_impl_.enable_mlir_graph_optimization_){}
    , decltype(_impl_.disable_output_partition_graphs_){}
    , decltype(_impl_.use_tfrt_){}
    , decltype(_impl_.enable_multi_host_){}
    , decltype(_impl_.disable_optimize_for_static_graph_){}
    , decltype(_impl_.disable_eager_executor_streaming_enqueue_){}
    , decltype(_impl_.target_tpu_){}
    , decltype(_impl_.target_gpu_){}
    , decltype(_impl_.disable_functional_ops_lowering_){}
    , decltype(_impl_.xla_prefer_single_graph_cluster_){}
    , decltype(_impl_.backend_server_port_){}
    , decltype(_impl_.stream_merge_threshold_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _impl_.collective_group_leader_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.collective_group_leader_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_collective_group_leader().empty()) {
    _this->_impl_.collective_group_leader_.Set(from._internal_collective_group_leader(), 
      _this->GetArenaForAllocation());
  }
  _impl_.executor_type_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.executor_type_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_executor_type().empty()) {
    _this->_impl_.executor_type_.Set(from._internal_executor_type(), 
      _this->GetArenaForAllocation());
  }
  if (from._internal_has_session_metadata()) {
    _this->_impl_.session_metadata_ = new ::tensorflow::SessionMetadata(*from._impl_.session_metadata_);
  }
  if (from._internal_has_coordination_config()) {
    _this->_impl_.coordination_config_ = new ::tensorflow::CoordinationServiceConfig(*from._impl_.coordination_config_);
  }
  ::memcpy(&_impl_.recv_buf_max_chunk_, &from._impl_.recv_buf_max_chunk_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.stream_merge_threshold_) -
    reinterpret_cast<char*>(&_impl_.recv_buf_max_chunk_)) + sizeof(_impl_.stream_merge_threshold_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.ConfigProto.Experimental)
}

inline void ConfigProto_Experimental::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.collective_group_leader_){}
    , decltype(_impl_.executor_type_){}
    , decltype(_impl_.session_metadata_){nullptr}
    , decltype(_impl_.coordination_config_){nullptr}
    , decltype(_impl_.recv_buf_max_chunk_){0}
    , decltype(_impl_.use_numa_affinity_){false}
    , decltype(_impl_.collective_deterministic_sequential_execution_){false}
    , decltype(_impl_.collective_nccl_){false}
    , decltype(_impl_.share_session_state_in_clusterspec_propagation_){false}
    , decltype(_impl_.disable_thread_spinning_){false}
    , decltype(_impl_.share_cluster_devices_in_session_){false}
    , decltype(_impl_.optimize_for_static_graph_){false}
    , decltype(_impl_.enable_mlir_bridge_){false}
    , decltype(_impl_.mlir_bridge_rollout_){0}
    , decltype(_impl_.xla_fusion_autotuner_thresh_){int64_t{0}}
    , decltype(_impl_.enable_mlir_graph_optimization_){false}
    , decltype(_impl_.disable_output_partition_graphs_){false}
    , decltype(_impl_.use_tfrt_){false}
    , decltype(_impl_.enable_multi_host_){false}
    , decltype(_impl_.disable_optimize_for_static_graph_){false}
    , decltype(_impl_.disable_eager_executor_streaming_enqueue_){false}
    , decltype(_impl_.target_tpu_){false}
    , decltype(_impl_.target_gpu_){false}
    , decltype(_impl_.disable_functional_ops_lowering_){false}
    , decltype(_impl_.xla_prefer_single_graph_cluster_){false}
    , decltype(_impl_.backend_server_port_){0}
    , decltype(_impl_.stream_merge_threshold_){0}
    , /*decltype(_impl_._cached_size_)*/{}
  };
  _impl_.collective_group_leader_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.collective_group_leader_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  _impl_.executor_type_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.executor_type_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
}

ConfigProto_Experimental::~ConfigProto_Experimental() {
  // @@protoc_insertion_point(destructor:tensorflow.ConfigProto.Experimental)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void ConfigProto_Experimental::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.collective_group_leader_.Destroy();
  _impl_.executor_type_.Destroy();
  if (this != internal_default_instance()) delete _impl_.session_metadata_;
  if (this != internal_default_instance()) delete _impl_.coordination_config_;
}

void ConfigProto_Experimental::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void ConfigProto_Experimental::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.ConfigProto.Experimental)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.collective_group_leader_.ClearToEmpty();
  _impl_.executor_type_.ClearToEmpty();
  if (GetArenaForAllocation() == nullptr && _impl_.session_metadata_ != nullptr) {
    delete _impl_.session_metadata_;
  }
  _impl_.session_metadata_ = nullptr;
  if (GetArenaForAllocation() == nullptr && _impl_.coordination_config_ != nullptr) {
    delete _impl_.coordination_config_;
  }
  _impl_.coordination_config_ = nullptr;
  ::memset(&_impl_.recv_buf_max_chunk_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.stream_merge_threshold_) -
      reinterpret_cast<char*>(&_impl_.recv_buf_max_chunk_)) + sizeof(_impl_.stream_merge_threshold_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* ConfigProto_Experimental::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // string collective_group_leader = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          auto str = _internal_mutable_collective_group_leader();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.ConfigProto.Experimental.collective_group_leader"));
        } else
          goto handle_unusual;
        continue;
      // string executor_type = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          auto str = _internal_mutable_executor_type();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.ConfigProto.Experimental.executor_type"));
        } else
          goto handle_unusual;
        continue;
      // int32 recv_buf_max_chunk = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 32)) {
          _impl_.recv_buf_max_chunk_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool use_numa_affinity = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 40)) {
          _impl_.use_numa_affinity_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool collective_deterministic_sequential_execution = 6;
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 48)) {
          _impl_.collective_deterministic_sequential_execution_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool collective_nccl = 7;
      case 7:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 56)) {
          _impl_.collective_nccl_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool share_session_state_in_clusterspec_propagation = 8;
      case 8:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 64)) {
          _impl_.share_session_state_in_clusterspec_propagation_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool disable_thread_spinning = 9;
      case 9:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 72)) {
          _impl_.disable_thread_spinning_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool share_cluster_devices_in_session = 10;
      case 10:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 80)) {
          _impl_.share_cluster_devices_in_session_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.SessionMetadata session_metadata = 11;
      case 11:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 90)) {
          ptr = ctx->ParseMessage(_internal_mutable_session_metadata(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool optimize_for_static_graph = 12;
      case 12:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 96)) {
          _impl_.optimize_for_static_graph_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool enable_mlir_bridge = 13;
      case 13:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 104)) {
          _impl_.enable_mlir_bridge_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool disable_output_partition_graphs = 14;
      case 14:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 112)) {
          _impl_.disable_output_partition_graphs_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 xla_fusion_autotuner_thresh = 15;
      case 15:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 120)) {
          _impl_.xla_fusion_autotuner_thresh_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool enable_mlir_graph_optimization = 16;
      case 16:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 128)) {
          _impl_.enable_mlir_graph_optimization_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;
      case 17:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 136)) {
          uint64_t val = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
          _internal_set_mlir_bridge_rollout(static_cast<::tensorflow::ConfigProto_Experimental_MlirBridgeRollout>(val));
        } else
          goto handle_unusual;
        continue;
      // bool use_tfrt = 18;
      case 18:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 144)) {
          _impl_.use_tfrt_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool disable_functional_ops_lowering = 21;
      case 21:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 168)) {
          _impl_.disable_functional_ops_lowering_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool xla_prefer_single_graph_cluster = 22;
      case 22:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 176)) {
          _impl_.xla_prefer_single_graph_cluster_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.CoordinationServiceConfig coordination_config = 23;
      case 23:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 186)) {
          ptr = ctx->ParseMessage(_internal_mutable_coordination_config(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool disable_optimize_for_static_graph = 24;
      case 24:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 192)) {
          _impl_.disable_optimize_for_static_graph_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool disable_eager_executor_streaming_enqueue = 26;
      case 26:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 208)) {
          _impl_.disable_eager_executor_streaming_enqueue_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool enable_multi_host = 27;
      case 27:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 216)) {
          _impl_.enable_multi_host_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 backend_server_port = 28;
      case 28:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 224)) {
          _impl_.backend_server_port_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool target_tpu = 29;
      case 29:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 232)) {
          _impl_.target_tpu_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool target_gpu = 30;
      case 30:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 240)) {
          _impl_.target_gpu_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 stream_merge_threshold = 31;
      case 31:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 248)) {
          _impl_.stream_merge_threshold_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* ConfigProto_Experimental::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.ConfigProto.Experimental)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // string collective_group_leader = 1;
  if (!this->_internal_collective_group_leader().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_collective_group_leader().data(), static_cast<int>(this->_internal_collective_group_leader().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.ConfigProto.Experimental.collective_group_leader");
    target = stream->WriteStringMaybeAliased(
        1, this->_internal_collective_group_leader(), target);
  }

  // string executor_type = 3;
  if (!this->_internal_executor_type().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_executor_type().data(), static_cast<int>(this->_internal_executor_type().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.ConfigProto.Experimental.executor_type");
    target = stream->WriteStringMaybeAliased(
        3, this->_internal_executor_type(), target);
  }

  // int32 recv_buf_max_chunk = 4;
  if (this->_internal_recv_buf_max_chunk() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(4, this->_internal_recv_buf_max_chunk(), target);
  }

  // bool use_numa_affinity = 5;
  if (this->_internal_use_numa_affinity() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(5, this->_internal_use_numa_affinity(), target);
  }

  // bool collective_deterministic_sequential_execution = 6;
  if (this->_internal_collective_deterministic_sequential_execution() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(6, this->_internal_collective_deterministic_sequential_execution(), target);
  }

  // bool collective_nccl = 7;
  if (this->_internal_collective_nccl() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(7, this->_internal_collective_nccl(), target);
  }

  // bool share_session_state_in_clusterspec_propagation = 8;
  if (this->_internal_share_session_state_in_clusterspec_propagation() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(8, this->_internal_share_session_state_in_clusterspec_propagation(), target);
  }

  // bool disable_thread_spinning = 9;
  if (this->_internal_disable_thread_spinning() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(9, this->_internal_disable_thread_spinning(), target);
  }

  // bool share_cluster_devices_in_session = 10;
  if (this->_internal_share_cluster_devices_in_session() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(10, this->_internal_share_cluster_devices_in_session(), target);
  }

  // .tensorflow.SessionMetadata session_metadata = 11;
  if (this->_internal_has_session_metadata()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(11, _Internal::session_metadata(this),
        _Internal::session_metadata(this).GetCachedSize(), target, stream);
  }

  // bool optimize_for_static_graph = 12;
  if (this->_internal_optimize_for_static_graph() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(12, this->_internal_optimize_for_static_graph(), target);
  }

  // bool enable_mlir_bridge = 13;
  if (this->_internal_enable_mlir_bridge() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(13, this->_internal_enable_mlir_bridge(), target);
  }

  // bool disable_output_partition_graphs = 14;
  if (this->_internal_disable_output_partition_graphs() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(14, this->_internal_disable_output_partition_graphs(), target);
  }

  // int64 xla_fusion_autotuner_thresh = 15;
  if (this->_internal_xla_fusion_autotuner_thresh() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(15, this->_internal_xla_fusion_autotuner_thresh(), target);
  }

  // bool enable_mlir_graph_optimization = 16;
  if (this->_internal_enable_mlir_graph_optimization() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(16, this->_internal_enable_mlir_graph_optimization(), target);
  }

  // .tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;
  if (this->_internal_mlir_bridge_rollout() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteEnumToArray(
      17, this->_internal_mlir_bridge_rollout(), target);
  }

  // bool use_tfrt = 18;
  if (this->_internal_use_tfrt() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(18, this->_internal_use_tfrt(), target);
  }

  // bool disable_functional_ops_lowering = 21;
  if (this->_internal_disable_functional_ops_lowering() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(21, this->_internal_disable_functional_ops_lowering(), target);
  }

  // bool xla_prefer_single_graph_cluster = 22;
  if (this->_internal_xla_prefer_single_graph_cluster() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(22, this->_internal_xla_prefer_single_graph_cluster(), target);
  }

  // .tensorflow.CoordinationServiceConfig coordination_config = 23;
  if (this->_internal_has_coordination_config()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(23, _Internal::coordination_config(this),
        _Internal::coordination_config(this).GetCachedSize(), target, stream);
  }

  // bool disable_optimize_for_static_graph = 24;
  if (this->_internal_disable_optimize_for_static_graph() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(24, this->_internal_disable_optimize_for_static_graph(), target);
  }

  // bool disable_eager_executor_streaming_enqueue = 26;
  if (this->_internal_disable_eager_executor_streaming_enqueue() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(26, this->_internal_disable_eager_executor_streaming_enqueue(), target);
  }

  // bool enable_multi_host = 27;
  if (this->_internal_enable_multi_host() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(27, this->_internal_enable_multi_host(), target);
  }

  // int32 backend_server_port = 28;
  if (this->_internal_backend_server_port() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(28, this->_internal_backend_server_port(), target);
  }

  // bool target_tpu = 29;
  if (this->_internal_target_tpu() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(29, this->_internal_target_tpu(), target);
  }

  // bool target_gpu = 30;
  if (this->_internal_target_gpu() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(30, this->_internal_target_gpu(), target);
  }

  // int32 stream_merge_threshold = 31;
  if (this->_internal_stream_merge_threshold() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(31, this->_internal_stream_merge_threshold(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.ConfigProto.Experimental)
  return target;
}

size_t ConfigProto_Experimental::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.ConfigProto.Experimental)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // string collective_group_leader = 1;
  if (!this->_internal_collective_group_leader().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_collective_group_leader());
  }

  // string executor_type = 3;
  if (!this->_internal_executor_type().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_executor_type());
  }

  // .tensorflow.SessionMetadata session_metadata = 11;
  if (this->_internal_has_session_metadata()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.session_metadata_);
  }

  // .tensorflow.CoordinationServiceConfig coordination_config = 23;
  if (this->_internal_has_coordination_config()) {
    total_size += 2 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.coordination_config_);
  }

  // int32 recv_buf_max_chunk = 4;
  if (this->_internal_recv_buf_max_chunk() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_recv_buf_max_chunk());
  }

  // bool use_numa_affinity = 5;
  if (this->_internal_use_numa_affinity() != 0) {
    total_size += 1 + 1;
  }

  // bool collective_deterministic_sequential_execution = 6;
  if (this->_internal_collective_deterministic_sequential_execution() != 0) {
    total_size += 1 + 1;
  }

  // bool collective_nccl = 7;
  if (this->_internal_collective_nccl() != 0) {
    total_size += 1 + 1;
  }

  // bool share_session_state_in_clusterspec_propagation = 8;
  if (this->_internal_share_session_state_in_clusterspec_propagation() != 0) {
    total_size += 1 + 1;
  }

  // bool disable_thread_spinning = 9;
  if (this->_internal_disable_thread_spinning() != 0) {
    total_size += 1 + 1;
  }

  // bool share_cluster_devices_in_session = 10;
  if (this->_internal_share_cluster_devices_in_session() != 0) {
    total_size += 1 + 1;
  }

  // bool optimize_for_static_graph = 12;
  if (this->_internal_optimize_for_static_graph() != 0) {
    total_size += 1 + 1;
  }

  // bool enable_mlir_bridge = 13;
  if (this->_internal_enable_mlir_bridge() != 0) {
    total_size += 1 + 1;
  }

  // .tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;
  if (this->_internal_mlir_bridge_rollout() != 0) {
    total_size += 2 +
      ::_pbi::WireFormatLite::EnumSize(this->_internal_mlir_bridge_rollout());
  }

  // int64 xla_fusion_autotuner_thresh = 15;
  if (this->_internal_xla_fusion_autotuner_thresh() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_xla_fusion_autotuner_thresh());
  }

  // bool enable_mlir_graph_optimization = 16;
  if (this->_internal_enable_mlir_graph_optimization() != 0) {
    total_size += 2 + 1;
  }

  // bool disable_output_partition_graphs = 14;
  if (this->_internal_disable_output_partition_graphs() != 0) {
    total_size += 1 + 1;
  }

  // bool use_tfrt = 18;
  if (this->_internal_use_tfrt() != 0) {
    total_size += 2 + 1;
  }

  // bool enable_multi_host = 27;
  if (this->_internal_enable_multi_host() != 0) {
    total_size += 2 + 1;
  }

  // bool disable_optimize_for_static_graph = 24;
  if (this->_internal_disable_optimize_for_static_graph() != 0) {
    total_size += 2 + 1;
  }

  // bool disable_eager_executor_streaming_enqueue = 26;
  if (this->_internal_disable_eager_executor_streaming_enqueue() != 0) {
    total_size += 2 + 1;
  }

  // bool target_tpu = 29;
  if (this->_internal_target_tpu() != 0) {
    total_size += 2 + 1;
  }

  // bool target_gpu = 30;
  if (this->_internal_target_gpu() != 0) {
    total_size += 2 + 1;
  }

  // bool disable_functional_ops_lowering = 21;
  if (this->_internal_disable_functional_ops_lowering() != 0) {
    total_size += 2 + 1;
  }

  // bool xla_prefer_single_graph_cluster = 22;
  if (this->_internal_xla_prefer_single_graph_cluster() != 0) {
    total_size += 2 + 1;
  }

  // int32 backend_server_port = 28;
  if (this->_internal_backend_server_port() != 0) {
    total_size += 2 +
      ::_pbi::WireFormatLite::Int32Size(
        this->_internal_backend_server_port());
  }

  // int32 stream_merge_threshold = 31;
  if (this->_internal_stream_merge_threshold() != 0) {
    total_size += 2 +
      ::_pbi::WireFormatLite::Int32Size(
        this->_internal_stream_merge_threshold());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData ConfigProto_Experimental::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    ConfigProto_Experimental::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*ConfigProto_Experimental::GetClassData() const { return &_class_data_; }


void ConfigProto_Experimental::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<ConfigProto_Experimental*>(&to_msg);
  auto& from = static_cast<const ConfigProto_Experimental&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.ConfigProto.Experimental)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (!from._internal_collective_group_leader().empty()) {
    _this->_internal_set_collective_group_leader(from._internal_collective_group_leader());
  }
  if (!from._internal_executor_type().empty()) {
    _this->_internal_set_executor_type(from._internal_executor_type());
  }
  if (from._internal_has_session_metadata()) {
    _this->_internal_mutable_session_metadata()->::tensorflow::SessionMetadata::MergeFrom(
        from._internal_session_metadata());
  }
  if (from._internal_has_coordination_config()) {
    _this->_internal_mutable_coordination_config()->::tensorflow::CoordinationServiceConfig::MergeFrom(
        from._internal_coordination_config());
  }
  if (from._internal_recv_buf_max_chunk() != 0) {
    _this->_internal_set_recv_buf_max_chunk(from._internal_recv_buf_max_chunk());
  }
  if (from._internal_use_numa_affinity() != 0) {
    _this->_internal_set_use_numa_affinity(from._internal_use_numa_affinity());
  }
  if (from._internal_collective_deterministic_sequential_execution() != 0) {
    _this->_internal_set_collective_deterministic_sequential_execution(from._internal_collective_deterministic_sequential_execution());
  }
  if (from._internal_collective_nccl() != 0) {
    _this->_internal_set_collective_nccl(from._internal_collective_nccl());
  }
  if (from._internal_share_session_state_in_clusterspec_propagation() != 0) {
    _this->_internal_set_share_session_state_in_clusterspec_propagation(from._internal_share_session_state_in_clusterspec_propagation());
  }
  if (from._internal_disable_thread_spinning() != 0) {
    _this->_internal_set_disable_thread_spinning(from._internal_disable_thread_spinning());
  }
  if (from._internal_share_cluster_devices_in_session() != 0) {
    _this->_internal_set_share_cluster_devices_in_session(from._internal_share_cluster_devices_in_session());
  }
  if (from._internal_optimize_for_static_graph() != 0) {
    _this->_internal_set_optimize_for_static_graph(from._internal_optimize_for_static_graph());
  }
  if (from._internal_enable_mlir_bridge() != 0) {
    _this->_internal_set_enable_mlir_bridge(from._internal_enable_mlir_bridge());
  }
  if (from._internal_mlir_bridge_rollout() != 0) {
    _this->_internal_set_mlir_bridge_rollout(from._internal_mlir_bridge_rollout());
  }
  if (from._internal_xla_fusion_autotuner_thresh() != 0) {
    _this->_internal_set_xla_fusion_autotuner_thresh(from._internal_xla_fusion_autotuner_thresh());
  }
  if (from._internal_enable_mlir_graph_optimization() != 0) {
    _this->_internal_set_enable_mlir_graph_optimization(from._internal_enable_mlir_graph_optimization());
  }
  if (from._internal_disable_output_partition_graphs() != 0) {
    _this->_internal_set_disable_output_partition_graphs(from._internal_disable_output_partition_graphs());
  }
  if (from._internal_use_tfrt() != 0) {
    _this->_internal_set_use_tfrt(from._internal_use_tfrt());
  }
  if (from._internal_enable_multi_host() != 0) {
    _this->_internal_set_enable_multi_host(from._internal_enable_multi_host());
  }
  if (from._internal_disable_optimize_for_static_graph() != 0) {
    _this->_internal_set_disable_optimize_for_static_graph(from._internal_disable_optimize_for_static_graph());
  }
  if (from._internal_disable_eager_executor_streaming_enqueue() != 0) {
    _this->_internal_set_disable_eager_executor_streaming_enqueue(from._internal_disable_eager_executor_streaming_enqueue());
  }
  if (from._internal_target_tpu() != 0) {
    _this->_internal_set_target_tpu(from._internal_target_tpu());
  }
  if (from._internal_target_gpu() != 0) {
    _this->_internal_set_target_gpu(from._internal_target_gpu());
  }
  if (from._internal_disable_functional_ops_lowering() != 0) {
    _this->_internal_set_disable_functional_ops_lowering(from._internal_disable_functional_ops_lowering());
  }
  if (from._internal_xla_prefer_single_graph_cluster() != 0) {
    _this->_internal_set_xla_prefer_single_graph_cluster(from._internal_xla_prefer_single_graph_cluster());
  }
  if (from._internal_backend_server_port() != 0) {
    _this->_internal_set_backend_server_port(from._internal_backend_server_port());
  }
  if (from._internal_stream_merge_threshold() != 0) {
    _this->_internal_set_stream_merge_threshold(from._internal_stream_merge_threshold());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void ConfigProto_Experimental::CopyFrom(const ConfigProto_Experimental& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.ConfigProto.Experimental)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConfigProto_Experimental::IsInitialized() const {
  return true;
}

void ConfigProto_Experimental::InternalSwap(ConfigProto_Experimental* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.collective_group_leader_, lhs_arena,
      &other->_impl_.collective_group_leader_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.executor_type_, lhs_arena,
      &other->_impl_.executor_type_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(ConfigProto_Experimental, _impl_.stream_merge_threshold_)
      + sizeof(ConfigProto_Experimental::_impl_.stream_merge_threshold_)
      - PROTOBUF_FIELD_OFFSET(ConfigProto_Experimental, _impl_.session_metadata_)>(
          reinterpret_cast<char*>(&_impl_.session_metadata_),
          reinterpret_cast<char*>(&other->_impl_.session_metadata_));
}

::PROTOBUF_NAMESPACE_ID::Metadata ConfigProto_Experimental::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[9]);
}

// ===================================================================

class ConfigProto::_Internal {
 public:
  static const ::tensorflow::GPUOptions& gpu_options(const ConfigProto* msg);
  static const ::tensorflow::GraphOptions& graph_options(const ConfigProto* msg);
  static const ::tensorflow::RPCOptions& rpc_options(const ConfigProto* msg);
  static const ::tensorflow::ClusterDef& cluster_def(const ConfigProto* msg);
  static const ::tensorflow::ConfigProto_Experimental& experimental(const ConfigProto* msg);
};

const ::tensorflow::GPUOptions&
ConfigProto::_Internal::gpu_options(const ConfigProto* msg) {
  return *msg->_impl_.gpu_options_;
}
const ::tensorflow::GraphOptions&
ConfigProto::_Internal::graph_options(const ConfigProto* msg) {
  return *msg->_impl_.graph_options_;
}
const ::tensorflow::RPCOptions&
ConfigProto::_Internal::rpc_options(const ConfigProto* msg) {
  return *msg->_impl_.rpc_options_;
}
const ::tensorflow::ClusterDef&
ConfigProto::_Internal::cluster_def(const ConfigProto* msg) {
  return *msg->_impl_.cluster_def_;
}
const ::tensorflow::ConfigProto_Experimental&
ConfigProto::_Internal::experimental(const ConfigProto* msg) {
  return *msg->_impl_.experimental_;
}
void ConfigProto::clear_rpc_options() {
  if (GetArenaForAllocation() == nullptr && _impl_.rpc_options_ != nullptr) {
    delete _impl_.rpc_options_;
  }
  _impl_.rpc_options_ = nullptr;
}
void ConfigProto::clear_cluster_def() {
  if (GetArenaForAllocation() == nullptr && _impl_.cluster_def_ != nullptr) {
    delete _impl_.cluster_def_;
  }
  _impl_.cluster_def_ = nullptr;
}
ConfigProto::ConfigProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  if (arena != nullptr && !is_message_owned) {
    arena->OwnCustomDestructor(this, &ConfigProto::ArenaDtor);
  }
  // @@protoc_insertion_point(arena_constructor:tensorflow.ConfigProto)
}
ConfigProto::ConfigProto(const ConfigProto& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  ConfigProto* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      /*decltype(_impl_.device_count_)*/{}
    , decltype(_impl_.device_filters_){from._impl_.device_filters_}
    , decltype(_impl_.session_inter_op_thread_pool_){from._impl_.session_inter_op_thread_pool_}
    , decltype(_impl_.gpu_options_){nullptr}
    , decltype(_impl_.graph_options_){nullptr}
    , decltype(_impl_.rpc_options_){nullptr}
    , decltype(_impl_.cluster_def_){nullptr}
    , decltype(_impl_.experimental_){nullptr}
    , decltype(_impl_.intra_op_parallelism_threads_){}
    , decltype(_impl_.placement_period_){}
    , decltype(_impl_.inter_op_parallelism_threads_){}
    , decltype(_impl_.use_per_session_threads_){}
    , decltype(_impl_.allow_soft_placement_){}
    , decltype(_impl_.log_device_placement_){}
    , decltype(_impl_.isolate_session_state_){}
    , decltype(_impl_.operation_timeout_in_ms_){}
    , decltype(_impl_.share_cluster_devices_in_session_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _this->_impl_.device_count_.MergeFrom(from._impl_.device_count_);
  if (from._internal_has_gpu_options()) {
    _this->_impl_.gpu_options_ = new ::tensorflow::GPUOptions(*from._impl_.gpu_options_);
  }
  if (from._internal_has_graph_options()) {
    _this->_impl_.graph_options_ = new ::tensorflow::GraphOptions(*from._impl_.graph_options_);
  }
  if (from._internal_has_rpc_options()) {
    _this->_impl_.rpc_options_ = new ::tensorflow::RPCOptions(*from._impl_.rpc_options_);
  }
  if (from._internal_has_cluster_def()) {
    _this->_impl_.cluster_def_ = new ::tensorflow::ClusterDef(*from._impl_.cluster_def_);
  }
  if (from._internal_has_experimental()) {
    _this->_impl_.experimental_ = new ::tensorflow::ConfigProto_Experimental(*from._impl_.experimental_);
  }
  ::memcpy(&_impl_.intra_op_parallelism_threads_, &from._impl_.intra_op_parallelism_threads_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.share_cluster_devices_in_session_) -
    reinterpret_cast<char*>(&_impl_.intra_op_parallelism_threads_)) + sizeof(_impl_.share_cluster_devices_in_session_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.ConfigProto)
}

inline void ConfigProto::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      /*decltype(_impl_.device_count_)*/{::_pbi::ArenaInitialized(), arena}
    , decltype(_impl_.device_filters_){arena}
    , decltype(_impl_.session_inter_op_thread_pool_){arena}
    , decltype(_impl_.gpu_options_){nullptr}
    , decltype(_impl_.graph_options_){nullptr}
    , decltype(_impl_.rpc_options_){nullptr}
    , decltype(_impl_.cluster_def_){nullptr}
    , decltype(_impl_.experimental_){nullptr}
    , decltype(_impl_.intra_op_parallelism_threads_){0}
    , decltype(_impl_.placement_period_){0}
    , decltype(_impl_.inter_op_parallelism_threads_){0}
    , decltype(_impl_.use_per_session_threads_){false}
    , decltype(_impl_.allow_soft_placement_){false}
    , decltype(_impl_.log_device_placement_){false}
    , decltype(_impl_.isolate_session_state_){false}
    , decltype(_impl_.operation_timeout_in_ms_){int64_t{0}}
    , decltype(_impl_.share_cluster_devices_in_session_){false}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

ConfigProto::~ConfigProto() {
  // @@protoc_insertion_point(destructor:tensorflow.ConfigProto)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    ArenaDtor(this);
    return;
  }
  SharedDtor();
}

inline void ConfigProto::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.device_count_.Destruct();
  _impl_.device_count_.~MapField();
  _impl_.device_filters_.~RepeatedPtrField();
  _impl_.session_inter_op_thread_pool_.~RepeatedPtrField();
  if (this != internal_default_instance()) delete _impl_.gpu_options_;
  if (this != internal_default_instance()) delete _impl_.graph_options_;
  if (this != internal_default_instance()) delete _impl_.rpc_options_;
  if (this != internal_default_instance()) delete _impl_.cluster_def_;
  if (this != internal_default_instance()) delete _impl_.experimental_;
}

void ConfigProto::ArenaDtor(void* object) {
  ConfigProto* _this = reinterpret_cast< ConfigProto* >(object);
  _this->_impl_.device_count_.Destruct();
}
void ConfigProto::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void ConfigProto::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.ConfigProto)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.device_count_.Clear();
  _impl_.device_filters_.Clear();
  _impl_.session_inter_op_thread_pool_.Clear();
  if (GetArenaForAllocation() == nullptr && _impl_.gpu_options_ != nullptr) {
    delete _impl_.gpu_options_;
  }
  _impl_.gpu_options_ = nullptr;
  if (GetArenaForAllocation() == nullptr && _impl_.graph_options_ != nullptr) {
    delete _impl_.graph_options_;
  }
  _impl_.graph_options_ = nullptr;
  if (GetArenaForAllocation() == nullptr && _impl_.rpc_options_ != nullptr) {
    delete _impl_.rpc_options_;
  }
  _impl_.rpc_options_ = nullptr;
  if (GetArenaForAllocation() == nullptr && _impl_.cluster_def_ != nullptr) {
    delete _impl_.cluster_def_;
  }
  _impl_.cluster_def_ = nullptr;
  if (GetArenaForAllocation() == nullptr && _impl_.experimental_ != nullptr) {
    delete _impl_.experimental_;
  }
  _impl_.experimental_ = nullptr;
  ::memset(&_impl_.intra_op_parallelism_threads_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.share_cluster_devices_in_session_) -
      reinterpret_cast<char*>(&_impl_.intra_op_parallelism_threads_)) + sizeof(_impl_.share_cluster_devices_in_session_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* ConfigProto::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // map<string, int32> device_count = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(&_impl_.device_count_, ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<10>(ptr));
        } else
          goto handle_unusual;
        continue;
      // int32 intra_op_parallelism_threads = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.intra_op_parallelism_threads_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 placement_period = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          _impl_.placement_period_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // repeated string device_filters = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 34)) {
          ptr -= 1;
          do {
            ptr += 1;
            auto str = _internal_add_device_filters();
            ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
            CHK_(ptr);
            CHK_(::_pbi::VerifyUTF8(str, "tensorflow.ConfigProto.device_filters"));
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<34>(ptr));
        } else
          goto handle_unusual;
        continue;
      // int32 inter_op_parallelism_threads = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 40)) {
          _impl_.inter_op_parallelism_threads_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.GPUOptions gpu_options = 6;
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 50)) {
          ptr = ctx->ParseMessage(_internal_mutable_gpu_options(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool allow_soft_placement = 7;
      case 7:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 56)) {
          _impl_.allow_soft_placement_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool log_device_placement = 8;
      case 8:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 64)) {
          _impl_.log_device_placement_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool use_per_session_threads = 9;
      case 9:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 72)) {
          _impl_.use_per_session_threads_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.GraphOptions graph_options = 10;
      case 10:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 82)) {
          ptr = ctx->ParseMessage(_internal_mutable_graph_options(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 operation_timeout_in_ms = 11;
      case 11:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 88)) {
          _impl_.operation_timeout_in_ms_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;
      case 12:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 98)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_session_inter_op_thread_pool(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<98>(ptr));
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.RPCOptions rpc_options = 13;
      case 13:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 106)) {
          ptr = ctx->ParseMessage(_internal_mutable_rpc_options(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.ClusterDef cluster_def = 14;
      case 14:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 114)) {
          ptr = ctx->ParseMessage(_internal_mutable_cluster_def(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool isolate_session_state = 15;
      case 15:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 120)) {
          _impl_.isolate_session_state_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.ConfigProto.Experimental experimental = 16;
      case 16:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 130)) {
          ptr = ctx->ParseMessage(_internal_mutable_experimental(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool share_cluster_devices_in_session = 17;
      case 17:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 136)) {
          _impl_.share_cluster_devices_in_session_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* ConfigProto::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.ConfigProto)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // map<string, int32> device_count = 1;
  if (!this->_internal_device_count().empty()) {
    using MapType = ::_pb::Map<std::string, int32_t>;
    using WireHelper = ConfigProto_DeviceCountEntry_DoNotUse::Funcs;
    const auto& map_field = this->_internal_device_count();
    auto check_utf8 = [](const MapType::value_type& entry) {
      (void)entry;
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
        entry.first.data(), static_cast<int>(entry.first.length()),
        ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
        "tensorflow.ConfigProto.DeviceCountEntry.key");
    };

    if (stream->IsSerializationDeterministic() && map_field.size() > 1) {
      for (const auto& entry : ::_pbi::MapSorterPtr<MapType>(map_field)) {
        target = WireHelper::InternalSerialize(1, entry.first, entry.second, target, stream);
        check_utf8(entry);
      }
    } else {
      for (const auto& entry : map_field) {
        target = WireHelper::InternalSerialize(1, entry.first, entry.second, target, stream);
        check_utf8(entry);
      }
    }
  }

  // int32 intra_op_parallelism_threads = 2;
  if (this->_internal_intra_op_parallelism_threads() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(2, this->_internal_intra_op_parallelism_threads(), target);
  }

  // int32 placement_period = 3;
  if (this->_internal_placement_period() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(3, this->_internal_placement_period(), target);
  }

  // repeated string device_filters = 4;
  for (int i = 0, n = this->_internal_device_filters_size(); i < n; i++) {
    const auto& s = this->_internal_device_filters(i);
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      s.data(), static_cast<int>(s.length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.ConfigProto.device_filters");
    target = stream->WriteString(4, s, target);
  }

  // int32 inter_op_parallelism_threads = 5;
  if (this->_internal_inter_op_parallelism_threads() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(5, this->_internal_inter_op_parallelism_threads(), target);
  }

  // .tensorflow.GPUOptions gpu_options = 6;
  if (this->_internal_has_gpu_options()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(6, _Internal::gpu_options(this),
        _Internal::gpu_options(this).GetCachedSize(), target, stream);
  }

  // bool allow_soft_placement = 7;
  if (this->_internal_allow_soft_placement() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(7, this->_internal_allow_soft_placement(), target);
  }

  // bool log_device_placement = 8;
  if (this->_internal_log_device_placement() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(8, this->_internal_log_device_placement(), target);
  }

  // bool use_per_session_threads = 9;
  if (this->_internal_use_per_session_threads() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(9, this->_internal_use_per_session_threads(), target);
  }

  // .tensorflow.GraphOptions graph_options = 10;
  if (this->_internal_has_graph_options()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(10, _Internal::graph_options(this),
        _Internal::graph_options(this).GetCachedSize(), target, stream);
  }

  // int64 operation_timeout_in_ms = 11;
  if (this->_internal_operation_timeout_in_ms() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(11, this->_internal_operation_timeout_in_ms(), target);
  }

  // repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_session_inter_op_thread_pool_size()); i < n; i++) {
    const auto& repfield = this->_internal_session_inter_op_thread_pool(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(12, repfield, repfield.GetCachedSize(), target, stream);
  }

  // .tensorflow.RPCOptions rpc_options = 13;
  if (this->_internal_has_rpc_options()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(13, _Internal::rpc_options(this),
        _Internal::rpc_options(this).GetCachedSize(), target, stream);
  }

  // .tensorflow.ClusterDef cluster_def = 14;
  if (this->_internal_has_cluster_def()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(14, _Internal::cluster_def(this),
        _Internal::cluster_def(this).GetCachedSize(), target, stream);
  }

  // bool isolate_session_state = 15;
  if (this->_internal_isolate_session_state() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(15, this->_internal_isolate_session_state(), target);
  }

  // .tensorflow.ConfigProto.Experimental experimental = 16;
  if (this->_internal_has_experimental()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(16, _Internal::experimental(this),
        _Internal::experimental(this).GetCachedSize(), target, stream);
  }

  // bool share_cluster_devices_in_session = 17;
  if (this->_internal_share_cluster_devices_in_session() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(17, this->_internal_share_cluster_devices_in_session(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.ConfigProto)
  return target;
}

size_t ConfigProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.ConfigProto)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // map<string, int32> device_count = 1;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(this->_internal_device_count_size());
  for (::PROTOBUF_NAMESPACE_ID::Map< std::string, int32_t >::const_iterator
      it = this->_internal_device_count().begin();
      it != this->_internal_device_count().end(); ++it) {
    total_size += ConfigProto_DeviceCountEntry_DoNotUse::Funcs::ByteSizeLong(it->first, it->second);
  }

  // repeated string device_filters = 4;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(_impl_.device_filters_.size());
  for (int i = 0, n = _impl_.device_filters_.size(); i < n; i++) {
    total_size += ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
      _impl_.device_filters_.Get(i));
  }

  // repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;
  total_size += 1UL * this->_internal_session_inter_op_thread_pool_size();
  for (const auto& msg : this->_impl_.session_inter_op_thread_pool_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // .tensorflow.GPUOptions gpu_options = 6;
  if (this->_internal_has_gpu_options()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.gpu_options_);
  }

  // .tensorflow.GraphOptions graph_options = 10;
  if (this->_internal_has_graph_options()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.graph_options_);
  }

  // .tensorflow.RPCOptions rpc_options = 13;
  if (this->_internal_has_rpc_options()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.rpc_options_);
  }

  // .tensorflow.ClusterDef cluster_def = 14;
  if (this->_internal_has_cluster_def()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.cluster_def_);
  }

  // .tensorflow.ConfigProto.Experimental experimental = 16;
  if (this->_internal_has_experimental()) {
    total_size += 2 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.experimental_);
  }

  // int32 intra_op_parallelism_threads = 2;
  if (this->_internal_intra_op_parallelism_threads() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_intra_op_parallelism_threads());
  }

  // int32 placement_period = 3;
  if (this->_internal_placement_period() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_placement_period());
  }

  // int32 inter_op_parallelism_threads = 5;
  if (this->_internal_inter_op_parallelism_threads() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_inter_op_parallelism_threads());
  }

  // bool use_per_session_threads = 9;
  if (this->_internal_use_per_session_threads() != 0) {
    total_size += 1 + 1;
  }

  // bool allow_soft_placement = 7;
  if (this->_internal_allow_soft_placement() != 0) {
    total_size += 1 + 1;
  }

  // bool log_device_placement = 8;
  if (this->_internal_log_device_placement() != 0) {
    total_size += 1 + 1;
  }

  // bool isolate_session_state = 15;
  if (this->_internal_isolate_session_state() != 0) {
    total_size += 1 + 1;
  }

  // int64 operation_timeout_in_ms = 11;
  if (this->_internal_operation_timeout_in_ms() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_operation_timeout_in_ms());
  }

  // bool share_cluster_devices_in_session = 17;
  if (this->_internal_share_cluster_devices_in_session() != 0) {
    total_size += 2 + 1;
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData ConfigProto::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    ConfigProto::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*ConfigProto::GetClassData() const { return &_class_data_; }


void ConfigProto::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<ConfigProto*>(&to_msg);
  auto& from = static_cast<const ConfigProto&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.ConfigProto)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.device_count_.MergeFrom(from._impl_.device_count_);
  _this->_impl_.device_filters_.MergeFrom(from._impl_.device_filters_);
  _this->_impl_.session_inter_op_thread_pool_.MergeFrom(from._impl_.session_inter_op_thread_pool_);
  if (from._internal_has_gpu_options()) {
    _this->_internal_mutable_gpu_options()->::tensorflow::GPUOptions::MergeFrom(
        from._internal_gpu_options());
  }
  if (from._internal_has_graph_options()) {
    _this->_internal_mutable_graph_options()->::tensorflow::GraphOptions::MergeFrom(
        from._internal_graph_options());
  }
  if (from._internal_has_rpc_options()) {
    _this->_internal_mutable_rpc_options()->::tensorflow::RPCOptions::MergeFrom(
        from._internal_rpc_options());
  }
  if (from._internal_has_cluster_def()) {
    _this->_internal_mutable_cluster_def()->::tensorflow::ClusterDef::MergeFrom(
        from._internal_cluster_def());
  }
  if (from._internal_has_experimental()) {
    _this->_internal_mutable_experimental()->::tensorflow::ConfigProto_Experimental::MergeFrom(
        from._internal_experimental());
  }
  if (from._internal_intra_op_parallelism_threads() != 0) {
    _this->_internal_set_intra_op_parallelism_threads(from._internal_intra_op_parallelism_threads());
  }
  if (from._internal_placement_period() != 0) {
    _this->_internal_set_placement_period(from._internal_placement_period());
  }
  if (from._internal_inter_op_parallelism_threads() != 0) {
    _this->_internal_set_inter_op_parallelism_threads(from._internal_inter_op_parallelism_threads());
  }
  if (from._internal_use_per_session_threads() != 0) {
    _this->_internal_set_use_per_session_threads(from._internal_use_per_session_threads());
  }
  if (from._internal_allow_soft_placement() != 0) {
    _this->_internal_set_allow_soft_placement(from._internal_allow_soft_placement());
  }
  if (from._internal_log_device_placement() != 0) {
    _this->_internal_set_log_device_placement(from._internal_log_device_placement());
  }
  if (from._internal_isolate_session_state() != 0) {
    _this->_internal_set_isolate_session_state(from._internal_isolate_session_state());
  }
  if (from._internal_operation_timeout_in_ms() != 0) {
    _this->_internal_set_operation_timeout_in_ms(from._internal_operation_timeout_in_ms());
  }
  if (from._internal_share_cluster_devices_in_session() != 0) {
    _this->_internal_set_share_cluster_devices_in_session(from._internal_share_cluster_devices_in_session());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void ConfigProto::CopyFrom(const ConfigProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.ConfigProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ConfigProto::IsInitialized() const {
  return true;
}

void ConfigProto::InternalSwap(ConfigProto* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.device_count_.InternalSwap(&other->_impl_.device_count_);
  _impl_.device_filters_.InternalSwap(&other->_impl_.device_filters_);
  _impl_.session_inter_op_thread_pool_.InternalSwap(&other->_impl_.session_inter_op_thread_pool_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(ConfigProto, _impl_.share_cluster_devices_in_session_)
      + sizeof(ConfigProto::_impl_.share_cluster_devices_in_session_)
      - PROTOBUF_FIELD_OFFSET(ConfigProto, _impl_.gpu_options_)>(
          reinterpret_cast<char*>(&_impl_.gpu_options_),
          reinterpret_cast<char*>(&other->_impl_.gpu_options_));
}

::PROTOBUF_NAMESPACE_ID::Metadata ConfigProto::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[10]);
}

// ===================================================================

class RunOptions_Experimental_RunHandlerPoolOptions::_Internal {
 public:
};

RunOptions_Experimental_RunHandlerPoolOptions::RunOptions_Experimental_RunHandlerPoolOptions(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.RunOptions.Experimental.RunHandlerPoolOptions)
}
RunOptions_Experimental_RunHandlerPoolOptions::RunOptions_Experimental_RunHandlerPoolOptions(const RunOptions_Experimental_RunHandlerPoolOptions& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  RunOptions_Experimental_RunHandlerPoolOptions* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.priority_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _this->_impl_.priority_ = from._impl_.priority_;
  // @@protoc_insertion_point(copy_constructor:tensorflow.RunOptions.Experimental.RunHandlerPoolOptions)
}

inline void RunOptions_Experimental_RunHandlerPoolOptions::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.priority_){int64_t{0}}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

RunOptions_Experimental_RunHandlerPoolOptions::~RunOptions_Experimental_RunHandlerPoolOptions() {
  // @@protoc_insertion_point(destructor:tensorflow.RunOptions.Experimental.RunHandlerPoolOptions)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void RunOptions_Experimental_RunHandlerPoolOptions::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
}

void RunOptions_Experimental_RunHandlerPoolOptions::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void RunOptions_Experimental_RunHandlerPoolOptions::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.RunOptions.Experimental.RunHandlerPoolOptions)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.priority_ = int64_t{0};
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* RunOptions_Experimental_RunHandlerPoolOptions::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // int64 priority = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          _impl_.priority_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* RunOptions_Experimental_RunHandlerPoolOptions::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.RunOptions.Experimental.RunHandlerPoolOptions)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 priority = 1;
  if (this->_internal_priority() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(1, this->_internal_priority(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.RunOptions.Experimental.RunHandlerPoolOptions)
  return target;
}

size_t RunOptions_Experimental_RunHandlerPoolOptions::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.RunOptions.Experimental.RunHandlerPoolOptions)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // int64 priority = 1;
  if (this->_internal_priority() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_priority());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData RunOptions_Experimental_RunHandlerPoolOptions::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    RunOptions_Experimental_RunHandlerPoolOptions::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*RunOptions_Experimental_RunHandlerPoolOptions::GetClassData() const { return &_class_data_; }


void RunOptions_Experimental_RunHandlerPoolOptions::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<RunOptions_Experimental_RunHandlerPoolOptions*>(&to_msg);
  auto& from = static_cast<const RunOptions_Experimental_RunHandlerPoolOptions&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.RunOptions.Experimental.RunHandlerPoolOptions)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_priority() != 0) {
    _this->_internal_set_priority(from._internal_priority());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void RunOptions_Experimental_RunHandlerPoolOptions::CopyFrom(const RunOptions_Experimental_RunHandlerPoolOptions& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.RunOptions.Experimental.RunHandlerPoolOptions)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RunOptions_Experimental_RunHandlerPoolOptions::IsInitialized() const {
  return true;
}

void RunOptions_Experimental_RunHandlerPoolOptions::InternalSwap(RunOptions_Experimental_RunHandlerPoolOptions* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  swap(_impl_.priority_, other->_impl_.priority_);
}

::PROTOBUF_NAMESPACE_ID::Metadata RunOptions_Experimental_RunHandlerPoolOptions::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[11]);
}

// ===================================================================

class RunOptions_Experimental::_Internal {
 public:
  static const ::tensorflow::RunOptions_Experimental_RunHandlerPoolOptions& run_handler_pool_options(const RunOptions_Experimental* msg);
};

const ::tensorflow::RunOptions_Experimental_RunHandlerPoolOptions&
RunOptions_Experimental::_Internal::run_handler_pool_options(const RunOptions_Experimental* msg) {
  return *msg->_impl_.run_handler_pool_options_;
}
RunOptions_Experimental::RunOptions_Experimental(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.RunOptions.Experimental)
}
RunOptions_Experimental::RunOptions_Experimental(const RunOptions_Experimental& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  RunOptions_Experimental* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.run_handler_pool_options_){nullptr}
    , decltype(_impl_.collective_graph_key_){}
    , decltype(_impl_.use_run_handler_pool_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_run_handler_pool_options()) {
    _this->_impl_.run_handler_pool_options_ = new ::tensorflow::RunOptions_Experimental_RunHandlerPoolOptions(*from._impl_.run_handler_pool_options_);
  }
  ::memcpy(&_impl_.collective_graph_key_, &from._impl_.collective_graph_key_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.use_run_handler_pool_) -
    reinterpret_cast<char*>(&_impl_.collective_graph_key_)) + sizeof(_impl_.use_run_handler_pool_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.RunOptions.Experimental)
}

inline void RunOptions_Experimental::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.run_handler_pool_options_){nullptr}
    , decltype(_impl_.collective_graph_key_){int64_t{0}}
    , decltype(_impl_.use_run_handler_pool_){false}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

RunOptions_Experimental::~RunOptions_Experimental() {
  // @@protoc_insertion_point(destructor:tensorflow.RunOptions.Experimental)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void RunOptions_Experimental::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  if (this != internal_default_instance()) delete _impl_.run_handler_pool_options_;
}

void RunOptions_Experimental::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void RunOptions_Experimental::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.RunOptions.Experimental)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaForAllocation() == nullptr && _impl_.run_handler_pool_options_ != nullptr) {
    delete _impl_.run_handler_pool_options_;
  }
  _impl_.run_handler_pool_options_ = nullptr;
  ::memset(&_impl_.collective_graph_key_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.use_run_handler_pool_) -
      reinterpret_cast<char*>(&_impl_.collective_graph_key_)) + sizeof(_impl_.use_run_handler_pool_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* RunOptions_Experimental::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // int64 collective_graph_key = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          _impl_.collective_graph_key_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool use_run_handler_pool = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.use_run_handler_pool_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.RunOptions.Experimental.RunHandlerPoolOptions run_handler_pool_options = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          ptr = ctx->ParseMessage(_internal_mutable_run_handler_pool_options(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* RunOptions_Experimental::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.RunOptions.Experimental)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 collective_graph_key = 1;
  if (this->_internal_collective_graph_key() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(1, this->_internal_collective_graph_key(), target);
  }

  // bool use_run_handler_pool = 2;
  if (this->_internal_use_run_handler_pool() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(2, this->_internal_use_run_handler_pool(), target);
  }

  // .tensorflow.RunOptions.Experimental.RunHandlerPoolOptions run_handler_pool_options = 3;
  if (this->_internal_has_run_handler_pool_options()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(3, _Internal::run_handler_pool_options(this),
        _Internal::run_handler_pool_options(this).GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.RunOptions.Experimental)
  return target;
}

size_t RunOptions_Experimental::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.RunOptions.Experimental)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .tensorflow.RunOptions.Experimental.RunHandlerPoolOptions run_handler_pool_options = 3;
  if (this->_internal_has_run_handler_pool_options()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.run_handler_pool_options_);
  }

  // int64 collective_graph_key = 1;
  if (this->_internal_collective_graph_key() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_collective_graph_key());
  }

  // bool use_run_handler_pool = 2;
  if (this->_internal_use_run_handler_pool() != 0) {
    total_size += 1 + 1;
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData RunOptions_Experimental::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    RunOptions_Experimental::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*RunOptions_Experimental::GetClassData() const { return &_class_data_; }


void RunOptions_Experimental::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<RunOptions_Experimental*>(&to_msg);
  auto& from = static_cast<const RunOptions_Experimental&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.RunOptions.Experimental)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_has_run_handler_pool_options()) {
    _this->_internal_mutable_run_handler_pool_options()->::tensorflow::RunOptions_Experimental_RunHandlerPoolOptions::MergeFrom(
        from._internal_run_handler_pool_options());
  }
  if (from._internal_collective_graph_key() != 0) {
    _this->_internal_set_collective_graph_key(from._internal_collective_graph_key());
  }
  if (from._internal_use_run_handler_pool() != 0) {
    _this->_internal_set_use_run_handler_pool(from._internal_use_run_handler_pool());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void RunOptions_Experimental::CopyFrom(const RunOptions_Experimental& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.RunOptions.Experimental)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RunOptions_Experimental::IsInitialized() const {
  return true;
}

void RunOptions_Experimental::InternalSwap(RunOptions_Experimental* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(RunOptions_Experimental, _impl_.use_run_handler_pool_)
      + sizeof(RunOptions_Experimental::_impl_.use_run_handler_pool_)
      - PROTOBUF_FIELD_OFFSET(RunOptions_Experimental, _impl_.run_handler_pool_options_)>(
          reinterpret_cast<char*>(&_impl_.run_handler_pool_options_),
          reinterpret_cast<char*>(&other->_impl_.run_handler_pool_options_));
}

::PROTOBUF_NAMESPACE_ID::Metadata RunOptions_Experimental::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[12]);
}

// ===================================================================

class RunOptions::_Internal {
 public:
  static const ::tensorflow::DebugOptions& debug_options(const RunOptions* msg);
  static const ::tensorflow::RunOptions_Experimental& experimental(const RunOptions* msg);
};

const ::tensorflow::DebugOptions&
RunOptions::_Internal::debug_options(const RunOptions* msg) {
  return *msg->_impl_.debug_options_;
}
const ::tensorflow::RunOptions_Experimental&
RunOptions::_Internal::experimental(const RunOptions* msg) {
  return *msg->_impl_.experimental_;
}
void RunOptions::clear_debug_options() {
  if (GetArenaForAllocation() == nullptr && _impl_.debug_options_ != nullptr) {
    delete _impl_.debug_options_;
  }
  _impl_.debug_options_ = nullptr;
}
RunOptions::RunOptions(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.RunOptions)
}
RunOptions::RunOptions(const RunOptions& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  RunOptions* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.debug_options_){nullptr}
    , decltype(_impl_.experimental_){nullptr}
    , decltype(_impl_.timeout_in_ms_){}
    , decltype(_impl_.trace_level_){}
    , decltype(_impl_.inter_op_thread_pool_){}
    , decltype(_impl_.output_partition_graphs_){}
    , decltype(_impl_.report_tensor_allocations_upon_oom_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_debug_options()) {
    _this->_impl_.debug_options_ = new ::tensorflow::DebugOptions(*from._impl_.debug_options_);
  }
  if (from._internal_has_experimental()) {
    _this->_impl_.experimental_ = new ::tensorflow::RunOptions_Experimental(*from._impl_.experimental_);
  }
  ::memcpy(&_impl_.timeout_in_ms_, &from._impl_.timeout_in_ms_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.report_tensor_allocations_upon_oom_) -
    reinterpret_cast<char*>(&_impl_.timeout_in_ms_)) + sizeof(_impl_.report_tensor_allocations_upon_oom_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.RunOptions)
}

inline void RunOptions::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.debug_options_){nullptr}
    , decltype(_impl_.experimental_){nullptr}
    , decltype(_impl_.timeout_in_ms_){int64_t{0}}
    , decltype(_impl_.trace_level_){0}
    , decltype(_impl_.inter_op_thread_pool_){0}
    , decltype(_impl_.output_partition_graphs_){false}
    , decltype(_impl_.report_tensor_allocations_upon_oom_){false}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

RunOptions::~RunOptions() {
  // @@protoc_insertion_point(destructor:tensorflow.RunOptions)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void RunOptions::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  if (this != internal_default_instance()) delete _impl_.debug_options_;
  if (this != internal_default_instance()) delete _impl_.experimental_;
}

void RunOptions::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void RunOptions::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.RunOptions)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaForAllocation() == nullptr && _impl_.debug_options_ != nullptr) {
    delete _impl_.debug_options_;
  }
  _impl_.debug_options_ = nullptr;
  if (GetArenaForAllocation() == nullptr && _impl_.experimental_ != nullptr) {
    delete _impl_.experimental_;
  }
  _impl_.experimental_ = nullptr;
  ::memset(&_impl_.timeout_in_ms_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.report_tensor_allocations_upon_oom_) -
      reinterpret_cast<char*>(&_impl_.timeout_in_ms_)) + sizeof(_impl_.report_tensor_allocations_upon_oom_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* RunOptions::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // .tensorflow.RunOptions.TraceLevel trace_level = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          uint64_t val = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
          _internal_set_trace_level(static_cast<::tensorflow::RunOptions_TraceLevel>(val));
        } else
          goto handle_unusual;
        continue;
      // int64 timeout_in_ms = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.timeout_in_ms_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int32 inter_op_thread_pool = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          _impl_.inter_op_thread_pool_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool output_partition_graphs = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 40)) {
          _impl_.output_partition_graphs_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.DebugOptions debug_options = 6;
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 50)) {
          ptr = ctx->ParseMessage(_internal_mutable_debug_options(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // bool report_tensor_allocations_upon_oom = 7;
      case 7:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 56)) {
          _impl_.report_tensor_allocations_upon_oom_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.RunOptions.Experimental experimental = 8;
      case 8:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 66)) {
          ptr = ctx->ParseMessage(_internal_mutable_experimental(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* RunOptions::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.RunOptions)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // .tensorflow.RunOptions.TraceLevel trace_level = 1;
  if (this->_internal_trace_level() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteEnumToArray(
      1, this->_internal_trace_level(), target);
  }

  // int64 timeout_in_ms = 2;
  if (this->_internal_timeout_in_ms() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(2, this->_internal_timeout_in_ms(), target);
  }

  // int32 inter_op_thread_pool = 3;
  if (this->_internal_inter_op_thread_pool() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(3, this->_internal_inter_op_thread_pool(), target);
  }

  // bool output_partition_graphs = 5;
  if (this->_internal_output_partition_graphs() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(5, this->_internal_output_partition_graphs(), target);
  }

  // .tensorflow.DebugOptions debug_options = 6;
  if (this->_internal_has_debug_options()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(6, _Internal::debug_options(this),
        _Internal::debug_options(this).GetCachedSize(), target, stream);
  }

  // bool report_tensor_allocations_upon_oom = 7;
  if (this->_internal_report_tensor_allocations_upon_oom() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(7, this->_internal_report_tensor_allocations_upon_oom(), target);
  }

  // .tensorflow.RunOptions.Experimental experimental = 8;
  if (this->_internal_has_experimental()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(8, _Internal::experimental(this),
        _Internal::experimental(this).GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.RunOptions)
  return target;
}

size_t RunOptions::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.RunOptions)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .tensorflow.DebugOptions debug_options = 6;
  if (this->_internal_has_debug_options()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.debug_options_);
  }

  // .tensorflow.RunOptions.Experimental experimental = 8;
  if (this->_internal_has_experimental()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.experimental_);
  }

  // int64 timeout_in_ms = 2;
  if (this->_internal_timeout_in_ms() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_timeout_in_ms());
  }

  // .tensorflow.RunOptions.TraceLevel trace_level = 1;
  if (this->_internal_trace_level() != 0) {
    total_size += 1 +
      ::_pbi::WireFormatLite::EnumSize(this->_internal_trace_level());
  }

  // int32 inter_op_thread_pool = 3;
  if (this->_internal_inter_op_thread_pool() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_inter_op_thread_pool());
  }

  // bool output_partition_graphs = 5;
  if (this->_internal_output_partition_graphs() != 0) {
    total_size += 1 + 1;
  }

  // bool report_tensor_allocations_upon_oom = 7;
  if (this->_internal_report_tensor_allocations_upon_oom() != 0) {
    total_size += 1 + 1;
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData RunOptions::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    RunOptions::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*RunOptions::GetClassData() const { return &_class_data_; }


void RunOptions::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<RunOptions*>(&to_msg);
  auto& from = static_cast<const RunOptions&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.RunOptions)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_has_debug_options()) {
    _this->_internal_mutable_debug_options()->::tensorflow::DebugOptions::MergeFrom(
        from._internal_debug_options());
  }
  if (from._internal_has_experimental()) {
    _this->_internal_mutable_experimental()->::tensorflow::RunOptions_Experimental::MergeFrom(
        from._internal_experimental());
  }
  if (from._internal_timeout_in_ms() != 0) {
    _this->_internal_set_timeout_in_ms(from._internal_timeout_in_ms());
  }
  if (from._internal_trace_level() != 0) {
    _this->_internal_set_trace_level(from._internal_trace_level());
  }
  if (from._internal_inter_op_thread_pool() != 0) {
    _this->_internal_set_inter_op_thread_pool(from._internal_inter_op_thread_pool());
  }
  if (from._internal_output_partition_graphs() != 0) {
    _this->_internal_set_output_partition_graphs(from._internal_output_partition_graphs());
  }
  if (from._internal_report_tensor_allocations_upon_oom() != 0) {
    _this->_internal_set_report_tensor_allocations_upon_oom(from._internal_report_tensor_allocations_upon_oom());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void RunOptions::CopyFrom(const RunOptions& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.RunOptions)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RunOptions::IsInitialized() const {
  return true;
}

void RunOptions::InternalSwap(RunOptions* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(RunOptions, _impl_.report_tensor_allocations_upon_oom_)
      + sizeof(RunOptions::_impl_.report_tensor_allocations_upon_oom_)
      - PROTOBUF_FIELD_OFFSET(RunOptions, _impl_.debug_options_)>(
          reinterpret_cast<char*>(&_impl_.debug_options_),
          reinterpret_cast<char*>(&other->_impl_.debug_options_));
}

::PROTOBUF_NAMESPACE_ID::Metadata RunOptions::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[13]);
}

// ===================================================================

class RunMetadata_FunctionGraphs::_Internal {
 public:
  static const ::tensorflow::GraphDef& pre_optimization_graph(const RunMetadata_FunctionGraphs* msg);
  static const ::tensorflow::GraphDef& post_optimization_graph(const RunMetadata_FunctionGraphs* msg);
};

const ::tensorflow::GraphDef&
RunMetadata_FunctionGraphs::_Internal::pre_optimization_graph(const RunMetadata_FunctionGraphs* msg) {
  return *msg->_impl_.pre_optimization_graph_;
}
const ::tensorflow::GraphDef&
RunMetadata_FunctionGraphs::_Internal::post_optimization_graph(const RunMetadata_FunctionGraphs* msg) {
  return *msg->_impl_.post_optimization_graph_;
}
void RunMetadata_FunctionGraphs::clear_partition_graphs() {
  _impl_.partition_graphs_.Clear();
}
void RunMetadata_FunctionGraphs::clear_pre_optimization_graph() {
  if (GetArenaForAllocation() == nullptr && _impl_.pre_optimization_graph_ != nullptr) {
    delete _impl_.pre_optimization_graph_;
  }
  _impl_.pre_optimization_graph_ = nullptr;
}
void RunMetadata_FunctionGraphs::clear_post_optimization_graph() {
  if (GetArenaForAllocation() == nullptr && _impl_.post_optimization_graph_ != nullptr) {
    delete _impl_.post_optimization_graph_;
  }
  _impl_.post_optimization_graph_ = nullptr;
}
RunMetadata_FunctionGraphs::RunMetadata_FunctionGraphs(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.RunMetadata.FunctionGraphs)
}
RunMetadata_FunctionGraphs::RunMetadata_FunctionGraphs(const RunMetadata_FunctionGraphs& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  RunMetadata_FunctionGraphs* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.partition_graphs_){from._impl_.partition_graphs_}
    , decltype(_impl_.pre_optimization_graph_){nullptr}
    , decltype(_impl_.post_optimization_graph_){nullptr}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_pre_optimization_graph()) {
    _this->_impl_.pre_optimization_graph_ = new ::tensorflow::GraphDef(*from._impl_.pre_optimization_graph_);
  }
  if (from._internal_has_post_optimization_graph()) {
    _this->_impl_.post_optimization_graph_ = new ::tensorflow::GraphDef(*from._impl_.post_optimization_graph_);
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.RunMetadata.FunctionGraphs)
}

inline void RunMetadata_FunctionGraphs::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.partition_graphs_){arena}
    , decltype(_impl_.pre_optimization_graph_){nullptr}
    , decltype(_impl_.post_optimization_graph_){nullptr}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

RunMetadata_FunctionGraphs::~RunMetadata_FunctionGraphs() {
  // @@protoc_insertion_point(destructor:tensorflow.RunMetadata.FunctionGraphs)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void RunMetadata_FunctionGraphs::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.partition_graphs_.~RepeatedPtrField();
  if (this != internal_default_instance()) delete _impl_.pre_optimization_graph_;
  if (this != internal_default_instance()) delete _impl_.post_optimization_graph_;
}

void RunMetadata_FunctionGraphs::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void RunMetadata_FunctionGraphs::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.RunMetadata.FunctionGraphs)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.partition_graphs_.Clear();
  if (GetArenaForAllocation() == nullptr && _impl_.pre_optimization_graph_ != nullptr) {
    delete _impl_.pre_optimization_graph_;
  }
  _impl_.pre_optimization_graph_ = nullptr;
  if (GetArenaForAllocation() == nullptr && _impl_.post_optimization_graph_ != nullptr) {
    delete _impl_.post_optimization_graph_;
  }
  _impl_.post_optimization_graph_ = nullptr;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* RunMetadata_FunctionGraphs::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // repeated .tensorflow.GraphDef partition_graphs = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_partition_graphs(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<10>(ptr));
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.GraphDef pre_optimization_graph = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          ptr = ctx->ParseMessage(_internal_mutable_pre_optimization_graph(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.GraphDef post_optimization_graph = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          ptr = ctx->ParseMessage(_internal_mutable_post_optimization_graph(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* RunMetadata_FunctionGraphs::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.RunMetadata.FunctionGraphs)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .tensorflow.GraphDef partition_graphs = 1;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_partition_graphs_size()); i < n; i++) {
    const auto& repfield = this->_internal_partition_graphs(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(1, repfield, repfield.GetCachedSize(), target, stream);
  }

  // .tensorflow.GraphDef pre_optimization_graph = 2;
  if (this->_internal_has_pre_optimization_graph()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(2, _Internal::pre_optimization_graph(this),
        _Internal::pre_optimization_graph(this).GetCachedSize(), target, stream);
  }

  // .tensorflow.GraphDef post_optimization_graph = 3;
  if (this->_internal_has_post_optimization_graph()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(3, _Internal::post_optimization_graph(this),
        _Internal::post_optimization_graph(this).GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.RunMetadata.FunctionGraphs)
  return target;
}

size_t RunMetadata_FunctionGraphs::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.RunMetadata.FunctionGraphs)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.GraphDef partition_graphs = 1;
  total_size += 1UL * this->_internal_partition_graphs_size();
  for (const auto& msg : this->_impl_.partition_graphs_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // .tensorflow.GraphDef pre_optimization_graph = 2;
  if (this->_internal_has_pre_optimization_graph()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.pre_optimization_graph_);
  }

  // .tensorflow.GraphDef post_optimization_graph = 3;
  if (this->_internal_has_post_optimization_graph()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.post_optimization_graph_);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData RunMetadata_FunctionGraphs::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    RunMetadata_FunctionGraphs::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*RunMetadata_FunctionGraphs::GetClassData() const { return &_class_data_; }


void RunMetadata_FunctionGraphs::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<RunMetadata_FunctionGraphs*>(&to_msg);
  auto& from = static_cast<const RunMetadata_FunctionGraphs&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.RunMetadata.FunctionGraphs)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.partition_graphs_.MergeFrom(from._impl_.partition_graphs_);
  if (from._internal_has_pre_optimization_graph()) {
    _this->_internal_mutable_pre_optimization_graph()->::tensorflow::GraphDef::MergeFrom(
        from._internal_pre_optimization_graph());
  }
  if (from._internal_has_post_optimization_graph()) {
    _this->_internal_mutable_post_optimization_graph()->::tensorflow::GraphDef::MergeFrom(
        from._internal_post_optimization_graph());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void RunMetadata_FunctionGraphs::CopyFrom(const RunMetadata_FunctionGraphs& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.RunMetadata.FunctionGraphs)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RunMetadata_FunctionGraphs::IsInitialized() const {
  return true;
}

void RunMetadata_FunctionGraphs::InternalSwap(RunMetadata_FunctionGraphs* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.partition_graphs_.InternalSwap(&other->_impl_.partition_graphs_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(RunMetadata_FunctionGraphs, _impl_.post_optimization_graph_)
      + sizeof(RunMetadata_FunctionGraphs::_impl_.post_optimization_graph_)
      - PROTOBUF_FIELD_OFFSET(RunMetadata_FunctionGraphs, _impl_.pre_optimization_graph_)>(
          reinterpret_cast<char*>(&_impl_.pre_optimization_graph_),
          reinterpret_cast<char*>(&other->_impl_.pre_optimization_graph_));
}

::PROTOBUF_NAMESPACE_ID::Metadata RunMetadata_FunctionGraphs::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[14]);
}

// ===================================================================

class RunMetadata::_Internal {
 public:
  static const ::tensorflow::StepStats& step_stats(const RunMetadata* msg);
  static const ::tensorflow::CostGraphDef& cost_graph(const RunMetadata* msg);
  static const ::tensorflow::SessionMetadata& session_metadata(const RunMetadata* msg);
};

const ::tensorflow::StepStats&
RunMetadata::_Internal::step_stats(const RunMetadata* msg) {
  return *msg->_impl_.step_stats_;
}
const ::tensorflow::CostGraphDef&
RunMetadata::_Internal::cost_graph(const RunMetadata* msg) {
  return *msg->_impl_.cost_graph_;
}
const ::tensorflow::SessionMetadata&
RunMetadata::_Internal::session_metadata(const RunMetadata* msg) {
  return *msg->_impl_.session_metadata_;
}
void RunMetadata::clear_step_stats() {
  if (GetArenaForAllocation() == nullptr && _impl_.step_stats_ != nullptr) {
    delete _impl_.step_stats_;
  }
  _impl_.step_stats_ = nullptr;
}
void RunMetadata::clear_cost_graph() {
  if (GetArenaForAllocation() == nullptr && _impl_.cost_graph_ != nullptr) {
    delete _impl_.cost_graph_;
  }
  _impl_.cost_graph_ = nullptr;
}
void RunMetadata::clear_partition_graphs() {
  _impl_.partition_graphs_.Clear();
}
RunMetadata::RunMetadata(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.RunMetadata)
}
RunMetadata::RunMetadata(const RunMetadata& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  RunMetadata* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.partition_graphs_){from._impl_.partition_graphs_}
    , decltype(_impl_.function_graphs_){from._impl_.function_graphs_}
    , decltype(_impl_.step_stats_){nullptr}
    , decltype(_impl_.cost_graph_){nullptr}
    , decltype(_impl_.session_metadata_){nullptr}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_step_stats()) {
    _this->_impl_.step_stats_ = new ::tensorflow::StepStats(*from._impl_.step_stats_);
  }
  if (from._internal_has_cost_graph()) {
    _this->_impl_.cost_graph_ = new ::tensorflow::CostGraphDef(*from._impl_.cost_graph_);
  }
  if (from._internal_has_session_metadata()) {
    _this->_impl_.session_metadata_ = new ::tensorflow::SessionMetadata(*from._impl_.session_metadata_);
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.RunMetadata)
}

inline void RunMetadata::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.partition_graphs_){arena}
    , decltype(_impl_.function_graphs_){arena}
    , decltype(_impl_.step_stats_){nullptr}
    , decltype(_impl_.cost_graph_){nullptr}
    , decltype(_impl_.session_metadata_){nullptr}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

RunMetadata::~RunMetadata() {
  // @@protoc_insertion_point(destructor:tensorflow.RunMetadata)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void RunMetadata::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.partition_graphs_.~RepeatedPtrField();
  _impl_.function_graphs_.~RepeatedPtrField();
  if (this != internal_default_instance()) delete _impl_.step_stats_;
  if (this != internal_default_instance()) delete _impl_.cost_graph_;
  if (this != internal_default_instance()) delete _impl_.session_metadata_;
}

void RunMetadata::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void RunMetadata::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.RunMetadata)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.partition_graphs_.Clear();
  _impl_.function_graphs_.Clear();
  if (GetArenaForAllocation() == nullptr && _impl_.step_stats_ != nullptr) {
    delete _impl_.step_stats_;
  }
  _impl_.step_stats_ = nullptr;
  if (GetArenaForAllocation() == nullptr && _impl_.cost_graph_ != nullptr) {
    delete _impl_.cost_graph_;
  }
  _impl_.cost_graph_ = nullptr;
  if (GetArenaForAllocation() == nullptr && _impl_.session_metadata_ != nullptr) {
    delete _impl_.session_metadata_;
  }
  _impl_.session_metadata_ = nullptr;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* RunMetadata::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // .tensorflow.StepStats step_stats = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr = ctx->ParseMessage(_internal_mutable_step_stats(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.CostGraphDef cost_graph = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          ptr = ctx->ParseMessage(_internal_mutable_cost_graph(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // repeated .tensorflow.GraphDef partition_graphs = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_partition_graphs(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<26>(ptr));
        } else
          goto handle_unusual;
        continue;
      // repeated .tensorflow.RunMetadata.FunctionGraphs function_graphs = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 34)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_function_graphs(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<34>(ptr));
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.SessionMetadata session_metadata = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 42)) {
          ptr = ctx->ParseMessage(_internal_mutable_session_metadata(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* RunMetadata::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.RunMetadata)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // .tensorflow.StepStats step_stats = 1;
  if (this->_internal_has_step_stats()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(1, _Internal::step_stats(this),
        _Internal::step_stats(this).GetCachedSize(), target, stream);
  }

  // .tensorflow.CostGraphDef cost_graph = 2;
  if (this->_internal_has_cost_graph()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(2, _Internal::cost_graph(this),
        _Internal::cost_graph(this).GetCachedSize(), target, stream);
  }

  // repeated .tensorflow.GraphDef partition_graphs = 3;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_partition_graphs_size()); i < n; i++) {
    const auto& repfield = this->_internal_partition_graphs(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(3, repfield, repfield.GetCachedSize(), target, stream);
  }

  // repeated .tensorflow.RunMetadata.FunctionGraphs function_graphs = 4;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_function_graphs_size()); i < n; i++) {
    const auto& repfield = this->_internal_function_graphs(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(4, repfield, repfield.GetCachedSize(), target, stream);
  }

  // .tensorflow.SessionMetadata session_metadata = 5;
  if (this->_internal_has_session_metadata()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(5, _Internal::session_metadata(this),
        _Internal::session_metadata(this).GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.RunMetadata)
  return target;
}

size_t RunMetadata::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.RunMetadata)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.GraphDef partition_graphs = 3;
  total_size += 1UL * this->_internal_partition_graphs_size();
  for (const auto& msg : this->_impl_.partition_graphs_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // repeated .tensorflow.RunMetadata.FunctionGraphs function_graphs = 4;
  total_size += 1UL * this->_internal_function_graphs_size();
  for (const auto& msg : this->_impl_.function_graphs_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // .tensorflow.StepStats step_stats = 1;
  if (this->_internal_has_step_stats()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.step_stats_);
  }

  // .tensorflow.CostGraphDef cost_graph = 2;
  if (this->_internal_has_cost_graph()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.cost_graph_);
  }

  // .tensorflow.SessionMetadata session_metadata = 5;
  if (this->_internal_has_session_metadata()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.session_metadata_);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData RunMetadata::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    RunMetadata::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*RunMetadata::GetClassData() const { return &_class_data_; }


void RunMetadata::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<RunMetadata*>(&to_msg);
  auto& from = static_cast<const RunMetadata&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.RunMetadata)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.partition_graphs_.MergeFrom(from._impl_.partition_graphs_);
  _this->_impl_.function_graphs_.MergeFrom(from._impl_.function_graphs_);
  if (from._internal_has_step_stats()) {
    _this->_internal_mutable_step_stats()->::tensorflow::StepStats::MergeFrom(
        from._internal_step_stats());
  }
  if (from._internal_has_cost_graph()) {
    _this->_internal_mutable_cost_graph()->::tensorflow::CostGraphDef::MergeFrom(
        from._internal_cost_graph());
  }
  if (from._internal_has_session_metadata()) {
    _this->_internal_mutable_session_metadata()->::tensorflow::SessionMetadata::MergeFrom(
        from._internal_session_metadata());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void RunMetadata::CopyFrom(const RunMetadata& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.RunMetadata)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RunMetadata::IsInitialized() const {
  return true;
}

void RunMetadata::InternalSwap(RunMetadata* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.partition_graphs_.InternalSwap(&other->_impl_.partition_graphs_);
  _impl_.function_graphs_.InternalSwap(&other->_impl_.function_graphs_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(RunMetadata, _impl_.session_metadata_)
      + sizeof(RunMetadata::_impl_.session_metadata_)
      - PROTOBUF_FIELD_OFFSET(RunMetadata, _impl_.step_stats_)>(
          reinterpret_cast<char*>(&_impl_.step_stats_),
          reinterpret_cast<char*>(&other->_impl_.step_stats_));
}

::PROTOBUF_NAMESPACE_ID::Metadata RunMetadata::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[15]);
}

// ===================================================================

class TensorConnection::_Internal {
 public:
};

TensorConnection::TensorConnection(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.TensorConnection)
}
TensorConnection::TensorConnection(const TensorConnection& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  TensorConnection* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.from_tensor_){}
    , decltype(_impl_.to_tensor_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _impl_.from_tensor_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.from_tensor_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_from_tensor().empty()) {
    _this->_impl_.from_tensor_.Set(from._internal_from_tensor(), 
      _this->GetArenaForAllocation());
  }
  _impl_.to_tensor_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.to_tensor_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_to_tensor().empty()) {
    _this->_impl_.to_tensor_.Set(from._internal_to_tensor(), 
      _this->GetArenaForAllocation());
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.TensorConnection)
}

inline void TensorConnection::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.from_tensor_){}
    , decltype(_impl_.to_tensor_){}
    , /*decltype(_impl_._cached_size_)*/{}
  };
  _impl_.from_tensor_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.from_tensor_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  _impl_.to_tensor_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.to_tensor_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
}

TensorConnection::~TensorConnection() {
  // @@protoc_insertion_point(destructor:tensorflow.TensorConnection)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void TensorConnection::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.from_tensor_.Destroy();
  _impl_.to_tensor_.Destroy();
}

void TensorConnection::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void TensorConnection::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.TensorConnection)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.from_tensor_.ClearToEmpty();
  _impl_.to_tensor_.ClearToEmpty();
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* TensorConnection::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // string from_tensor = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          auto str = _internal_mutable_from_tensor();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.TensorConnection.from_tensor"));
        } else
          goto handle_unusual;
        continue;
      // string to_tensor = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          auto str = _internal_mutable_to_tensor();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.TensorConnection.to_tensor"));
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* TensorConnection::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.TensorConnection)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // string from_tensor = 1;
  if (!this->_internal_from_tensor().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_from_tensor().data(), static_cast<int>(this->_internal_from_tensor().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.TensorConnection.from_tensor");
    target = stream->WriteStringMaybeAliased(
        1, this->_internal_from_tensor(), target);
  }

  // string to_tensor = 2;
  if (!this->_internal_to_tensor().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_to_tensor().data(), static_cast<int>(this->_internal_to_tensor().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.TensorConnection.to_tensor");
    target = stream->WriteStringMaybeAliased(
        2, this->_internal_to_tensor(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.TensorConnection)
  return target;
}

size_t TensorConnection::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.TensorConnection)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // string from_tensor = 1;
  if (!this->_internal_from_tensor().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_from_tensor());
  }

  // string to_tensor = 2;
  if (!this->_internal_to_tensor().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_to_tensor());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData TensorConnection::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    TensorConnection::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*TensorConnection::GetClassData() const { return &_class_data_; }


void TensorConnection::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<TensorConnection*>(&to_msg);
  auto& from = static_cast<const TensorConnection&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.TensorConnection)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (!from._internal_from_tensor().empty()) {
    _this->_internal_set_from_tensor(from._internal_from_tensor());
  }
  if (!from._internal_to_tensor().empty()) {
    _this->_internal_set_to_tensor(from._internal_to_tensor());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void TensorConnection::CopyFrom(const TensorConnection& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.TensorConnection)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool TensorConnection::IsInitialized() const {
  return true;
}

void TensorConnection::InternalSwap(TensorConnection* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.from_tensor_, lhs_arena,
      &other->_impl_.from_tensor_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.to_tensor_, lhs_arena,
      &other->_impl_.to_tensor_, rhs_arena
  );
}

::PROTOBUF_NAMESPACE_ID::Metadata TensorConnection::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[16]);
}

// ===================================================================

CallableOptions_FeedDevicesEntry_DoNotUse::CallableOptions_FeedDevicesEntry_DoNotUse() {}
CallableOptions_FeedDevicesEntry_DoNotUse::CallableOptions_FeedDevicesEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena)
    : SuperType(arena) {}
void CallableOptions_FeedDevicesEntry_DoNotUse::MergeFrom(const CallableOptions_FeedDevicesEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::PROTOBUF_NAMESPACE_ID::Metadata CallableOptions_FeedDevicesEntry_DoNotUse::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[17]);
}

// ===================================================================

CallableOptions_FetchDevicesEntry_DoNotUse::CallableOptions_FetchDevicesEntry_DoNotUse() {}
CallableOptions_FetchDevicesEntry_DoNotUse::CallableOptions_FetchDevicesEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena)
    : SuperType(arena) {}
void CallableOptions_FetchDevicesEntry_DoNotUse::MergeFrom(const CallableOptions_FetchDevicesEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::PROTOBUF_NAMESPACE_ID::Metadata CallableOptions_FetchDevicesEntry_DoNotUse::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[18]);
}

// ===================================================================

class CallableOptions::_Internal {
 public:
  static const ::tensorflow::RunOptions& run_options(const CallableOptions* msg);
};

const ::tensorflow::RunOptions&
CallableOptions::_Internal::run_options(const CallableOptions* msg) {
  return *msg->_impl_.run_options_;
}
CallableOptions::CallableOptions(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  if (arena != nullptr && !is_message_owned) {
    arena->OwnCustomDestructor(this, &CallableOptions::ArenaDtor);
  }
  // @@protoc_insertion_point(arena_constructor:tensorflow.CallableOptions)
}
CallableOptions::CallableOptions(const CallableOptions& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  CallableOptions* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.feed_){from._impl_.feed_}
    , decltype(_impl_.fetch_){from._impl_.fetch_}
    , decltype(_impl_.target_){from._impl_.target_}
    , decltype(_impl_.tensor_connection_){from._impl_.tensor_connection_}
    , /*decltype(_impl_.feed_devices_)*/{}
    , /*decltype(_impl_.fetch_devices_)*/{}
    , decltype(_impl_.run_options_){nullptr}
    , decltype(_impl_.fetch_skip_sync_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _this->_impl_.feed_devices_.MergeFrom(from._impl_.feed_devices_);
  _this->_impl_.fetch_devices_.MergeFrom(from._impl_.fetch_devices_);
  if (from._internal_has_run_options()) {
    _this->_impl_.run_options_ = new ::tensorflow::RunOptions(*from._impl_.run_options_);
  }
  _this->_impl_.fetch_skip_sync_ = from._impl_.fetch_skip_sync_;
  // @@protoc_insertion_point(copy_constructor:tensorflow.CallableOptions)
}

inline void CallableOptions::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.feed_){arena}
    , decltype(_impl_.fetch_){arena}
    , decltype(_impl_.target_){arena}
    , decltype(_impl_.tensor_connection_){arena}
    , /*decltype(_impl_.feed_devices_)*/{::_pbi::ArenaInitialized(), arena}
    , /*decltype(_impl_.fetch_devices_)*/{::_pbi::ArenaInitialized(), arena}
    , decltype(_impl_.run_options_){nullptr}
    , decltype(_impl_.fetch_skip_sync_){false}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

CallableOptions::~CallableOptions() {
  // @@protoc_insertion_point(destructor:tensorflow.CallableOptions)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    ArenaDtor(this);
    return;
  }
  SharedDtor();
}

inline void CallableOptions::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.feed_.~RepeatedPtrField();
  _impl_.fetch_.~RepeatedPtrField();
  _impl_.target_.~RepeatedPtrField();
  _impl_.tensor_connection_.~RepeatedPtrField();
  _impl_.feed_devices_.Destruct();
  _impl_.feed_devices_.~MapField();
  _impl_.fetch_devices_.Destruct();
  _impl_.fetch_devices_.~MapField();
  if (this != internal_default_instance()) delete _impl_.run_options_;
}

void CallableOptions::ArenaDtor(void* object) {
  CallableOptions* _this = reinterpret_cast< CallableOptions* >(object);
  _this->_impl_.feed_devices_.Destruct();
  _this->_impl_.fetch_devices_.Destruct();
}
void CallableOptions::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void CallableOptions::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.CallableOptions)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.feed_.Clear();
  _impl_.fetch_.Clear();
  _impl_.target_.Clear();
  _impl_.tensor_connection_.Clear();
  _impl_.feed_devices_.Clear();
  _impl_.fetch_devices_.Clear();
  if (GetArenaForAllocation() == nullptr && _impl_.run_options_ != nullptr) {
    delete _impl_.run_options_;
  }
  _impl_.run_options_ = nullptr;
  _impl_.fetch_skip_sync_ = false;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* CallableOptions::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // repeated string feed = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr -= 1;
          do {
            ptr += 1;
            auto str = _internal_add_feed();
            ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
            CHK_(ptr);
            CHK_(::_pbi::VerifyUTF8(str, "tensorflow.CallableOptions.feed"));
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<10>(ptr));
        } else
          goto handle_unusual;
        continue;
      // repeated string fetch = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          ptr -= 1;
          do {
            ptr += 1;
            auto str = _internal_add_fetch();
            ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
            CHK_(ptr);
            CHK_(::_pbi::VerifyUTF8(str, "tensorflow.CallableOptions.fetch"));
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<18>(ptr));
        } else
          goto handle_unusual;
        continue;
      // repeated string target = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          ptr -= 1;
          do {
            ptr += 1;
            auto str = _internal_add_target();
            ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
            CHK_(ptr);
            CHK_(::_pbi::VerifyUTF8(str, "tensorflow.CallableOptions.target"));
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<26>(ptr));
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.RunOptions run_options = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 34)) {
          ptr = ctx->ParseMessage(_internal_mutable_run_options(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // repeated .tensorflow.TensorConnection tensor_connection = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 42)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_tensor_connection(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<42>(ptr));
        } else
          goto handle_unusual;
        continue;
      // map<string, string> feed_devices = 6;
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 50)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(&_impl_.feed_devices_, ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<50>(ptr));
        } else
          goto handle_unusual;
        continue;
      // map<string, string> fetch_devices = 7;
      case 7:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 58)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(&_impl_.fetch_devices_, ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<58>(ptr));
        } else
          goto handle_unusual;
        continue;
      // bool fetch_skip_sync = 8;
      case 8:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 64)) {
          _impl_.fetch_skip_sync_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* CallableOptions::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.CallableOptions)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated string feed = 1;
  for (int i = 0, n = this->_internal_feed_size(); i < n; i++) {
    const auto& s = this->_internal_feed(i);
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      s.data(), static_cast<int>(s.length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.CallableOptions.feed");
    target = stream->WriteString(1, s, target);
  }

  // repeated string fetch = 2;
  for (int i = 0, n = this->_internal_fetch_size(); i < n; i++) {
    const auto& s = this->_internal_fetch(i);
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      s.data(), static_cast<int>(s.length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.CallableOptions.fetch");
    target = stream->WriteString(2, s, target);
  }

  // repeated string target = 3;
  for (int i = 0, n = this->_internal_target_size(); i < n; i++) {
    const auto& s = this->_internal_target(i);
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      s.data(), static_cast<int>(s.length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.CallableOptions.target");
    target = stream->WriteString(3, s, target);
  }

  // .tensorflow.RunOptions run_options = 4;
  if (this->_internal_has_run_options()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(4, _Internal::run_options(this),
        _Internal::run_options(this).GetCachedSize(), target, stream);
  }

  // repeated .tensorflow.TensorConnection tensor_connection = 5;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_tensor_connection_size()); i < n; i++) {
    const auto& repfield = this->_internal_tensor_connection(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(5, repfield, repfield.GetCachedSize(), target, stream);
  }

  // map<string, string> feed_devices = 6;
  if (!this->_internal_feed_devices().empty()) {
    using MapType = ::_pb::Map<std::string, std::string>;
    using WireHelper = CallableOptions_FeedDevicesEntry_DoNotUse::Funcs;
    const auto& map_field = this->_internal_feed_devices();
    auto check_utf8 = [](const MapType::value_type& entry) {
      (void)entry;
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
        entry.first.data(), static_cast<int>(entry.first.length()),
        ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
        "tensorflow.CallableOptions.FeedDevicesEntry.key");
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
        entry.second.data(), static_cast<int>(entry.second.length()),
        ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
        "tensorflow.CallableOptions.FeedDevicesEntry.value");
    };

    if (stream->IsSerializationDeterministic() && map_field.size() > 1) {
      for (const auto& entry : ::_pbi::MapSorterPtr<MapType>(map_field)) {
        target = WireHelper::InternalSerialize(6, entry.first, entry.second, target, stream);
        check_utf8(entry);
      }
    } else {
      for (const auto& entry : map_field) {
        target = WireHelper::InternalSerialize(6, entry.first, entry.second, target, stream);
        check_utf8(entry);
      }
    }
  }

  // map<string, string> fetch_devices = 7;
  if (!this->_internal_fetch_devices().empty()) {
    using MapType = ::_pb::Map<std::string, std::string>;
    using WireHelper = CallableOptions_FetchDevicesEntry_DoNotUse::Funcs;
    const auto& map_field = this->_internal_fetch_devices();
    auto check_utf8 = [](const MapType::value_type& entry) {
      (void)entry;
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
        entry.first.data(), static_cast<int>(entry.first.length()),
        ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
        "tensorflow.CallableOptions.FetchDevicesEntry.key");
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
        entry.second.data(), static_cast<int>(entry.second.length()),
        ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
        "tensorflow.CallableOptions.FetchDevicesEntry.value");
    };

    if (stream->IsSerializationDeterministic() && map_field.size() > 1) {
      for (const auto& entry : ::_pbi::MapSorterPtr<MapType>(map_field)) {
        target = WireHelper::InternalSerialize(7, entry.first, entry.second, target, stream);
        check_utf8(entry);
      }
    } else {
      for (const auto& entry : map_field) {
        target = WireHelper::InternalSerialize(7, entry.first, entry.second, target, stream);
        check_utf8(entry);
      }
    }
  }

  // bool fetch_skip_sync = 8;
  if (this->_internal_fetch_skip_sync() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteBoolToArray(8, this->_internal_fetch_skip_sync(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.CallableOptions)
  return target;
}

size_t CallableOptions::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.CallableOptions)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated string feed = 1;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(_impl_.feed_.size());
  for (int i = 0, n = _impl_.feed_.size(); i < n; i++) {
    total_size += ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
      _impl_.feed_.Get(i));
  }

  // repeated string fetch = 2;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(_impl_.fetch_.size());
  for (int i = 0, n = _impl_.fetch_.size(); i < n; i++) {
    total_size += ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
      _impl_.fetch_.Get(i));
  }

  // repeated string target = 3;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(_impl_.target_.size());
  for (int i = 0, n = _impl_.target_.size(); i < n; i++) {
    total_size += ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
      _impl_.target_.Get(i));
  }

  // repeated .tensorflow.TensorConnection tensor_connection = 5;
  total_size += 1UL * this->_internal_tensor_connection_size();
  for (const auto& msg : this->_impl_.tensor_connection_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // map<string, string> feed_devices = 6;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(this->_internal_feed_devices_size());
  for (::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >::const_iterator
      it = this->_internal_feed_devices().begin();
      it != this->_internal_feed_devices().end(); ++it) {
    total_size += CallableOptions_FeedDevicesEntry_DoNotUse::Funcs::ByteSizeLong(it->first, it->second);
  }

  // map<string, string> fetch_devices = 7;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(this->_internal_fetch_devices_size());
  for (::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >::const_iterator
      it = this->_internal_fetch_devices().begin();
      it != this->_internal_fetch_devices().end(); ++it) {
    total_size += CallableOptions_FetchDevicesEntry_DoNotUse::Funcs::ByteSizeLong(it->first, it->second);
  }

  // .tensorflow.RunOptions run_options = 4;
  if (this->_internal_has_run_options()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.run_options_);
  }

  // bool fetch_skip_sync = 8;
  if (this->_internal_fetch_skip_sync() != 0) {
    total_size += 1 + 1;
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData CallableOptions::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    CallableOptions::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*CallableOptions::GetClassData() const { return &_class_data_; }


void CallableOptions::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<CallableOptions*>(&to_msg);
  auto& from = static_cast<const CallableOptions&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.CallableOptions)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.feed_.MergeFrom(from._impl_.feed_);
  _this->_impl_.fetch_.MergeFrom(from._impl_.fetch_);
  _this->_impl_.target_.MergeFrom(from._impl_.target_);
  _this->_impl_.tensor_connection_.MergeFrom(from._impl_.tensor_connection_);
  _this->_impl_.feed_devices_.MergeFrom(from._impl_.feed_devices_);
  _this->_impl_.fetch_devices_.MergeFrom(from._impl_.fetch_devices_);
  if (from._internal_has_run_options()) {
    _this->_internal_mutable_run_options()->::tensorflow::RunOptions::MergeFrom(
        from._internal_run_options());
  }
  if (from._internal_fetch_skip_sync() != 0) {
    _this->_internal_set_fetch_skip_sync(from._internal_fetch_skip_sync());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void CallableOptions::CopyFrom(const CallableOptions& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.CallableOptions)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CallableOptions::IsInitialized() const {
  return true;
}

void CallableOptions::InternalSwap(CallableOptions* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.feed_.InternalSwap(&other->_impl_.feed_);
  _impl_.fetch_.InternalSwap(&other->_impl_.fetch_);
  _impl_.target_.InternalSwap(&other->_impl_.target_);
  _impl_.tensor_connection_.InternalSwap(&other->_impl_.tensor_connection_);
  _impl_.feed_devices_.InternalSwap(&other->_impl_.feed_devices_);
  _impl_.fetch_devices_.InternalSwap(&other->_impl_.fetch_devices_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(CallableOptions, _impl_.fetch_skip_sync_)
      + sizeof(CallableOptions::_impl_.fetch_skip_sync_)
      - PROTOBUF_FIELD_OFFSET(CallableOptions, _impl_.run_options_)>(
          reinterpret_cast<char*>(&_impl_.run_options_),
          reinterpret_cast<char*>(&other->_impl_.run_options_));
}

::PROTOBUF_NAMESPACE_ID::Metadata CallableOptions::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fprotobuf_2fconfig_2eproto[19]);
}

// @@protoc_insertion_point(namespace_scope)
}  // namespace tensorflow
PROTOBUF_NAMESPACE_OPEN
template<> PROTOBUF_NOINLINE ::tensorflow::GPUOptions_Experimental_VirtualDevices*
Arena::CreateMaybeMessage< ::tensorflow::GPUOptions_Experimental_VirtualDevices >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::GPUOptions_Experimental_VirtualDevices >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::GPUOptions_Experimental_StreamMergeOptions*
Arena::CreateMaybeMessage< ::tensorflow::GPUOptions_Experimental_StreamMergeOptions >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::GPUOptions_Experimental_StreamMergeOptions >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::GPUOptions_Experimental*
Arena::CreateMaybeMessage< ::tensorflow::GPUOptions_Experimental >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::GPUOptions_Experimental >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::GPUOptions*
Arena::CreateMaybeMessage< ::tensorflow::GPUOptions >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::GPUOptions >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::OptimizerOptions*
Arena::CreateMaybeMessage< ::tensorflow::OptimizerOptions >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::OptimizerOptions >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::GraphOptions*
Arena::CreateMaybeMessage< ::tensorflow::GraphOptions >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::GraphOptions >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::ThreadPoolOptionProto*
Arena::CreateMaybeMessage< ::tensorflow::ThreadPoolOptionProto >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::ThreadPoolOptionProto >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::SessionMetadata*
Arena::CreateMaybeMessage< ::tensorflow::SessionMetadata >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::SessionMetadata >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::ConfigProto_DeviceCountEntry_DoNotUse*
Arena::CreateMaybeMessage< ::tensorflow::ConfigProto_DeviceCountEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::ConfigProto_DeviceCountEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::ConfigProto_Experimental*
Arena::CreateMaybeMessage< ::tensorflow::ConfigProto_Experimental >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::ConfigProto_Experimental >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::ConfigProto*
Arena::CreateMaybeMessage< ::tensorflow::ConfigProto >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::ConfigProto >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::RunOptions_Experimental_RunHandlerPoolOptions*
Arena::CreateMaybeMessage< ::tensorflow::RunOptions_Experimental_RunHandlerPoolOptions >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::RunOptions_Experimental_RunHandlerPoolOptions >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::RunOptions_Experimental*
Arena::CreateMaybeMessage< ::tensorflow::RunOptions_Experimental >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::RunOptions_Experimental >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::RunOptions*
Arena::CreateMaybeMessage< ::tensorflow::RunOptions >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::RunOptions >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::RunMetadata_FunctionGraphs*
Arena::CreateMaybeMessage< ::tensorflow::RunMetadata_FunctionGraphs >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::RunMetadata_FunctionGraphs >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::RunMetadata*
Arena::CreateMaybeMessage< ::tensorflow::RunMetadata >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::RunMetadata >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::TensorConnection*
Arena::CreateMaybeMessage< ::tensorflow::TensorConnection >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::TensorConnection >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::CallableOptions_FeedDevicesEntry_DoNotUse*
Arena::CreateMaybeMessage< ::tensorflow::CallableOptions_FeedDevicesEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::CallableOptions_FeedDevicesEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::CallableOptions_FetchDevicesEntry_DoNotUse*
Arena::CreateMaybeMessage< ::tensorflow::CallableOptions_FetchDevicesEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::CallableOptions_FetchDevicesEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::CallableOptions*
Arena::CreateMaybeMessage< ::tensorflow::CallableOptions >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::CallableOptions >(arena);
}
PROTOBUF_NAMESPACE_CLOSE

// @@protoc_insertion_point(global_scope)
#include <google/protobuf/port_undef.inc>
