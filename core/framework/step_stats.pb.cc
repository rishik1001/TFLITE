// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/framework/step_stats.proto

#include "tensorflow/core/framework/step_stats.pb.h"

#include <algorithm>

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/extension_set.h>
#include <google/protobuf/wire_format_lite.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>

PROTOBUF_PRAGMA_INIT_SEG

namespace _pb = ::PROTOBUF_NAMESPACE_ID;
namespace _pbi = _pb::internal;

namespace tensorflow {
PROTOBUF_CONSTEXPR AllocationRecord::AllocationRecord(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.alloc_micros_)*/int64_t{0}
  , /*decltype(_impl_.alloc_bytes_)*/int64_t{0}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct AllocationRecordDefaultTypeInternal {
  PROTOBUF_CONSTEXPR AllocationRecordDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~AllocationRecordDefaultTypeInternal() {}
  union {
    AllocationRecord _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 AllocationRecordDefaultTypeInternal _AllocationRecord_default_instance_;
PROTOBUF_CONSTEXPR AllocatorMemoryUsed::AllocatorMemoryUsed(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.allocation_records_)*/{}
  , /*decltype(_impl_.allocator_name_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.total_bytes_)*/int64_t{0}
  , /*decltype(_impl_.peak_bytes_)*/int64_t{0}
  , /*decltype(_impl_.live_bytes_)*/int64_t{0}
  , /*decltype(_impl_.allocator_bytes_in_use_)*/int64_t{0}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct AllocatorMemoryUsedDefaultTypeInternal {
  PROTOBUF_CONSTEXPR AllocatorMemoryUsedDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~AllocatorMemoryUsedDefaultTypeInternal() {}
  union {
    AllocatorMemoryUsed _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 AllocatorMemoryUsedDefaultTypeInternal _AllocatorMemoryUsed_default_instance_;
PROTOBUF_CONSTEXPR NodeOutput::NodeOutput(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.tensor_description_)*/nullptr
  , /*decltype(_impl_.slot_)*/0
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct NodeOutputDefaultTypeInternal {
  PROTOBUF_CONSTEXPR NodeOutputDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~NodeOutputDefaultTypeInternal() {}
  union {
    NodeOutput _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 NodeOutputDefaultTypeInternal _NodeOutput_default_instance_;
PROTOBUF_CONSTEXPR MemoryStats::MemoryStats(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.persistent_tensor_alloc_ids_)*/{}
  , /*decltype(_impl_._persistent_tensor_alloc_ids_cached_byte_size_)*/{0}
  , /*decltype(_impl_.device_persistent_tensor_alloc_ids_)*/{}
  , /*decltype(_impl_._device_persistent_tensor_alloc_ids_cached_byte_size_)*/{0}
  , /*decltype(_impl_.temp_memory_size_)*/int64_t{0}
  , /*decltype(_impl_.device_temp_memory_size_)*/int64_t{0}
  , /*decltype(_impl_.persistent_memory_size_)*/int64_t{0}
  , /*decltype(_impl_.device_persistent_memory_size_)*/int64_t{0}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct MemoryStatsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR MemoryStatsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~MemoryStatsDefaultTypeInternal() {}
  union {
    MemoryStats _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 MemoryStatsDefaultTypeInternal _MemoryStats_default_instance_;
PROTOBUF_CONSTEXPR NodeExecStats::NodeExecStats(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.memory_)*/{}
  , /*decltype(_impl_.output_)*/{}
  , /*decltype(_impl_.referenced_tensor_)*/{}
  , /*decltype(_impl_.node_name_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.timeline_label_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.memory_stats_)*/nullptr
  , /*decltype(_impl_.all_start_micros_)*/int64_t{0}
  , /*decltype(_impl_.op_start_rel_micros_)*/int64_t{0}
  , /*decltype(_impl_.op_end_rel_micros_)*/int64_t{0}
  , /*decltype(_impl_.all_end_rel_micros_)*/int64_t{0}
  , /*decltype(_impl_.scheduled_micros_)*/int64_t{0}
  , /*decltype(_impl_.all_start_nanos_)*/int64_t{0}
  , /*decltype(_impl_.op_start_rel_nanos_)*/int64_t{0}
  , /*decltype(_impl_.op_end_rel_nanos_)*/int64_t{0}
  , /*decltype(_impl_.all_end_rel_nanos_)*/int64_t{0}
  , /*decltype(_impl_.scheduled_nanos_)*/int64_t{0}
  , /*decltype(_impl_.thread_id_)*/0u
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct NodeExecStatsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR NodeExecStatsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~NodeExecStatsDefaultTypeInternal() {}
  union {
    NodeExecStats _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 NodeExecStatsDefaultTypeInternal _NodeExecStats_default_instance_;
PROTOBUF_CONSTEXPR DeviceStepStats_ThreadNamesEntry_DoNotUse::DeviceStepStats_ThreadNamesEntry_DoNotUse(
    ::_pbi::ConstantInitialized) {}
struct DeviceStepStats_ThreadNamesEntry_DoNotUseDefaultTypeInternal {
  PROTOBUF_CONSTEXPR DeviceStepStats_ThreadNamesEntry_DoNotUseDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~DeviceStepStats_ThreadNamesEntry_DoNotUseDefaultTypeInternal() {}
  union {
    DeviceStepStats_ThreadNamesEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 DeviceStepStats_ThreadNamesEntry_DoNotUseDefaultTypeInternal _DeviceStepStats_ThreadNamesEntry_DoNotUse_default_instance_;
PROTOBUF_CONSTEXPR DeviceStepStats::DeviceStepStats(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.node_stats_)*/{}
  , /*decltype(_impl_.thread_names_)*/{::_pbi::ConstantInitialized()}
  , /*decltype(_impl_.device_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct DeviceStepStatsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR DeviceStepStatsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~DeviceStepStatsDefaultTypeInternal() {}
  union {
    DeviceStepStats _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 DeviceStepStatsDefaultTypeInternal _DeviceStepStats_default_instance_;
PROTOBUF_CONSTEXPR StepStats::StepStats(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.dev_stats_)*/{}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct StepStatsDefaultTypeInternal {
  PROTOBUF_CONSTEXPR StepStatsDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~StepStatsDefaultTypeInternal() {}
  union {
    StepStats _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 StepStatsDefaultTypeInternal _StepStats_default_instance_;
}  // namespace tensorflow
static ::_pb::Metadata file_level_metadata_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto[8];
static constexpr ::_pb::EnumDescriptor const** file_level_enum_descriptors_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto = nullptr;
static constexpr ::_pb::ServiceDescriptor const** file_level_service_descriptors_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto = nullptr;

const uint32_t TableStruct_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto::offsets[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::AllocationRecord, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::AllocationRecord, _impl_.alloc_micros_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::AllocationRecord, _impl_.alloc_bytes_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::AllocatorMemoryUsed, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::AllocatorMemoryUsed, _impl_.allocator_name_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::AllocatorMemoryUsed, _impl_.total_bytes_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::AllocatorMemoryUsed, _impl_.peak_bytes_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::AllocatorMemoryUsed, _impl_.live_bytes_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::AllocatorMemoryUsed, _impl_.allocation_records_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::AllocatorMemoryUsed, _impl_.allocator_bytes_in_use_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeOutput, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeOutput, _impl_.slot_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeOutput, _impl_.tensor_description_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::MemoryStats, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::MemoryStats, _impl_.temp_memory_size_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::MemoryStats, _impl_.persistent_memory_size_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::MemoryStats, _impl_.persistent_tensor_alloc_ids_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::MemoryStats, _impl_.device_temp_memory_size_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::MemoryStats, _impl_.device_persistent_memory_size_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::MemoryStats, _impl_.device_persistent_tensor_alloc_ids_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.node_name_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.all_start_micros_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.op_start_rel_micros_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.op_end_rel_micros_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.all_end_rel_micros_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.memory_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.output_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.timeline_label_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.scheduled_micros_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.thread_id_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.referenced_tensor_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.memory_stats_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.all_start_nanos_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.op_start_rel_nanos_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.op_end_rel_nanos_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.all_end_rel_nanos_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::NodeExecStats, _impl_.scheduled_nanos_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::DeviceStepStats_ThreadNamesEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::DeviceStepStats_ThreadNamesEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::DeviceStepStats_ThreadNamesEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::DeviceStepStats_ThreadNamesEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::DeviceStepStats, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::DeviceStepStats, _impl_.device_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::DeviceStepStats, _impl_.node_stats_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::DeviceStepStats, _impl_.thread_names_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::StepStats, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::StepStats, _impl_.dev_stats_),
};
static const ::_pbi::MigrationSchema schemas[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  { 0, -1, -1, sizeof(::tensorflow::AllocationRecord)},
  { 8, -1, -1, sizeof(::tensorflow::AllocatorMemoryUsed)},
  { 20, -1, -1, sizeof(::tensorflow::NodeOutput)},
  { 28, -1, -1, sizeof(::tensorflow::MemoryStats)},
  { 40, -1, -1, sizeof(::tensorflow::NodeExecStats)},
  { 63, 71, -1, sizeof(::tensorflow::DeviceStepStats_ThreadNamesEntry_DoNotUse)},
  { 73, -1, -1, sizeof(::tensorflow::DeviceStepStats)},
  { 82, -1, -1, sizeof(::tensorflow::StepStats)},
};

static const ::_pb::Message* const file_default_instances[] = {
  &::tensorflow::_AllocationRecord_default_instance_._instance,
  &::tensorflow::_AllocatorMemoryUsed_default_instance_._instance,
  &::tensorflow::_NodeOutput_default_instance_._instance,
  &::tensorflow::_MemoryStats_default_instance_._instance,
  &::tensorflow::_NodeExecStats_default_instance_._instance,
  &::tensorflow::_DeviceStepStats_ThreadNamesEntry_DoNotUse_default_instance_._instance,
  &::tensorflow::_DeviceStepStats_default_instance_._instance,
  &::tensorflow::_StepStats_default_instance_._instance,
};

const char descriptor_table_protodef_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) =
  "\n*tensorflow/core/framework/step_stats.p"
  "roto\022\ntensorflow\0326tensorflow/core/framew"
  "ork/allocation_description.proto\0322tensor"
  "flow/core/framework/tensor_description.p"
  "roto\"=\n\020AllocationRecord\022\024\n\014alloc_micros"
  "\030\001 \001(\003\022\023\n\013alloc_bytes\030\002 \001(\003\"\304\001\n\023Allocato"
  "rMemoryUsed\022\026\n\016allocator_name\030\001 \001(\t\022\023\n\013t"
  "otal_bytes\030\002 \001(\003\022\022\n\npeak_bytes\030\003 \001(\003\022\022\n\n"
  "live_bytes\030\004 \001(\003\0228\n\022allocation_records\030\006"
  " \003(\0132\034.tensorflow.AllocationRecord\022\036\n\026al"
  "locator_bytes_in_use\030\005 \001(\003\"U\n\nNodeOutput"
  "\022\014\n\004slot\030\001 \001(\005\0229\n\022tensor_description\030\003 \001"
  "(\0132\035.tensorflow.TensorDescription\"\354\001\n\013Me"
  "moryStats\022\030\n\020temp_memory_size\030\001 \001(\003\022\036\n\026p"
  "ersistent_memory_size\030\003 \001(\003\022#\n\033persisten"
  "t_tensor_alloc_ids\030\005 \003(\003\022#\n\027device_temp_"
  "memory_size\030\002 \001(\003B\002\030\001\022)\n\035device_persiste"
  "nt_memory_size\030\004 \001(\003B\002\030\001\022.\n\"device_persi"
  "stent_tensor_alloc_ids\030\006 \003(\003B\002\030\001\"\236\004\n\rNod"
  "eExecStats\022\021\n\tnode_name\030\001 \001(\t\022\030\n\020all_sta"
  "rt_micros\030\002 \001(\003\022\033\n\023op_start_rel_micros\030\003"
  " \001(\003\022\031\n\021op_end_rel_micros\030\004 \001(\003\022\032\n\022all_e"
  "nd_rel_micros\030\005 \001(\003\022/\n\006memory\030\006 \003(\0132\037.te"
  "nsorflow.AllocatorMemoryUsed\022&\n\006output\030\007"
  " \003(\0132\026.tensorflow.NodeOutput\022\026\n\016timeline"
  "_label\030\010 \001(\t\022\030\n\020scheduled_micros\030\t \001(\003\022\021"
  "\n\tthread_id\030\n \001(\r\022<\n\021referenced_tensor\030\013"
  " \003(\0132!.tensorflow.AllocationDescription\022"
  "-\n\014memory_stats\030\014 \001(\0132\027.tensorflow.Memor"
  "yStats\022\027\n\017all_start_nanos\030\r \001(\003\022\032\n\022op_st"
  "art_rel_nanos\030\016 \001(\003\022\030\n\020op_end_rel_nanos\030"
  "\017 \001(\003\022\031\n\021all_end_rel_nanos\030\020 \001(\003\022\027\n\017sche"
  "duled_nanos\030\021 \001(\003\"\310\001\n\017DeviceStepStats\022\016\n"
  "\006device\030\001 \001(\t\022-\n\nnode_stats\030\002 \003(\0132\031.tens"
  "orflow.NodeExecStats\022B\n\014thread_names\030\003 \003"
  "(\0132,.tensorflow.DeviceStepStats.ThreadNa"
  "mesEntry\0322\n\020ThreadNamesEntry\022\013\n\003key\030\001 \001("
  "\r\022\r\n\005value\030\002 \001(\t:\0028\001\";\n\tStepStats\022.\n\tdev"
  "_stats\030\001 \003(\0132\033.tensorflow.DeviceStepStat"
  "sB\203\001\n\030org.tensorflow.frameworkB\017StepStat"
  "sProtosP\001ZQgithub.com/tensorflow/tensorf"
  "low/tensorflow/go/core/framework/step_st"
  "ats_go_proto\370\001\001b\006proto3"
  ;
static const ::_pbi::DescriptorTable* const descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_deps[2] = {
  &::descriptor_table_tensorflow_2fcore_2fframework_2fallocation_5fdescription_2eproto,
  &::descriptor_table_tensorflow_2fcore_2fframework_2ftensor_5fdescription_2eproto,
};
static ::_pbi::once_flag descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_once;
const ::_pbi::DescriptorTable descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto = {
    false, false, 1703, descriptor_table_protodef_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto,
    "tensorflow/core/framework/step_stats.proto",
    &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_once, descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_deps, 2, 8,
    schemas, file_default_instances, TableStruct_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto::offsets,
    file_level_metadata_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto, file_level_enum_descriptors_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto,
    file_level_service_descriptors_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto,
};
PROTOBUF_ATTRIBUTE_WEAK const ::_pbi::DescriptorTable* descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_getter() {
  return &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto;
}

// Force running AddDescriptors() at dynamic initialization time.
PROTOBUF_ATTRIBUTE_INIT_PRIORITY2 static ::_pbi::AddDescriptorsRunner dynamic_init_dummy_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto(&descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto);
namespace tensorflow {

// ===================================================================

class AllocationRecord::_Internal {
 public:
};

AllocationRecord::AllocationRecord(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.AllocationRecord)
}
AllocationRecord::AllocationRecord(const AllocationRecord& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  AllocationRecord* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.alloc_micros_){}
    , decltype(_impl_.alloc_bytes_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  ::memcpy(&_impl_.alloc_micros_, &from._impl_.alloc_micros_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.alloc_bytes_) -
    reinterpret_cast<char*>(&_impl_.alloc_micros_)) + sizeof(_impl_.alloc_bytes_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.AllocationRecord)
}

inline void AllocationRecord::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.alloc_micros_){int64_t{0}}
    , decltype(_impl_.alloc_bytes_){int64_t{0}}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

AllocationRecord::~AllocationRecord() {
  // @@protoc_insertion_point(destructor:tensorflow.AllocationRecord)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void AllocationRecord::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
}

void AllocationRecord::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void AllocationRecord::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.AllocationRecord)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  ::memset(&_impl_.alloc_micros_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.alloc_bytes_) -
      reinterpret_cast<char*>(&_impl_.alloc_micros_)) + sizeof(_impl_.alloc_bytes_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* AllocationRecord::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // int64 alloc_micros = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          _impl_.alloc_micros_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 alloc_bytes = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.alloc_bytes_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* AllocationRecord::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.AllocationRecord)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 alloc_micros = 1;
  if (this->_internal_alloc_micros() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(1, this->_internal_alloc_micros(), target);
  }

  // int64 alloc_bytes = 2;
  if (this->_internal_alloc_bytes() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(2, this->_internal_alloc_bytes(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.AllocationRecord)
  return target;
}

size_t AllocationRecord::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.AllocationRecord)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // int64 alloc_micros = 1;
  if (this->_internal_alloc_micros() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_alloc_micros());
  }

  // int64 alloc_bytes = 2;
  if (this->_internal_alloc_bytes() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_alloc_bytes());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData AllocationRecord::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    AllocationRecord::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*AllocationRecord::GetClassData() const { return &_class_data_; }


void AllocationRecord::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<AllocationRecord*>(&to_msg);
  auto& from = static_cast<const AllocationRecord&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.AllocationRecord)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_alloc_micros() != 0) {
    _this->_internal_set_alloc_micros(from._internal_alloc_micros());
  }
  if (from._internal_alloc_bytes() != 0) {
    _this->_internal_set_alloc_bytes(from._internal_alloc_bytes());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void AllocationRecord::CopyFrom(const AllocationRecord& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.AllocationRecord)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AllocationRecord::IsInitialized() const {
  return true;
}

void AllocationRecord::InternalSwap(AllocationRecord* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(AllocationRecord, _impl_.alloc_bytes_)
      + sizeof(AllocationRecord::_impl_.alloc_bytes_)
      - PROTOBUF_FIELD_OFFSET(AllocationRecord, _impl_.alloc_micros_)>(
          reinterpret_cast<char*>(&_impl_.alloc_micros_),
          reinterpret_cast<char*>(&other->_impl_.alloc_micros_));
}

::PROTOBUF_NAMESPACE_ID::Metadata AllocationRecord::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto[0]);
}

// ===================================================================

class AllocatorMemoryUsed::_Internal {
 public:
};

AllocatorMemoryUsed::AllocatorMemoryUsed(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.AllocatorMemoryUsed)
}
AllocatorMemoryUsed::AllocatorMemoryUsed(const AllocatorMemoryUsed& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  AllocatorMemoryUsed* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.allocation_records_){from._impl_.allocation_records_}
    , decltype(_impl_.allocator_name_){}
    , decltype(_impl_.total_bytes_){}
    , decltype(_impl_.peak_bytes_){}
    , decltype(_impl_.live_bytes_){}
    , decltype(_impl_.allocator_bytes_in_use_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _impl_.allocator_name_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.allocator_name_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_allocator_name().empty()) {
    _this->_impl_.allocator_name_.Set(from._internal_allocator_name(), 
      _this->GetArenaForAllocation());
  }
  ::memcpy(&_impl_.total_bytes_, &from._impl_.total_bytes_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.allocator_bytes_in_use_) -
    reinterpret_cast<char*>(&_impl_.total_bytes_)) + sizeof(_impl_.allocator_bytes_in_use_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.AllocatorMemoryUsed)
}

inline void AllocatorMemoryUsed::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.allocation_records_){arena}
    , decltype(_impl_.allocator_name_){}
    , decltype(_impl_.total_bytes_){int64_t{0}}
    , decltype(_impl_.peak_bytes_){int64_t{0}}
    , decltype(_impl_.live_bytes_){int64_t{0}}
    , decltype(_impl_.allocator_bytes_in_use_){int64_t{0}}
    , /*decltype(_impl_._cached_size_)*/{}
  };
  _impl_.allocator_name_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.allocator_name_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
}

AllocatorMemoryUsed::~AllocatorMemoryUsed() {
  // @@protoc_insertion_point(destructor:tensorflow.AllocatorMemoryUsed)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void AllocatorMemoryUsed::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.allocation_records_.~RepeatedPtrField();
  _impl_.allocator_name_.Destroy();
}

void AllocatorMemoryUsed::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void AllocatorMemoryUsed::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.AllocatorMemoryUsed)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.allocation_records_.Clear();
  _impl_.allocator_name_.ClearToEmpty();
  ::memset(&_impl_.total_bytes_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.allocator_bytes_in_use_) -
      reinterpret_cast<char*>(&_impl_.total_bytes_)) + sizeof(_impl_.allocator_bytes_in_use_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* AllocatorMemoryUsed::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // string allocator_name = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          auto str = _internal_mutable_allocator_name();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.AllocatorMemoryUsed.allocator_name"));
        } else
          goto handle_unusual;
        continue;
      // int64 total_bytes = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.total_bytes_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 peak_bytes = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          _impl_.peak_bytes_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 live_bytes = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 32)) {
          _impl_.live_bytes_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 allocator_bytes_in_use = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 40)) {
          _impl_.allocator_bytes_in_use_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // repeated .tensorflow.AllocationRecord allocation_records = 6;
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 50)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_allocation_records(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<50>(ptr));
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* AllocatorMemoryUsed::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.AllocatorMemoryUsed)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // string allocator_name = 1;
  if (!this->_internal_allocator_name().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_allocator_name().data(), static_cast<int>(this->_internal_allocator_name().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.AllocatorMemoryUsed.allocator_name");
    target = stream->WriteStringMaybeAliased(
        1, this->_internal_allocator_name(), target);
  }

  // int64 total_bytes = 2;
  if (this->_internal_total_bytes() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(2, this->_internal_total_bytes(), target);
  }

  // int64 peak_bytes = 3;
  if (this->_internal_peak_bytes() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(3, this->_internal_peak_bytes(), target);
  }

  // int64 live_bytes = 4;
  if (this->_internal_live_bytes() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(4, this->_internal_live_bytes(), target);
  }

  // int64 allocator_bytes_in_use = 5;
  if (this->_internal_allocator_bytes_in_use() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(5, this->_internal_allocator_bytes_in_use(), target);
  }

  // repeated .tensorflow.AllocationRecord allocation_records = 6;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_allocation_records_size()); i < n; i++) {
    const auto& repfield = this->_internal_allocation_records(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(6, repfield, repfield.GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.AllocatorMemoryUsed)
  return target;
}

size_t AllocatorMemoryUsed::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.AllocatorMemoryUsed)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.AllocationRecord allocation_records = 6;
  total_size += 1UL * this->_internal_allocation_records_size();
  for (const auto& msg : this->_impl_.allocation_records_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // string allocator_name = 1;
  if (!this->_internal_allocator_name().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_allocator_name());
  }

  // int64 total_bytes = 2;
  if (this->_internal_total_bytes() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_total_bytes());
  }

  // int64 peak_bytes = 3;
  if (this->_internal_peak_bytes() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_peak_bytes());
  }

  // int64 live_bytes = 4;
  if (this->_internal_live_bytes() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_live_bytes());
  }

  // int64 allocator_bytes_in_use = 5;
  if (this->_internal_allocator_bytes_in_use() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_allocator_bytes_in_use());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData AllocatorMemoryUsed::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    AllocatorMemoryUsed::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*AllocatorMemoryUsed::GetClassData() const { return &_class_data_; }


void AllocatorMemoryUsed::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<AllocatorMemoryUsed*>(&to_msg);
  auto& from = static_cast<const AllocatorMemoryUsed&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.AllocatorMemoryUsed)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.allocation_records_.MergeFrom(from._impl_.allocation_records_);
  if (!from._internal_allocator_name().empty()) {
    _this->_internal_set_allocator_name(from._internal_allocator_name());
  }
  if (from._internal_total_bytes() != 0) {
    _this->_internal_set_total_bytes(from._internal_total_bytes());
  }
  if (from._internal_peak_bytes() != 0) {
    _this->_internal_set_peak_bytes(from._internal_peak_bytes());
  }
  if (from._internal_live_bytes() != 0) {
    _this->_internal_set_live_bytes(from._internal_live_bytes());
  }
  if (from._internal_allocator_bytes_in_use() != 0) {
    _this->_internal_set_allocator_bytes_in_use(from._internal_allocator_bytes_in_use());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void AllocatorMemoryUsed::CopyFrom(const AllocatorMemoryUsed& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.AllocatorMemoryUsed)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool AllocatorMemoryUsed::IsInitialized() const {
  return true;
}

void AllocatorMemoryUsed::InternalSwap(AllocatorMemoryUsed* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.allocation_records_.InternalSwap(&other->_impl_.allocation_records_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.allocator_name_, lhs_arena,
      &other->_impl_.allocator_name_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(AllocatorMemoryUsed, _impl_.allocator_bytes_in_use_)
      + sizeof(AllocatorMemoryUsed::_impl_.allocator_bytes_in_use_)
      - PROTOBUF_FIELD_OFFSET(AllocatorMemoryUsed, _impl_.total_bytes_)>(
          reinterpret_cast<char*>(&_impl_.total_bytes_),
          reinterpret_cast<char*>(&other->_impl_.total_bytes_));
}

::PROTOBUF_NAMESPACE_ID::Metadata AllocatorMemoryUsed::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto[1]);
}

// ===================================================================

class NodeOutput::_Internal {
 public:
  static const ::tensorflow::TensorDescription& tensor_description(const NodeOutput* msg);
};

const ::tensorflow::TensorDescription&
NodeOutput::_Internal::tensor_description(const NodeOutput* msg) {
  return *msg->_impl_.tensor_description_;
}
void NodeOutput::clear_tensor_description() {
  if (GetArenaForAllocation() == nullptr && _impl_.tensor_description_ != nullptr) {
    delete _impl_.tensor_description_;
  }
  _impl_.tensor_description_ = nullptr;
}
NodeOutput::NodeOutput(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.NodeOutput)
}
NodeOutput::NodeOutput(const NodeOutput& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  NodeOutput* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.tensor_description_){nullptr}
    , decltype(_impl_.slot_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_tensor_description()) {
    _this->_impl_.tensor_description_ = new ::tensorflow::TensorDescription(*from._impl_.tensor_description_);
  }
  _this->_impl_.slot_ = from._impl_.slot_;
  // @@protoc_insertion_point(copy_constructor:tensorflow.NodeOutput)
}

inline void NodeOutput::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.tensor_description_){nullptr}
    , decltype(_impl_.slot_){0}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

NodeOutput::~NodeOutput() {
  // @@protoc_insertion_point(destructor:tensorflow.NodeOutput)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void NodeOutput::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  if (this != internal_default_instance()) delete _impl_.tensor_description_;
}

void NodeOutput::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void NodeOutput::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.NodeOutput)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaForAllocation() == nullptr && _impl_.tensor_description_ != nullptr) {
    delete _impl_.tensor_description_;
  }
  _impl_.tensor_description_ = nullptr;
  _impl_.slot_ = 0;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* NodeOutput::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // int32 slot = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          _impl_.slot_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.TensorDescription tensor_description = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          ptr = ctx->ParseMessage(_internal_mutable_tensor_description(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* NodeOutput::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.NodeOutput)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // int32 slot = 1;
  if (this->_internal_slot() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt32ToArray(1, this->_internal_slot(), target);
  }

  // .tensorflow.TensorDescription tensor_description = 3;
  if (this->_internal_has_tensor_description()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(3, _Internal::tensor_description(this),
        _Internal::tensor_description(this).GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.NodeOutput)
  return target;
}

size_t NodeOutput::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.NodeOutput)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .tensorflow.TensorDescription tensor_description = 3;
  if (this->_internal_has_tensor_description()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.tensor_description_);
  }

  // int32 slot = 1;
  if (this->_internal_slot() != 0) {
    total_size += ::_pbi::WireFormatLite::Int32SizePlusOne(this->_internal_slot());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData NodeOutput::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    NodeOutput::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*NodeOutput::GetClassData() const { return &_class_data_; }


void NodeOutput::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<NodeOutput*>(&to_msg);
  auto& from = static_cast<const NodeOutput&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.NodeOutput)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_has_tensor_description()) {
    _this->_internal_mutable_tensor_description()->::tensorflow::TensorDescription::MergeFrom(
        from._internal_tensor_description());
  }
  if (from._internal_slot() != 0) {
    _this->_internal_set_slot(from._internal_slot());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void NodeOutput::CopyFrom(const NodeOutput& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.NodeOutput)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NodeOutput::IsInitialized() const {
  return true;
}

void NodeOutput::InternalSwap(NodeOutput* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(NodeOutput, _impl_.slot_)
      + sizeof(NodeOutput::_impl_.slot_)
      - PROTOBUF_FIELD_OFFSET(NodeOutput, _impl_.tensor_description_)>(
          reinterpret_cast<char*>(&_impl_.tensor_description_),
          reinterpret_cast<char*>(&other->_impl_.tensor_description_));
}

::PROTOBUF_NAMESPACE_ID::Metadata NodeOutput::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto[2]);
}

// ===================================================================

class MemoryStats::_Internal {
 public:
};

MemoryStats::MemoryStats(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.MemoryStats)
}
MemoryStats::MemoryStats(const MemoryStats& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  MemoryStats* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.persistent_tensor_alloc_ids_){from._impl_.persistent_tensor_alloc_ids_}
    , /*decltype(_impl_._persistent_tensor_alloc_ids_cached_byte_size_)*/{0}
    , decltype(_impl_.device_persistent_tensor_alloc_ids_){from._impl_.device_persistent_tensor_alloc_ids_}
    , /*decltype(_impl_._device_persistent_tensor_alloc_ids_cached_byte_size_)*/{0}
    , decltype(_impl_.temp_memory_size_){}
    , decltype(_impl_.device_temp_memory_size_){}
    , decltype(_impl_.persistent_memory_size_){}
    , decltype(_impl_.device_persistent_memory_size_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  ::memcpy(&_impl_.temp_memory_size_, &from._impl_.temp_memory_size_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.device_persistent_memory_size_) -
    reinterpret_cast<char*>(&_impl_.temp_memory_size_)) + sizeof(_impl_.device_persistent_memory_size_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.MemoryStats)
}

inline void MemoryStats::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.persistent_tensor_alloc_ids_){arena}
    , /*decltype(_impl_._persistent_tensor_alloc_ids_cached_byte_size_)*/{0}
    , decltype(_impl_.device_persistent_tensor_alloc_ids_){arena}
    , /*decltype(_impl_._device_persistent_tensor_alloc_ids_cached_byte_size_)*/{0}
    , decltype(_impl_.temp_memory_size_){int64_t{0}}
    , decltype(_impl_.device_temp_memory_size_){int64_t{0}}
    , decltype(_impl_.persistent_memory_size_){int64_t{0}}
    , decltype(_impl_.device_persistent_memory_size_){int64_t{0}}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

MemoryStats::~MemoryStats() {
  // @@protoc_insertion_point(destructor:tensorflow.MemoryStats)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void MemoryStats::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.persistent_tensor_alloc_ids_.~RepeatedField();
  _impl_.device_persistent_tensor_alloc_ids_.~RepeatedField();
}

void MemoryStats::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void MemoryStats::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.MemoryStats)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.persistent_tensor_alloc_ids_.Clear();
  _impl_.device_persistent_tensor_alloc_ids_.Clear();
  ::memset(&_impl_.temp_memory_size_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.device_persistent_memory_size_) -
      reinterpret_cast<char*>(&_impl_.temp_memory_size_)) + sizeof(_impl_.device_persistent_memory_size_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* MemoryStats::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // int64 temp_memory_size = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          _impl_.temp_memory_size_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 device_temp_memory_size = 2 [deprecated = true];
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.device_temp_memory_size_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 persistent_memory_size = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          _impl_.persistent_memory_size_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 device_persistent_memory_size = 4 [deprecated = true];
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 32)) {
          _impl_.device_persistent_memory_size_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // repeated int64 persistent_tensor_alloc_ids = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 42)) {
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::PackedInt64Parser(_internal_mutable_persistent_tensor_alloc_ids(), ptr, ctx);
          CHK_(ptr);
        } else if (static_cast<uint8_t>(tag) == 40) {
          _internal_add_persistent_tensor_alloc_ids(::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr));
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // repeated int64 device_persistent_tensor_alloc_ids = 6 [deprecated = true];
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 50)) {
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::PackedInt64Parser(_internal_mutable_device_persistent_tensor_alloc_ids(), ptr, ctx);
          CHK_(ptr);
        } else if (static_cast<uint8_t>(tag) == 48) {
          _internal_add_device_persistent_tensor_alloc_ids(::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr));
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* MemoryStats::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.MemoryStats)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // int64 temp_memory_size = 1;
  if (this->_internal_temp_memory_size() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(1, this->_internal_temp_memory_size(), target);
  }

  // int64 device_temp_memory_size = 2 [deprecated = true];
  if (this->_internal_device_temp_memory_size() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(2, this->_internal_device_temp_memory_size(), target);
  }

  // int64 persistent_memory_size = 3;
  if (this->_internal_persistent_memory_size() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(3, this->_internal_persistent_memory_size(), target);
  }

  // int64 device_persistent_memory_size = 4 [deprecated = true];
  if (this->_internal_device_persistent_memory_size() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(4, this->_internal_device_persistent_memory_size(), target);
  }

  // repeated int64 persistent_tensor_alloc_ids = 5;
  {
    int byte_size = _impl_._persistent_tensor_alloc_ids_cached_byte_size_.load(std::memory_order_relaxed);
    if (byte_size > 0) {
      target = stream->WriteInt64Packed(
          5, _internal_persistent_tensor_alloc_ids(), byte_size, target);
    }
  }

  // repeated int64 device_persistent_tensor_alloc_ids = 6 [deprecated = true];
  {
    int byte_size = _impl_._device_persistent_tensor_alloc_ids_cached_byte_size_.load(std::memory_order_relaxed);
    if (byte_size > 0) {
      target = stream->WriteInt64Packed(
          6, _internal_device_persistent_tensor_alloc_ids(), byte_size, target);
    }
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.MemoryStats)
  return target;
}

size_t MemoryStats::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.MemoryStats)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated int64 persistent_tensor_alloc_ids = 5;
  {
    size_t data_size = ::_pbi::WireFormatLite::
      Int64Size(this->_impl_.persistent_tensor_alloc_ids_);
    if (data_size > 0) {
      total_size += 1 +
        ::_pbi::WireFormatLite::Int32Size(static_cast<int32_t>(data_size));
    }
    int cached_size = ::_pbi::ToCachedSize(data_size);
    _impl_._persistent_tensor_alloc_ids_cached_byte_size_.store(cached_size,
                                    std::memory_order_relaxed);
    total_size += data_size;
  }

  // repeated int64 device_persistent_tensor_alloc_ids = 6 [deprecated = true];
  {
    size_t data_size = ::_pbi::WireFormatLite::
      Int64Size(this->_impl_.device_persistent_tensor_alloc_ids_);
    if (data_size > 0) {
      total_size += 1 +
        ::_pbi::WireFormatLite::Int32Size(static_cast<int32_t>(data_size));
    }
    int cached_size = ::_pbi::ToCachedSize(data_size);
    _impl_._device_persistent_tensor_alloc_ids_cached_byte_size_.store(cached_size,
                                    std::memory_order_relaxed);
    total_size += data_size;
  }

  // int64 temp_memory_size = 1;
  if (this->_internal_temp_memory_size() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_temp_memory_size());
  }

  // int64 device_temp_memory_size = 2 [deprecated = true];
  if (this->_internal_device_temp_memory_size() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_device_temp_memory_size());
  }

  // int64 persistent_memory_size = 3;
  if (this->_internal_persistent_memory_size() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_persistent_memory_size());
  }

  // int64 device_persistent_memory_size = 4 [deprecated = true];
  if (this->_internal_device_persistent_memory_size() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_device_persistent_memory_size());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData MemoryStats::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    MemoryStats::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*MemoryStats::GetClassData() const { return &_class_data_; }


void MemoryStats::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<MemoryStats*>(&to_msg);
  auto& from = static_cast<const MemoryStats&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.MemoryStats)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.persistent_tensor_alloc_ids_.MergeFrom(from._impl_.persistent_tensor_alloc_ids_);
  _this->_impl_.device_persistent_tensor_alloc_ids_.MergeFrom(from._impl_.device_persistent_tensor_alloc_ids_);
  if (from._internal_temp_memory_size() != 0) {
    _this->_internal_set_temp_memory_size(from._internal_temp_memory_size());
  }
  if (from._internal_device_temp_memory_size() != 0) {
    _this->_internal_set_device_temp_memory_size(from._internal_device_temp_memory_size());
  }
  if (from._internal_persistent_memory_size() != 0) {
    _this->_internal_set_persistent_memory_size(from._internal_persistent_memory_size());
  }
  if (from._internal_device_persistent_memory_size() != 0) {
    _this->_internal_set_device_persistent_memory_size(from._internal_device_persistent_memory_size());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void MemoryStats::CopyFrom(const MemoryStats& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.MemoryStats)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MemoryStats::IsInitialized() const {
  return true;
}

void MemoryStats::InternalSwap(MemoryStats* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.persistent_tensor_alloc_ids_.InternalSwap(&other->_impl_.persistent_tensor_alloc_ids_);
  _impl_.device_persistent_tensor_alloc_ids_.InternalSwap(&other->_impl_.device_persistent_tensor_alloc_ids_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(MemoryStats, _impl_.device_persistent_memory_size_)
      + sizeof(MemoryStats::_impl_.device_persistent_memory_size_)
      - PROTOBUF_FIELD_OFFSET(MemoryStats, _impl_.temp_memory_size_)>(
          reinterpret_cast<char*>(&_impl_.temp_memory_size_),
          reinterpret_cast<char*>(&other->_impl_.temp_memory_size_));
}

::PROTOBUF_NAMESPACE_ID::Metadata MemoryStats::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto[3]);
}

// ===================================================================

class NodeExecStats::_Internal {
 public:
  static const ::tensorflow::MemoryStats& memory_stats(const NodeExecStats* msg);
};

const ::tensorflow::MemoryStats&
NodeExecStats::_Internal::memory_stats(const NodeExecStats* msg) {
  return *msg->_impl_.memory_stats_;
}
void NodeExecStats::clear_referenced_tensor() {
  _impl_.referenced_tensor_.Clear();
}
NodeExecStats::NodeExecStats(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.NodeExecStats)
}
NodeExecStats::NodeExecStats(const NodeExecStats& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  NodeExecStats* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.memory_){from._impl_.memory_}
    , decltype(_impl_.output_){from._impl_.output_}
    , decltype(_impl_.referenced_tensor_){from._impl_.referenced_tensor_}
    , decltype(_impl_.node_name_){}
    , decltype(_impl_.timeline_label_){}
    , decltype(_impl_.memory_stats_){nullptr}
    , decltype(_impl_.all_start_micros_){}
    , decltype(_impl_.op_start_rel_micros_){}
    , decltype(_impl_.op_end_rel_micros_){}
    , decltype(_impl_.all_end_rel_micros_){}
    , decltype(_impl_.scheduled_micros_){}
    , decltype(_impl_.all_start_nanos_){}
    , decltype(_impl_.op_start_rel_nanos_){}
    , decltype(_impl_.op_end_rel_nanos_){}
    , decltype(_impl_.all_end_rel_nanos_){}
    , decltype(_impl_.scheduled_nanos_){}
    , decltype(_impl_.thread_id_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _impl_.node_name_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.node_name_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_node_name().empty()) {
    _this->_impl_.node_name_.Set(from._internal_node_name(), 
      _this->GetArenaForAllocation());
  }
  _impl_.timeline_label_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.timeline_label_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_timeline_label().empty()) {
    _this->_impl_.timeline_label_.Set(from._internal_timeline_label(), 
      _this->GetArenaForAllocation());
  }
  if (from._internal_has_memory_stats()) {
    _this->_impl_.memory_stats_ = new ::tensorflow::MemoryStats(*from._impl_.memory_stats_);
  }
  ::memcpy(&_impl_.all_start_micros_, &from._impl_.all_start_micros_,
    static_cast<size_t>(reinterpret_cast<char*>(&_impl_.thread_id_) -
    reinterpret_cast<char*>(&_impl_.all_start_micros_)) + sizeof(_impl_.thread_id_));
  // @@protoc_insertion_point(copy_constructor:tensorflow.NodeExecStats)
}

inline void NodeExecStats::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.memory_){arena}
    , decltype(_impl_.output_){arena}
    , decltype(_impl_.referenced_tensor_){arena}
    , decltype(_impl_.node_name_){}
    , decltype(_impl_.timeline_label_){}
    , decltype(_impl_.memory_stats_){nullptr}
    , decltype(_impl_.all_start_micros_){int64_t{0}}
    , decltype(_impl_.op_start_rel_micros_){int64_t{0}}
    , decltype(_impl_.op_end_rel_micros_){int64_t{0}}
    , decltype(_impl_.all_end_rel_micros_){int64_t{0}}
    , decltype(_impl_.scheduled_micros_){int64_t{0}}
    , decltype(_impl_.all_start_nanos_){int64_t{0}}
    , decltype(_impl_.op_start_rel_nanos_){int64_t{0}}
    , decltype(_impl_.op_end_rel_nanos_){int64_t{0}}
    , decltype(_impl_.all_end_rel_nanos_){int64_t{0}}
    , decltype(_impl_.scheduled_nanos_){int64_t{0}}
    , decltype(_impl_.thread_id_){0u}
    , /*decltype(_impl_._cached_size_)*/{}
  };
  _impl_.node_name_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.node_name_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  _impl_.timeline_label_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.timeline_label_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
}

NodeExecStats::~NodeExecStats() {
  // @@protoc_insertion_point(destructor:tensorflow.NodeExecStats)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void NodeExecStats::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.memory_.~RepeatedPtrField();
  _impl_.output_.~RepeatedPtrField();
  _impl_.referenced_tensor_.~RepeatedPtrField();
  _impl_.node_name_.Destroy();
  _impl_.timeline_label_.Destroy();
  if (this != internal_default_instance()) delete _impl_.memory_stats_;
}

void NodeExecStats::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void NodeExecStats::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.NodeExecStats)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.memory_.Clear();
  _impl_.output_.Clear();
  _impl_.referenced_tensor_.Clear();
  _impl_.node_name_.ClearToEmpty();
  _impl_.timeline_label_.ClearToEmpty();
  if (GetArenaForAllocation() == nullptr && _impl_.memory_stats_ != nullptr) {
    delete _impl_.memory_stats_;
  }
  _impl_.memory_stats_ = nullptr;
  ::memset(&_impl_.all_start_micros_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&_impl_.thread_id_) -
      reinterpret_cast<char*>(&_impl_.all_start_micros_)) + sizeof(_impl_.thread_id_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* NodeExecStats::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // string node_name = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          auto str = _internal_mutable_node_name();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.NodeExecStats.node_name"));
        } else
          goto handle_unusual;
        continue;
      // int64 all_start_micros = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          _impl_.all_start_micros_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 op_start_rel_micros = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          _impl_.op_start_rel_micros_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 op_end_rel_micros = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 32)) {
          _impl_.op_end_rel_micros_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 all_end_rel_micros = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 40)) {
          _impl_.all_end_rel_micros_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // repeated .tensorflow.AllocatorMemoryUsed memory = 6;
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 50)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_memory(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<50>(ptr));
        } else
          goto handle_unusual;
        continue;
      // repeated .tensorflow.NodeOutput output = 7;
      case 7:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 58)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_output(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<58>(ptr));
        } else
          goto handle_unusual;
        continue;
      // string timeline_label = 8;
      case 8:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 66)) {
          auto str = _internal_mutable_timeline_label();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.NodeExecStats.timeline_label"));
        } else
          goto handle_unusual;
        continue;
      // int64 scheduled_micros = 9;
      case 9:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 72)) {
          _impl_.scheduled_micros_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // uint32 thread_id = 10;
      case 10:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 80)) {
          _impl_.thread_id_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint32(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // repeated .tensorflow.AllocationDescription referenced_tensor = 11;
      case 11:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 90)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_referenced_tensor(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<90>(ptr));
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.MemoryStats memory_stats = 12;
      case 12:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 98)) {
          ptr = ctx->ParseMessage(_internal_mutable_memory_stats(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 all_start_nanos = 13;
      case 13:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 104)) {
          _impl_.all_start_nanos_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 op_start_rel_nanos = 14;
      case 14:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 112)) {
          _impl_.op_start_rel_nanos_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 op_end_rel_nanos = 15;
      case 15:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 120)) {
          _impl_.op_end_rel_nanos_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 all_end_rel_nanos = 16;
      case 16:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 128)) {
          _impl_.all_end_rel_nanos_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // int64 scheduled_nanos = 17;
      case 17:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 136)) {
          _impl_.scheduled_nanos_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* NodeExecStats::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.NodeExecStats)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // string node_name = 1;
  if (!this->_internal_node_name().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_node_name().data(), static_cast<int>(this->_internal_node_name().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.NodeExecStats.node_name");
    target = stream->WriteStringMaybeAliased(
        1, this->_internal_node_name(), target);
  }

  // int64 all_start_micros = 2;
  if (this->_internal_all_start_micros() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(2, this->_internal_all_start_micros(), target);
  }

  // int64 op_start_rel_micros = 3;
  if (this->_internal_op_start_rel_micros() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(3, this->_internal_op_start_rel_micros(), target);
  }

  // int64 op_end_rel_micros = 4;
  if (this->_internal_op_end_rel_micros() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(4, this->_internal_op_end_rel_micros(), target);
  }

  // int64 all_end_rel_micros = 5;
  if (this->_internal_all_end_rel_micros() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(5, this->_internal_all_end_rel_micros(), target);
  }

  // repeated .tensorflow.AllocatorMemoryUsed memory = 6;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_memory_size()); i < n; i++) {
    const auto& repfield = this->_internal_memory(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(6, repfield, repfield.GetCachedSize(), target, stream);
  }

  // repeated .tensorflow.NodeOutput output = 7;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_output_size()); i < n; i++) {
    const auto& repfield = this->_internal_output(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(7, repfield, repfield.GetCachedSize(), target, stream);
  }

  // string timeline_label = 8;
  if (!this->_internal_timeline_label().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_timeline_label().data(), static_cast<int>(this->_internal_timeline_label().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.NodeExecStats.timeline_label");
    target = stream->WriteStringMaybeAliased(
        8, this->_internal_timeline_label(), target);
  }

  // int64 scheduled_micros = 9;
  if (this->_internal_scheduled_micros() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(9, this->_internal_scheduled_micros(), target);
  }

  // uint32 thread_id = 10;
  if (this->_internal_thread_id() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteUInt32ToArray(10, this->_internal_thread_id(), target);
  }

  // repeated .tensorflow.AllocationDescription referenced_tensor = 11;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_referenced_tensor_size()); i < n; i++) {
    const auto& repfield = this->_internal_referenced_tensor(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(11, repfield, repfield.GetCachedSize(), target, stream);
  }

  // .tensorflow.MemoryStats memory_stats = 12;
  if (this->_internal_has_memory_stats()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(12, _Internal::memory_stats(this),
        _Internal::memory_stats(this).GetCachedSize(), target, stream);
  }

  // int64 all_start_nanos = 13;
  if (this->_internal_all_start_nanos() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(13, this->_internal_all_start_nanos(), target);
  }

  // int64 op_start_rel_nanos = 14;
  if (this->_internal_op_start_rel_nanos() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(14, this->_internal_op_start_rel_nanos(), target);
  }

  // int64 op_end_rel_nanos = 15;
  if (this->_internal_op_end_rel_nanos() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(15, this->_internal_op_end_rel_nanos(), target);
  }

  // int64 all_end_rel_nanos = 16;
  if (this->_internal_all_end_rel_nanos() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(16, this->_internal_all_end_rel_nanos(), target);
  }

  // int64 scheduled_nanos = 17;
  if (this->_internal_scheduled_nanos() != 0) {
    target = stream->EnsureSpace(target);
    target = ::_pbi::WireFormatLite::WriteInt64ToArray(17, this->_internal_scheduled_nanos(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.NodeExecStats)
  return target;
}

size_t NodeExecStats::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.NodeExecStats)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.AllocatorMemoryUsed memory = 6;
  total_size += 1UL * this->_internal_memory_size();
  for (const auto& msg : this->_impl_.memory_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // repeated .tensorflow.NodeOutput output = 7;
  total_size += 1UL * this->_internal_output_size();
  for (const auto& msg : this->_impl_.output_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // repeated .tensorflow.AllocationDescription referenced_tensor = 11;
  total_size += 1UL * this->_internal_referenced_tensor_size();
  for (const auto& msg : this->_impl_.referenced_tensor_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // string node_name = 1;
  if (!this->_internal_node_name().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_node_name());
  }

  // string timeline_label = 8;
  if (!this->_internal_timeline_label().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_timeline_label());
  }

  // .tensorflow.MemoryStats memory_stats = 12;
  if (this->_internal_has_memory_stats()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.memory_stats_);
  }

  // int64 all_start_micros = 2;
  if (this->_internal_all_start_micros() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_all_start_micros());
  }

  // int64 op_start_rel_micros = 3;
  if (this->_internal_op_start_rel_micros() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_op_start_rel_micros());
  }

  // int64 op_end_rel_micros = 4;
  if (this->_internal_op_end_rel_micros() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_op_end_rel_micros());
  }

  // int64 all_end_rel_micros = 5;
  if (this->_internal_all_end_rel_micros() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_all_end_rel_micros());
  }

  // int64 scheduled_micros = 9;
  if (this->_internal_scheduled_micros() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_scheduled_micros());
  }

  // int64 all_start_nanos = 13;
  if (this->_internal_all_start_nanos() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_all_start_nanos());
  }

  // int64 op_start_rel_nanos = 14;
  if (this->_internal_op_start_rel_nanos() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_op_start_rel_nanos());
  }

  // int64 op_end_rel_nanos = 15;
  if (this->_internal_op_end_rel_nanos() != 0) {
    total_size += ::_pbi::WireFormatLite::Int64SizePlusOne(this->_internal_op_end_rel_nanos());
  }

  // int64 all_end_rel_nanos = 16;
  if (this->_internal_all_end_rel_nanos() != 0) {
    total_size += 2 +
      ::_pbi::WireFormatLite::Int64Size(
        this->_internal_all_end_rel_nanos());
  }

  // int64 scheduled_nanos = 17;
  if (this->_internal_scheduled_nanos() != 0) {
    total_size += 2 +
      ::_pbi::WireFormatLite::Int64Size(
        this->_internal_scheduled_nanos());
  }

  // uint32 thread_id = 10;
  if (this->_internal_thread_id() != 0) {
    total_size += ::_pbi::WireFormatLite::UInt32SizePlusOne(this->_internal_thread_id());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData NodeExecStats::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    NodeExecStats::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*NodeExecStats::GetClassData() const { return &_class_data_; }


void NodeExecStats::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<NodeExecStats*>(&to_msg);
  auto& from = static_cast<const NodeExecStats&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.NodeExecStats)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.memory_.MergeFrom(from._impl_.memory_);
  _this->_impl_.output_.MergeFrom(from._impl_.output_);
  _this->_impl_.referenced_tensor_.MergeFrom(from._impl_.referenced_tensor_);
  if (!from._internal_node_name().empty()) {
    _this->_internal_set_node_name(from._internal_node_name());
  }
  if (!from._internal_timeline_label().empty()) {
    _this->_internal_set_timeline_label(from._internal_timeline_label());
  }
  if (from._internal_has_memory_stats()) {
    _this->_internal_mutable_memory_stats()->::tensorflow::MemoryStats::MergeFrom(
        from._internal_memory_stats());
  }
  if (from._internal_all_start_micros() != 0) {
    _this->_internal_set_all_start_micros(from._internal_all_start_micros());
  }
  if (from._internal_op_start_rel_micros() != 0) {
    _this->_internal_set_op_start_rel_micros(from._internal_op_start_rel_micros());
  }
  if (from._internal_op_end_rel_micros() != 0) {
    _this->_internal_set_op_end_rel_micros(from._internal_op_end_rel_micros());
  }
  if (from._internal_all_end_rel_micros() != 0) {
    _this->_internal_set_all_end_rel_micros(from._internal_all_end_rel_micros());
  }
  if (from._internal_scheduled_micros() != 0) {
    _this->_internal_set_scheduled_micros(from._internal_scheduled_micros());
  }
  if (from._internal_all_start_nanos() != 0) {
    _this->_internal_set_all_start_nanos(from._internal_all_start_nanos());
  }
  if (from._internal_op_start_rel_nanos() != 0) {
    _this->_internal_set_op_start_rel_nanos(from._internal_op_start_rel_nanos());
  }
  if (from._internal_op_end_rel_nanos() != 0) {
    _this->_internal_set_op_end_rel_nanos(from._internal_op_end_rel_nanos());
  }
  if (from._internal_all_end_rel_nanos() != 0) {
    _this->_internal_set_all_end_rel_nanos(from._internal_all_end_rel_nanos());
  }
  if (from._internal_scheduled_nanos() != 0) {
    _this->_internal_set_scheduled_nanos(from._internal_scheduled_nanos());
  }
  if (from._internal_thread_id() != 0) {
    _this->_internal_set_thread_id(from._internal_thread_id());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void NodeExecStats::CopyFrom(const NodeExecStats& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.NodeExecStats)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool NodeExecStats::IsInitialized() const {
  return true;
}

void NodeExecStats::InternalSwap(NodeExecStats* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.memory_.InternalSwap(&other->_impl_.memory_);
  _impl_.output_.InternalSwap(&other->_impl_.output_);
  _impl_.referenced_tensor_.InternalSwap(&other->_impl_.referenced_tensor_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.node_name_, lhs_arena,
      &other->_impl_.node_name_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.timeline_label_, lhs_arena,
      &other->_impl_.timeline_label_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(NodeExecStats, _impl_.thread_id_)
      + sizeof(NodeExecStats::_impl_.thread_id_)
      - PROTOBUF_FIELD_OFFSET(NodeExecStats, _impl_.memory_stats_)>(
          reinterpret_cast<char*>(&_impl_.memory_stats_),
          reinterpret_cast<char*>(&other->_impl_.memory_stats_));
}

::PROTOBUF_NAMESPACE_ID::Metadata NodeExecStats::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto[4]);
}

// ===================================================================

DeviceStepStats_ThreadNamesEntry_DoNotUse::DeviceStepStats_ThreadNamesEntry_DoNotUse() {}
DeviceStepStats_ThreadNamesEntry_DoNotUse::DeviceStepStats_ThreadNamesEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena)
    : SuperType(arena) {}
void DeviceStepStats_ThreadNamesEntry_DoNotUse::MergeFrom(const DeviceStepStats_ThreadNamesEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::PROTOBUF_NAMESPACE_ID::Metadata DeviceStepStats_ThreadNamesEntry_DoNotUse::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto[5]);
}

// ===================================================================

class DeviceStepStats::_Internal {
 public:
};

DeviceStepStats::DeviceStepStats(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  if (arena != nullptr && !is_message_owned) {
    arena->OwnCustomDestructor(this, &DeviceStepStats::ArenaDtor);
  }
  // @@protoc_insertion_point(arena_constructor:tensorflow.DeviceStepStats)
}
DeviceStepStats::DeviceStepStats(const DeviceStepStats& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  DeviceStepStats* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.node_stats_){from._impl_.node_stats_}
    , /*decltype(_impl_.thread_names_)*/{}
    , decltype(_impl_.device_){}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _this->_impl_.thread_names_.MergeFrom(from._impl_.thread_names_);
  _impl_.device_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.device_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_device().empty()) {
    _this->_impl_.device_.Set(from._internal_device(), 
      _this->GetArenaForAllocation());
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.DeviceStepStats)
}

inline void DeviceStepStats::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.node_stats_){arena}
    , /*decltype(_impl_.thread_names_)*/{::_pbi::ArenaInitialized(), arena}
    , decltype(_impl_.device_){}
    , /*decltype(_impl_._cached_size_)*/{}
  };
  _impl_.device_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.device_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
}

DeviceStepStats::~DeviceStepStats() {
  // @@protoc_insertion_point(destructor:tensorflow.DeviceStepStats)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    ArenaDtor(this);
    return;
  }
  SharedDtor();
}

inline void DeviceStepStats::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.node_stats_.~RepeatedPtrField();
  _impl_.thread_names_.Destruct();
  _impl_.thread_names_.~MapField();
  _impl_.device_.Destroy();
}

void DeviceStepStats::ArenaDtor(void* object) {
  DeviceStepStats* _this = reinterpret_cast< DeviceStepStats* >(object);
  _this->_impl_.thread_names_.Destruct();
}
void DeviceStepStats::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void DeviceStepStats::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.DeviceStepStats)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.node_stats_.Clear();
  _impl_.thread_names_.Clear();
  _impl_.device_.ClearToEmpty();
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* DeviceStepStats::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // string device = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          auto str = _internal_mutable_device();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.DeviceStepStats.device"));
        } else
          goto handle_unusual;
        continue;
      // repeated .tensorflow.NodeExecStats node_stats = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_node_stats(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<18>(ptr));
        } else
          goto handle_unusual;
        continue;
      // map<uint32, string> thread_names = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(&_impl_.thread_names_, ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<26>(ptr));
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* DeviceStepStats::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.DeviceStepStats)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // string device = 1;
  if (!this->_internal_device().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_device().data(), static_cast<int>(this->_internal_device().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.DeviceStepStats.device");
    target = stream->WriteStringMaybeAliased(
        1, this->_internal_device(), target);
  }

  // repeated .tensorflow.NodeExecStats node_stats = 2;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_node_stats_size()); i < n; i++) {
    const auto& repfield = this->_internal_node_stats(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(2, repfield, repfield.GetCachedSize(), target, stream);
  }

  // map<uint32, string> thread_names = 3;
  if (!this->_internal_thread_names().empty()) {
    using MapType = ::_pb::Map<uint32_t, std::string>;
    using WireHelper = DeviceStepStats_ThreadNamesEntry_DoNotUse::Funcs;
    const auto& map_field = this->_internal_thread_names();
    auto check_utf8 = [](const MapType::value_type& entry) {
      (void)entry;
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
        entry.second.data(), static_cast<int>(entry.second.length()),
        ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
        "tensorflow.DeviceStepStats.ThreadNamesEntry.value");
    };

    if (stream->IsSerializationDeterministic() && map_field.size() > 1) {
      for (const auto& entry : ::_pbi::MapSorterFlat<MapType>(map_field)) {
        target = WireHelper::InternalSerialize(3, entry.first, entry.second, target, stream);
        check_utf8(entry);
      }
    } else {
      for (const auto& entry : map_field) {
        target = WireHelper::InternalSerialize(3, entry.first, entry.second, target, stream);
        check_utf8(entry);
      }
    }
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.DeviceStepStats)
  return target;
}

size_t DeviceStepStats::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.DeviceStepStats)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.NodeExecStats node_stats = 2;
  total_size += 1UL * this->_internal_node_stats_size();
  for (const auto& msg : this->_impl_.node_stats_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // map<uint32, string> thread_names = 3;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(this->_internal_thread_names_size());
  for (::PROTOBUF_NAMESPACE_ID::Map< uint32_t, std::string >::const_iterator
      it = this->_internal_thread_names().begin();
      it != this->_internal_thread_names().end(); ++it) {
    total_size += DeviceStepStats_ThreadNamesEntry_DoNotUse::Funcs::ByteSizeLong(it->first, it->second);
  }

  // string device = 1;
  if (!this->_internal_device().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_device());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData DeviceStepStats::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    DeviceStepStats::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*DeviceStepStats::GetClassData() const { return &_class_data_; }


void DeviceStepStats::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<DeviceStepStats*>(&to_msg);
  auto& from = static_cast<const DeviceStepStats&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.DeviceStepStats)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.node_stats_.MergeFrom(from._impl_.node_stats_);
  _this->_impl_.thread_names_.MergeFrom(from._impl_.thread_names_);
  if (!from._internal_device().empty()) {
    _this->_internal_set_device(from._internal_device());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void DeviceStepStats::CopyFrom(const DeviceStepStats& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.DeviceStepStats)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool DeviceStepStats::IsInitialized() const {
  return true;
}

void DeviceStepStats::InternalSwap(DeviceStepStats* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.node_stats_.InternalSwap(&other->_impl_.node_stats_);
  _impl_.thread_names_.InternalSwap(&other->_impl_.thread_names_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.device_, lhs_arena,
      &other->_impl_.device_, rhs_arena
  );
}

::PROTOBUF_NAMESPACE_ID::Metadata DeviceStepStats::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto[6]);
}

// ===================================================================

class StepStats::_Internal {
 public:
};

StepStats::StepStats(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.StepStats)
}
StepStats::StepStats(const StepStats& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  StepStats* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.dev_stats_){from._impl_.dev_stats_}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:tensorflow.StepStats)
}

inline void StepStats::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.dev_stats_){arena}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

StepStats::~StepStats() {
  // @@protoc_insertion_point(destructor:tensorflow.StepStats)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void StepStats::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.dev_stats_.~RepeatedPtrField();
}

void StepStats::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void StepStats::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.StepStats)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.dev_stats_.Clear();
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* StepStats::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // repeated .tensorflow.DeviceStepStats dev_stats = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_dev_stats(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<10>(ptr));
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* StepStats::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.StepStats)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .tensorflow.DeviceStepStats dev_stats = 1;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_dev_stats_size()); i < n; i++) {
    const auto& repfield = this->_internal_dev_stats(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(1, repfield, repfield.GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.StepStats)
  return target;
}

size_t StepStats::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.StepStats)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.DeviceStepStats dev_stats = 1;
  total_size += 1UL * this->_internal_dev_stats_size();
  for (const auto& msg : this->_impl_.dev_stats_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData StepStats::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    StepStats::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*StepStats::GetClassData() const { return &_class_data_; }


void StepStats::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<StepStats*>(&to_msg);
  auto& from = static_cast<const StepStats&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.StepStats)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.dev_stats_.MergeFrom(from._impl_.dev_stats_);
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void StepStats::CopyFrom(const StepStats& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.StepStats)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool StepStats::IsInitialized() const {
  return true;
}

void StepStats::InternalSwap(StepStats* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.dev_stats_.InternalSwap(&other->_impl_.dev_stats_);
}

::PROTOBUF_NAMESPACE_ID::Metadata StepStats::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_getter, &descriptor_table_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto_once,
      file_level_metadata_tensorflow_2fcore_2fframework_2fstep_5fstats_2eproto[7]);
}

// @@protoc_insertion_point(namespace_scope)
}  // namespace tensorflow
PROTOBUF_NAMESPACE_OPEN
template<> PROTOBUF_NOINLINE ::tensorflow::AllocationRecord*
Arena::CreateMaybeMessage< ::tensorflow::AllocationRecord >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::AllocationRecord >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::AllocatorMemoryUsed*
Arena::CreateMaybeMessage< ::tensorflow::AllocatorMemoryUsed >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::AllocatorMemoryUsed >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::NodeOutput*
Arena::CreateMaybeMessage< ::tensorflow::NodeOutput >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::NodeOutput >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::MemoryStats*
Arena::CreateMaybeMessage< ::tensorflow::MemoryStats >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::MemoryStats >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::NodeExecStats*
Arena::CreateMaybeMessage< ::tensorflow::NodeExecStats >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::NodeExecStats >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::DeviceStepStats_ThreadNamesEntry_DoNotUse*
Arena::CreateMaybeMessage< ::tensorflow::DeviceStepStats_ThreadNamesEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::DeviceStepStats_ThreadNamesEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::DeviceStepStats*
Arena::CreateMaybeMessage< ::tensorflow::DeviceStepStats >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::DeviceStepStats >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::StepStats*
Arena::CreateMaybeMessage< ::tensorflow::StepStats >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::StepStats >(arena);
}
PROTOBUF_NAMESPACE_CLOSE

// @@protoc_insertion_point(global_scope)
#include <google/protobuf/port_undef.inc>
